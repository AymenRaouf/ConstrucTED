Eid|Sid|Order|Title|Created at|Description|Course
0|0|1|Statistical Learning: 1.1 Opening Remarks|2022-10-07T16:59:45Z|This is an introductory-level course in supervised learning, with a focus on regression and classification methods. The syllabus includes: linear and polynomial regression, logistic regression and linear discriminant analysis; cross-validation and the bootstrap, model selection and regularization methods (ridge and lasso); nonlinear models, splines and generalized additive models; tree-based methods, random forests and boosting; support-vector machines; neural networks and deep learning; survival models; multiple testing. Some unsupervised learning methods are discussed: principal components and clustering (k-means and hierarchical).  This is not a math-heavy class, so we try and describe the methods without heavy reliance on formulas and complex mathematics. We focus on what we consider to be the important elements of modern data science. Computing is done in Python. There are lectures devoted to Python, giving tutorials from the ground up, and progressing with more detailed sessions that implement the techniques in each chapter.  The lectures cover all the material in An Introduction to Statistical Learning, with Applications in Python by James, Witten, Hastie, Tibshirani and Taylor (Springer, 2023). The pdf for this book is available for free on the book website.  What You'll Learn: Overview of statistical learning Linear regression Classification Resampling methods Linear model selection and regularization Moving beyond linearity Tree-based methods Support vector machines Deep learning Survival modeling Unsupervised learning Multiple testing|LvySJGj-88U
1|0|2|Statistical Learning: 8 Years Later (Second Edition of the Course)|2022-10-07T17:00:04Z|This is an introductory-level course in supervised learning, with a focus on regression and classification methods. The syllabus includes: linear and polynomial regression, logistic regression and linear discriminant analysis; cross-validation and the bootstrap, model selection and regularization methods (ridge and lasso); nonlinear models, splines and generalized additive models; tree-based methods, random forests and boosting; support-vector machines; neural networks and deep learning; survival models; multiple testing. Some unsupervised learning methods are discussed: principal components and clustering (k-means and hierarchical).  This is not a math-heavy class, so we try and describe the methods without heavy reliance on formulas and complex mathematics. We focus on what we consider to be the important elements of modern data science. Computing is done in Python. There are lectures devoted to Python, giving tutorials from the ground up, and progressing with more detailed sessions that implement the techniques in each chapter.  The lectures cover all the material in An Introduction to Statistical Learning, with Applications in Python by James, Witten, Hastie, Tibshirani and Taylor (Springer, 2023). The pdf for this book is available for free on the book website.  What You'll Learn: Overview of statistical learning Linear regression Classification Resampling methods Linear model selection and regularization Moving beyond linearity Tree-based methods Support vector machines Deep learning Survival modeling Unsupervised learning Multiple testing|9vlDVxG4ulA
2|0|3|Statistical Learning I Introducing Jonathan - Third Edition of the Course I 2023|2023-12-05T18:58:08Z|This is an introductory-level course in supervised learning, with a focus on regression and classification methods. The syllabus includes: linear and polynomial regression, logistic regression and linear discriminant analysis; cross-validation and the bootstrap, model selection and regularization methods (ridge and lasso); nonlinear models, splines and generalized additive models; tree-based methods, random forests and boosting; support-vector machines; neural networks and deep learning; survival models; multiple testing. Some unsupervised learning methods are discussed: principal components and clustering (k-means and hierarchical).  This is not a math-heavy class, so we try and describe the methods without heavy reliance on formulas and complex mathematics. We focus on what we consider to be the important elements of modern data science. Computing is done in Python. There are lectures devoted to Python, giving tutorials from the ground up, and progressing with more detailed sessions that implement the techniques in each chapter.  The lectures cover all the material in An Introduction to Statistical Learning, with Applications in Python by James, Witten, Hastie, Tibshirani and Taylor (Springer, 2023). The pdf for this book is available for free on the book website.  What You'll Learn: Overview of statistical learning Linear regression Classification Resampling methods Linear model selection and regularization Moving beyond linearity Tree-based methods Support vector machines Deep learning Survival modeling Unsupervised learning Multiple testing|Igd5srPxZfU
3|0|4|Statistical Learning: 1.2 Examples and Framework|2022-10-07T17:00:20Z|This is an introductory-level course in supervised learning, with a focus on regression and classification methods. The syllabus includes: linear and polynomial regression, logistic regression and linear discriminant analysis; cross-validation and the bootstrap, model selection and regularization methods (ridge and lasso); nonlinear models, splines and generalized additive models; tree-based methods, random forests and boosting; support-vector machines; neural networks and deep learning; survival models; multiple testing. Some unsupervised learning methods are discussed: principal components and clustering (k-means and hierarchical).  This is not a math-heavy class, so we try and describe the methods without heavy reliance on formulas and complex mathematics. We focus on what we consider to be the important elements of modern data science. Computing is done in Python. There are lectures devoted to Python, giving tutorials from the ground up, and progressing with more detailed sessions that implement the techniques in each chapter.  The lectures cover all the material in An Introduction to Statistical Learning, with Applications in Python by James, Witten, Hastie, Tibshirani and Taylor (Springer, 2023). The pdf for this book is available for free on the book website.  What You'll Learn: Overview of statistical learning Linear regression Classification Resampling methods Linear model selection and regularization Moving beyond linearity Tree-based methods Support vector machines Deep learning Survival modeling Unsupervised learning Multiple testing|B9s8rpdNxU0
4|0|5|Statistical Learning: 2.1 Introduction to Regression Models|2022-10-07T17:00:41Z|This is an introductory-level course in supervised learning, with a focus on regression and classification methods. The syllabus includes: linear and polynomial regression, logistic regression and linear discriminant analysis; cross-validation and the bootstrap, model selection and regularization methods (ridge and lasso); nonlinear models, splines and generalized additive models; tree-based methods, random forests and boosting; support-vector machines; neural networks and deep learning; survival models; multiple testing. Some unsupervised learning methods are discussed: principal components and clustering (k-means and hierarchical).  This is not a math-heavy class, so we try and describe the methods without heavy reliance on formulas and complex mathematics. We focus on what we consider to be the important elements of modern data science. Computing is done in Python. There are lectures devoted to Python, giving tutorials from the ground up, and progressing with more detailed sessions that implement the techniques in each chapter.  The lectures cover all the material in An Introduction to Statistical Learning, with Applications in Python by James, Witten, Hastie, Tibshirani and Taylor (Springer, 2023). The pdf for this book is available for free on the book website.  What You'll Learn: Overview of statistical learning Linear regression Classification Resampling methods Linear model selection and regularization Moving beyond linearity Tree-based methods Support vector machines Deep learning Survival modeling Unsupervised learning Multiple testing|ox0cKk7h4o0
5|0|6|Statistical Learning: 2.2 Dimensionality and Structured Models|2022-10-07T17:00:57Z|This is an introductory-level course in supervised learning, with a focus on regression and classification methods. The syllabus includes: linear and polynomial regression, logistic regression and linear discriminant analysis; cross-validation and the bootstrap, model selection and regularization methods (ridge and lasso); nonlinear models, splines and generalized additive models; tree-based methods, random forests and boosting; support-vector machines; neural networks and deep learning; survival models; multiple testing. Some unsupervised learning methods are discussed: principal components and clustering (k-means and hierarchical).  This is not a math-heavy class, so we try and describe the methods without heavy reliance on formulas and complex mathematics. We focus on what we consider to be the important elements of modern data science. Computing is done in Python. There are lectures devoted to Python, giving tutorials from the ground up, and progressing with more detailed sessions that implement the techniques in each chapter.  The lectures cover all the material in An Introduction to Statistical Learning, with Applications in Python by James, Witten, Hastie, Tibshirani and Taylor (Springer, 2023). The pdf for this book is available for free on the book website.  What You'll Learn: Overview of statistical learning Linear regression Classification Resampling methods Linear model selection and regularization Moving beyond linearity Tree-based methods Support vector machines Deep learning Survival modeling Unsupervised learning Multiple testing|uFwbrdvrAJs
6|0|7|Statistical Learning: 2.3 Model Selection and Bias Variance Tradeoff|2022-10-07T17:01:13Z|This is an introductory-level course in supervised learning, with a focus on regression and classification methods. The syllabus includes: linear and polynomial regression, logistic regression and linear discriminant analysis; cross-validation and the bootstrap, model selection and regularization methods (ridge and lasso); nonlinear models, splines and generalized additive models; tree-based methods, random forests and boosting; support-vector machines; neural networks and deep learning; survival models; multiple testing. Some unsupervised learning methods are discussed: principal components and clustering (k-means and hierarchical).  This is not a math-heavy class, so we try and describe the methods without heavy reliance on formulas and complex mathematics. We focus on what we consider to be the important elements of modern data science. Computing is done in Python. There are lectures devoted to Python, giving tutorials from the ground up, and progressing with more detailed sessions that implement the techniques in each chapter.  The lectures cover all the material in An Introduction to Statistical Learning, with Applications in Python by James, Witten, Hastie, Tibshirani and Taylor (Springer, 2023). The pdf for this book is available for free on the book website.  What You'll Learn: Overview of statistical learning Linear regression Classification Resampling methods Linear model selection and regularization Moving beyond linearity Tree-based methods Support vector machines Deep learning Survival modeling Unsupervised learning Multiple testing|pvcEQfcO3pk
7|0|8|Statistical Learning: 2.4 Classification|2022-10-07T17:01:31Z|This is an introductory-level course in supervised learning, with a focus on regression and classification methods. The syllabus includes: linear and polynomial regression, logistic regression and linear discriminant analysis; cross-validation and the bootstrap, model selection and regularization methods (ridge and lasso); nonlinear models, splines and generalized additive models; tree-based methods, random forests and boosting; support-vector machines; neural networks and deep learning; survival models; multiple testing. Some unsupervised learning methods are discussed: principal components and clustering (k-means and hierarchical).  This is not a math-heavy class, so we try and describe the methods without heavy reliance on formulas and complex mathematics. We focus on what we consider to be the important elements of modern data science. Computing is done in Python. There are lectures devoted to Python, giving tutorials from the ground up, and progressing with more detailed sessions that implement the techniques in each chapter.  The lectures cover all the material in An Introduction to Statistical Learning, with Applications in Python by James, Witten, Hastie, Tibshirani and Taylor (Springer, 2023). The pdf for this book is available for free on the book website.  What You'll Learn: Overview of statistical learning Linear regression Classification Resampling methods Linear model selection and regularization Moving beyond linearity Tree-based methods Support vector machines Deep learning Survival modeling Unsupervised learning Multiple testing|BMJQ3LQ_QKU
8|0|9|Statistical Learning: 2.Py Setting Up Python I 2023|2023-12-05T18:58:20Z|This is an introductory-level course in supervised learning, with a focus on regression and classification methods. The syllabus includes: linear and polynomial regression, logistic regression and linear discriminant analysis; cross-validation and the bootstrap, model selection and regularization methods (ridge and lasso); nonlinear models, splines and generalized additive models; tree-based methods, random forests and boosting; support-vector machines; neural networks and deep learning; survival models; multiple testing. Some unsupervised learning methods are discussed: principal components and clustering (k-means and hierarchical).  This is not a math-heavy class, so we try and describe the methods without heavy reliance on formulas and complex mathematics. We focus on what we consider to be the important elements of modern data science. Computing is done in Python. There are lectures devoted to Python, giving tutorials from the ground up, and progressing with more detailed sessions that implement the techniques in each chapter.  The lectures cover all the material in An Introduction to Statistical Learning, with Applications in Python by James, Witten, Hastie, Tibshirani and Taylor (Springer, 2023). The pdf for this book is available for free on the book website.  What You'll Learn: Overview of statistical learning Linear regression Classification Resampling methods Linear model selection and regularization Moving beyond linearity Tree-based methods Support vector machines Deep learning Survival modeling Unsupervised learning Multiple testing|RelOJOIKaTk
9|0|10|Statistical Learning: 2.Py Data Types, Arrays, and Basics I 2023|2023-12-05T18:58:28Z|This is an introductory-level course in supervised learning, with a focus on regression and classification methods. The syllabus includes: linear and polynomial regression, logistic regression and linear discriminant analysis; cross-validation and the bootstrap, model selection and regularization methods (ridge and lasso); nonlinear models, splines and generalized additive models; tree-based methods, random forests and boosting; support-vector machines; neural networks and deep learning; survival models; multiple testing. Some unsupervised learning methods are discussed: principal components and clustering (k-means and hierarchical).  This is not a math-heavy class, so we try and describe the methods without heavy reliance on formulas and complex mathematics. We focus on what we consider to be the important elements of modern data science. Computing is done in Python. There are lectures devoted to Python, giving tutorials from the ground up, and progressing with more detailed sessions that implement the techniques in each chapter.  The lectures cover all the material in An Introduction to Statistical Learning, with Applications in Python by James, Witten, Hastie, Tibshirani and Taylor (Springer, 2023). The pdf for this book is available for free on the book website.  What You'll Learn: Overview of statistical learning Linear regression Classification Resampling methods Linear model selection and regularization Moving beyond linearity Tree-based methods Support vector machines Deep learning Survival modeling Unsupervised learning Multiple testing|Cv1sx_HNRHM
10|0|11|Statistical Learning: 2.Py.3 Graphics I 2023|2023-12-05T18:58:39Z|This is an introductory-level course in supervised learning, with a focus on regression and classification methods. The syllabus includes: linear and polynomial regression, logistic regression and linear discriminant analysis; cross-validation and the bootstrap, model selection and regularization methods (ridge and lasso); nonlinear models, splines and generalized additive models; tree-based methods, random forests and boosting; support-vector machines; neural networks and deep learning; survival models; multiple testing. Some unsupervised learning methods are discussed: principal components and clustering (k-means and hierarchical).  This is not a math-heavy class, so we try and describe the methods without heavy reliance on formulas and complex mathematics. We focus on what we consider to be the important elements of modern data science. Computing is done in Python. There are lectures devoted to Python, giving tutorials from the ground up, and progressing with more detailed sessions that implement the techniques in each chapter.  The lectures cover all the material in An Introduction to Statistical Learning, with Applications in Python by James, Witten, Hastie, Tibshirani and Taylor (Springer, 2023). The pdf for this book is available for free on the book website.  What You'll Learn: Overview of statistical learning Linear regression Classification Resampling methods Linear model selection and regularization Moving beyond linearity Tree-based methods Support vector machines Deep learning Survival modeling Unsupervised learning Multiple testing|JykmuvCY130
11|0|12|Statistical Learning: 2.Py Indexing and Dataframes I 2023|2023-12-05T18:58:45Z|This is an introductory-level course in supervised learning, with a focus on regression and classification methods. The syllabus includes: linear and polynomial regression, logistic regression and linear discriminant analysis; cross-validation and the bootstrap, model selection and regularization methods (ridge and lasso); nonlinear models, splines and generalized additive models; tree-based methods, random forests and boosting; support-vector machines; neural networks and deep learning; survival models; multiple testing. Some unsupervised learning methods are discussed: principal components and clustering (k-means and hierarchical).  This is not a math-heavy class, so we try and describe the methods without heavy reliance on formulas and complex mathematics. We focus on what we consider to be the important elements of modern data science. Computing is done in Python. There are lectures devoted to Python, giving tutorials from the ground up, and progressing with more detailed sessions that implement the techniques in each chapter.  The lectures cover all the material in An Introduction to Statistical Learning, with Applications in Python by James, Witten, Hastie, Tibshirani and Taylor (Springer, 2023). The pdf for this book is available for free on the book website.  What You'll Learn: Overview of statistical learning Linear regression Classification Resampling methods Linear model selection and regularization Moving beyond linearity Tree-based methods Support vector machines Deep learning Survival modeling Unsupervised learning Multiple testing|Zcb-1JmmO_U
12|0|13|Statistical Learning: 3.1 Simple linear regression|2022-10-07T17:02:04Z|This is an introductory-level course in supervised learning, with a focus on regression and classification methods. The syllabus includes: linear and polynomial regression, logistic regression and linear discriminant analysis; cross-validation and the bootstrap, model selection and regularization methods (ridge and lasso); nonlinear models, splines and generalized additive models; tree-based methods, random forests and boosting; support-vector machines; neural networks and deep learning; survival models; multiple testing. Some unsupervised learning methods are discussed: principal components and clustering (k-means and hierarchical).  This is not a math-heavy class, so we try and describe the methods without heavy reliance on formulas and complex mathematics. We focus on what we consider to be the important elements of modern data science. Computing is done in Python. There are lectures devoted to Python, giving tutorials from the ground up, and progressing with more detailed sessions that implement the techniques in each chapter.  The lectures cover all the material in An Introduction to Statistical Learning, with Applications in Python by James, Witten, Hastie, Tibshirani and Taylor (Springer, 2023). The pdf for this book is available for free on the book website.  What You'll Learn: Overview of statistical learning Linear regression Classification Resampling methods Linear model selection and regularization Moving beyond linearity Tree-based methods Support vector machines Deep learning Survival modeling Unsupervised learning Multiple testing|vCHtY6Me5FI
13|0|14|Statistical Learning: 3.2 Hypothesis Testing and Confidence Intervals|2022-10-07T17:02:22Z|This is an introductory-level course in supervised learning, with a focus on regression and classification methods. The syllabus includes: linear and polynomial regression, logistic regression and linear discriminant analysis; cross-validation and the bootstrap, model selection and regularization methods (ridge and lasso); nonlinear models, splines and generalized additive models; tree-based methods, random forests and boosting; support-vector machines; neural networks and deep learning; survival models; multiple testing. Some unsupervised learning methods are discussed: principal components and clustering (k-means and hierarchical).  This is not a math-heavy class, so we try and describe the methods without heavy reliance on formulas and complex mathematics. We focus on what we consider to be the important elements of modern data science. Computing is done in Python. There are lectures devoted to Python, giving tutorials from the ground up, and progressing with more detailed sessions that implement the techniques in each chapter.  The lectures cover all the material in An Introduction to Statistical Learning, with Applications in Python by James, Witten, Hastie, Tibshirani and Taylor (Springer, 2023). The pdf for this book is available for free on the book website.  What You'll Learn: Overview of statistical learning Linear regression Classification Resampling methods Linear model selection and regularization Moving beyond linearity Tree-based methods Support vector machines Deep learning Survival modeling Unsupervised learning Multiple testing|3GiWpRfkSjc
14|0|15|Statistical Learning: 3.3 Multiple Linear Regression|2022-10-07T17:02:38Z|This is an introductory-level course in supervised learning, with a focus on regression and classification methods. The syllabus includes: linear and polynomial regression, logistic regression and linear discriminant analysis; cross-validation and the bootstrap, model selection and regularization methods (ridge and lasso); nonlinear models, splines and generalized additive models; tree-based methods, random forests and boosting; support-vector machines; neural networks and deep learning; survival models; multiple testing. Some unsupervised learning methods are discussed: principal components and clustering (k-means and hierarchical).  This is not a math-heavy class, so we try and describe the methods without heavy reliance on formulas and complex mathematics. We focus on what we consider to be the important elements of modern data science. Computing is done in Python. There are lectures devoted to Python, giving tutorials from the ground up, and progressing with more detailed sessions that implement the techniques in each chapter.  The lectures cover all the material in An Introduction to Statistical Learning, with Applications in Python by James, Witten, Hastie, Tibshirani and Taylor (Springer, 2023). The pdf for this book is available for free on the book website.  What You'll Learn: Overview of statistical learning Linear regression Classification Resampling methods Linear model selection and regularization Moving beyond linearity Tree-based methods Support vector machines Deep learning Survival modeling Unsupervised learning Multiple testing|o9hoLdylWKo
15|0|16|Statistical Learning: 3.4 Some important questions|2022-10-07T17:02:54Z|This is an introductory-level course in supervised learning, with a focus on regression and classification methods. The syllabus includes: linear and polynomial regression, logistic regression and linear discriminant analysis; cross-validation and the bootstrap, model selection and regularization methods (ridge and lasso); nonlinear models, splines and generalized additive models; tree-based methods, random forests and boosting; support-vector machines; neural networks and deep learning; survival models; multiple testing. Some unsupervised learning methods are discussed: principal components and clustering (k-means and hierarchical).  This is not a math-heavy class, so we try and describe the methods without heavy reliance on formulas and complex mathematics. We focus on what we consider to be the important elements of modern data science. Computing is done in Python. There are lectures devoted to Python, giving tutorials from the ground up, and progressing with more detailed sessions that implement the techniques in each chapter.  The lectures cover all the material in An Introduction to Statistical Learning, with Applications in Python by James, Witten, Hastie, Tibshirani and Taylor (Springer, 2023). The pdf for this book is available for free on the book website.  What You'll Learn: Overview of statistical learning Linear regression Classification Resampling methods Linear model selection and regularization Moving beyond linearity Tree-based methods Support vector machines Deep learning Survival modeling Unsupervised learning Multiple testing|50sv4UTjE90
16|0|17|Statistical Learning: 3.5 Extensions of the Linear Model|2022-10-07T17:03:10Z|This is an introductory-level course in supervised learning, with a focus on regression and classification methods. The syllabus includes: linear and polynomial regression, logistic regression and linear discriminant analysis; cross-validation and the bootstrap, model selection and regularization methods (ridge and lasso); nonlinear models, splines and generalized additive models; tree-based methods, random forests and boosting; support-vector machines; neural networks and deep learning; survival models; multiple testing. Some unsupervised learning methods are discussed: principal components and clustering (k-means and hierarchical).  This is not a math-heavy class, so we try and describe the methods without heavy reliance on formulas and complex mathematics. We focus on what we consider to be the important elements of modern data science. Computing is done in Python. There are lectures devoted to Python, giving tutorials from the ground up, and progressing with more detailed sessions that implement the techniques in each chapter.  The lectures cover all the material in An Introduction to Statistical Learning, with Applications in Python by James, Witten, Hastie, Tibshirani and Taylor (Springer, 2023). The pdf for this book is available for free on the book website.  What You'll Learn: Overview of statistical learning Linear regression Classification Resampling methods Linear model selection and regularization Moving beyond linearity Tree-based methods Support vector machines Deep learning Survival modeling Unsupervised learning Multiple testing|dEBQmiXv9fk
17|0|18|Statistical Learning: 3.Py Linear Regression and statsmodels Package I 2023|2023-12-05T18:58:55Z|This is an introductory-level course in supervised learning, with a focus on regression and classification methods. The syllabus includes: linear and polynomial regression, logistic regression and linear discriminant analysis; cross-validation and the bootstrap, model selection and regularization methods (ridge and lasso); nonlinear models, splines and generalized additive models; tree-based methods, random forests and boosting; support-vector machines; neural networks and deep learning; survival models; multiple testing. Some unsupervised learning methods are discussed: principal components and clustering (k-means and hierarchical).  This is not a math-heavy class, so we try and describe the methods without heavy reliance on formulas and complex mathematics. We focus on what we consider to be the important elements of modern data science. Computing is done in Python. There are lectures devoted to Python, giving tutorials from the ground up, and progressing with more detailed sessions that implement the techniques in each chapter.  The lectures cover all the material in An Introduction to Statistical Learning, with Applications in Python by James, Witten, Hastie, Tibshirani and Taylor (Springer, 2023). The pdf for this book is available for free on the book website.  What You'll Learn: Overview of statistical learning Linear regression Classification Resampling methods Linear model selection and regularization Moving beyond linearity Tree-based methods Support vector machines Deep learning Survival modeling Unsupervised learning Multiple testing|mKalBNrxToU
18|0|19|Statistical Learning: 3.Py Multiple Linear Regression Package I 2023|2023-12-05T18:59:02Z|This is an introductory-level course in supervised learning, with a focus on regression and classification methods. The syllabus includes: linear and polynomial regression, logistic regression and linear discriminant analysis; cross-validation and the bootstrap, model selection and regularization methods (ridge and lasso); nonlinear models, splines and generalized additive models; tree-based methods, random forests and boosting; support-vector machines; neural networks and deep learning; survival models; multiple testing. Some unsupervised learning methods are discussed: principal components and clustering (k-means and hierarchical).  This is not a math-heavy class, so we try and describe the methods without heavy reliance on formulas and complex mathematics. We focus on what we consider to be the important elements of modern data science. Computing is done in Python. There are lectures devoted to Python, giving tutorials from the ground up, and progressing with more detailed sessions that implement the techniques in each chapter.  The lectures cover all the material in An Introduction to Statistical Learning, with Applications in Python by James, Witten, Hastie, Tibshirani and Taylor (Springer, 2023). The pdf for this book is available for free on the book website.  What You'll Learn: Overview of statistical learning Linear regression Classification Resampling methods Linear model selection and regularization Moving beyond linearity Tree-based methods Support vector machines Deep learning Survival modeling Unsupervised learning Multiple testing|pGOr3HZhUZQ
19|0|20|Statistical Learning: 3.Py Interactions, Qualitative Predictors and Other Details I 2023|2023-12-05T18:59:12Z|This is an introductory-level course in supervised learning, with a focus on regression and classification methods. The syllabus includes: linear and polynomial regression, logistic regression and linear discriminant analysis; cross-validation and the bootstrap, model selection and regularization methods (ridge and lasso); nonlinear models, splines and generalized additive models; tree-based methods, random forests and boosting; support-vector machines; neural networks and deep learning; survival models; multiple testing. Some unsupervised learning methods are discussed: principal components and clustering (k-means and hierarchical).  This is not a math-heavy class, so we try and describe the methods without heavy reliance on formulas and complex mathematics. We focus on what we consider to be the important elements of modern data science. Computing is done in Python. There are lectures devoted to Python, giving tutorials from the ground up, and progressing with more detailed sessions that implement the techniques in each chapter.  The lectures cover all the material in An Introduction to Statistical Learning, with Applications in Python by James, Witten, Hastie, Tibshirani and Taylor (Springer, 2023). The pdf for this book is available for free on the book website.  What You'll Learn: Overview of statistical learning Linear regression Classification Resampling methods Linear model selection and regularization Moving beyond linearity Tree-based methods Support vector machines Deep learning Survival modeling Unsupervised learning Multiple testing|d0K2mDclyXM
20|0|21|Statistical Learning: 4.1 Introduction to Classification Problems|2022-10-07T17:03:41Z|This is an introductory-level course in supervised learning, with a focus on regression and classification methods. The syllabus includes: linear and polynomial regression, logistic regression and linear discriminant analysis; cross-validation and the bootstrap, model selection and regularization methods (ridge and lasso); nonlinear models, splines and generalized additive models; tree-based methods, random forests and boosting; support-vector machines; neural networks and deep learning; survival models; multiple testing. Some unsupervised learning methods are discussed: principal components and clustering (k-means and hierarchical).  This is not a math-heavy class, so we try and describe the methods without heavy reliance on formulas and complex mathematics. We focus on what we consider to be the important elements of modern data science. Computing is done in Python. There are lectures devoted to Python, giving tutorials from the ground up, and progressing with more detailed sessions that implement the techniques in each chapter.  The lectures cover all the material in An Introduction to Statistical Learning, with Applications in Python by James, Witten, Hastie, Tibshirani and Taylor (Springer, 2023). The pdf for this book is available for free on the book website.  What You'll Learn: Overview of statistical learning Linear regression Classification Resampling methods Linear model selection and regularization Moving beyond linearity Tree-based methods Support vector machines Deep learning Survival modeling Unsupervised learning Multiple testing|ju3J7iRy6xI
21|0|22|Statistical Learning: 4.2 Logistic Regression|2022-10-07T17:03:56Z|This is an introductory-level course in supervised learning, with a focus on regression and classification methods. The syllabus includes: linear and polynomial regression, logistic regression and linear discriminant analysis; cross-validation and the bootstrap, model selection and regularization methods (ridge and lasso); nonlinear models, splines and generalized additive models; tree-based methods, random forests and boosting; support-vector machines; neural networks and deep learning; survival models; multiple testing. Some unsupervised learning methods are discussed: principal components and clustering (k-means and hierarchical).  This is not a math-heavy class, so we try and describe the methods without heavy reliance on formulas and complex mathematics. We focus on what we consider to be the important elements of modern data science. Computing is done in Python. There are lectures devoted to Python, giving tutorials from the ground up, and progressing with more detailed sessions that implement the techniques in each chapter.  The lectures cover all the material in An Introduction to Statistical Learning, with Applications in Python by James, Witten, Hastie, Tibshirani and Taylor (Springer, 2023). The pdf for this book is available for free on the book website.  What You'll Learn: Overview of statistical learning Linear regression Classification Resampling methods Linear model selection and regularization Moving beyond linearity Tree-based methods Support vector machines Deep learning Survival modeling Unsupervised learning Multiple testing|kr_Be9NVXOM
22|0|23|Statistical Learning: 4.3 Multivariate Logistic Regression|2022-10-07T17:04:12Z|This is an introductory-level course in supervised learning, with a focus on regression and classification methods. The syllabus includes: linear and polynomial regression, logistic regression and linear discriminant analysis; cross-validation and the bootstrap, model selection and regularization methods (ridge and lasso); nonlinear models, splines and generalized additive models; tree-based methods, random forests and boosting; support-vector machines; neural networks and deep learning; survival models; multiple testing. Some unsupervised learning methods are discussed: principal components and clustering (k-means and hierarchical).  This is not a math-heavy class, so we try and describe the methods without heavy reliance on formulas and complex mathematics. We focus on what we consider to be the important elements of modern data science. Computing is done in Python. There are lectures devoted to Python, giving tutorials from the ground up, and progressing with more detailed sessions that implement the techniques in each chapter.  The lectures cover all the material in An Introduction to Statistical Learning, with Applications in Python by James, Witten, Hastie, Tibshirani and Taylor (Springer, 2023). The pdf for this book is available for free on the book website.  What You'll Learn: Overview of statistical learning Linear regression Classification Resampling methods Linear model selection and regularization Moving beyond linearity Tree-based methods Support vector machines Deep learning Survival modeling Unsupervised learning Multiple testing|1uJVE8bkabc
23|0|24|Statistical Learning: 4.4 Logistic Regression Case Control Sampling and Multiclass|2022-10-07T17:04:27Z|This is an introductory-level course in supervised learning, with a focus on regression and classification methods. The syllabus includes: linear and polynomial regression, logistic regression and linear discriminant analysis; cross-validation and the bootstrap, model selection and regularization methods (ridge and lasso); nonlinear models, splines and generalized additive models; tree-based methods, random forests and boosting; support-vector machines; neural networks and deep learning; survival models; multiple testing. Some unsupervised learning methods are discussed: principal components and clustering (k-means and hierarchical).  This is not a math-heavy class, so we try and describe the methods without heavy reliance on formulas and complex mathematics. We focus on what we consider to be the important elements of modern data science. Computing is done in Python. There are lectures devoted to Python, giving tutorials from the ground up, and progressing with more detailed sessions that implement the techniques in each chapter.  The lectures cover all the material in An Introduction to Statistical Learning, with Applications in Python by James, Witten, Hastie, Tibshirani and Taylor (Springer, 2023). The pdf for this book is available for free on the book website.  What You'll Learn: Overview of statistical learning Linear regression Classification Resampling methods Linear model selection and regularization Moving beyond linearity Tree-based methods Support vector machines Deep learning Survival modeling Unsupervised learning Multiple testing|sYDDk6R-be0
24|0|25|Statistical Learning: 4.5 Discriminant Analysis|2022-10-07T17:04:43Z|This is an introductory-level course in supervised learning, with a focus on regression and classification methods. The syllabus includes: linear and polynomial regression, logistic regression and linear discriminant analysis; cross-validation and the bootstrap, model selection and regularization methods (ridge and lasso); nonlinear models, splines and generalized additive models; tree-based methods, random forests and boosting; support-vector machines; neural networks and deep learning; survival models; multiple testing. Some unsupervised learning methods are discussed: principal components and clustering (k-means and hierarchical).  This is not a math-heavy class, so we try and describe the methods without heavy reliance on formulas and complex mathematics. We focus on what we consider to be the important elements of modern data science. Computing is done in Python. There are lectures devoted to Python, giving tutorials from the ground up, and progressing with more detailed sessions that implement the techniques in each chapter.  The lectures cover all the material in An Introduction to Statistical Learning, with Applications in Python by James, Witten, Hastie, Tibshirani and Taylor (Springer, 2023). The pdf for this book is available for free on the book website.  What You'll Learn: Overview of statistical learning Linear regression Classification Resampling methods Linear model selection and regularization Moving beyond linearity Tree-based methods Support vector machines Deep learning Survival modeling Unsupervised learning Multiple testing|oJc2r246VoQ
25|0|26|Statistical Learning: 4.6 Gaussian Discriminant Analysis (One Variable)|2022-10-07T17:04:58Z|This is an introductory-level course in supervised learning, with a focus on regression and classification methods. The syllabus includes: linear and polynomial regression, logistic regression and linear discriminant analysis; cross-validation and the bootstrap, model selection and regularization methods (ridge and lasso); nonlinear models, splines and generalized additive models; tree-based methods, random forests and boosting; support-vector machines; neural networks and deep learning; survival models; multiple testing. Some unsupervised learning methods are discussed: principal components and clustering (k-means and hierarchical).  This is not a math-heavy class, so we try and describe the methods without heavy reliance on formulas and complex mathematics. We focus on what we consider to be the important elements of modern data science. Computing is done in Python. There are lectures devoted to Python, giving tutorials from the ground up, and progressing with more detailed sessions that implement the techniques in each chapter.  The lectures cover all the material in An Introduction to Statistical Learning, with Applications in Python by James, Witten, Hastie, Tibshirani and Taylor (Springer, 2023). The pdf for this book is available for free on the book website.  What You'll Learn: Overview of statistical learning Linear regression Classification Resampling methods Linear model selection and regularization Moving beyond linearity Tree-based methods Support vector machines Deep learning Survival modeling Unsupervised learning Multiple testing|14JVlzWHKgk
26|0|27|Statistical Learning: 4.7 Gaussian Discriminant Analysis (Many Variables)|2022-10-07T17:05:13Z|This is an introductory-level course in supervised learning, with a focus on regression and classification methods. The syllabus includes: linear and polynomial regression, logistic regression and linear discriminant analysis; cross-validation and the bootstrap, model selection and regularization methods (ridge and lasso); nonlinear models, splines and generalized additive models; tree-based methods, random forests and boosting; support-vector machines; neural networks and deep learning; survival models; multiple testing. Some unsupervised learning methods are discussed: principal components and clustering (k-means and hierarchical).  This is not a math-heavy class, so we try and describe the methods without heavy reliance on formulas and complex mathematics. We focus on what we consider to be the important elements of modern data science. Computing is done in Python. There are lectures devoted to Python, giving tutorials from the ground up, and progressing with more detailed sessions that implement the techniques in each chapter.  The lectures cover all the material in An Introduction to Statistical Learning, with Applications in Python by James, Witten, Hastie, Tibshirani and Taylor (Springer, 2023). The pdf for this book is available for free on the book website.  What You'll Learn: Overview of statistical learning Linear regression Classification Resampling methods Linear model selection and regularization Moving beyond linearity Tree-based methods Support vector machines Deep learning Survival modeling Unsupervised learning Multiple testing|aUlTqhDtpnw
27|0|28|Statistical Learning: 4.8 Generalized Linear Models|2022-10-07T17:05:28Z|This is an introductory-level course in supervised learning, with a focus on regression and classification methods. The syllabus includes: linear and polynomial regression, logistic regression and linear discriminant analysis; cross-validation and the bootstrap, model selection and regularization methods (ridge and lasso); nonlinear models, splines and generalized additive models; tree-based methods, random forests and boosting; support-vector machines; neural networks and deep learning; survival models; multiple testing. Some unsupervised learning methods are discussed: principal components and clustering (k-means and hierarchical).  This is not a math-heavy class, so we try and describe the methods without heavy reliance on formulas and complex mathematics. We focus on what we consider to be the important elements of modern data science. Computing is done in Python. There are lectures devoted to Python, giving tutorials from the ground up, and progressing with more detailed sessions that implement the techniques in each chapter.  The lectures cover all the material in An Introduction to Statistical Learning, with Applications in Python by James, Witten, Hastie, Tibshirani and Taylor (Springer, 2023). The pdf for this book is available for free on the book website.  What You'll Learn: Overview of statistical learning Linear regression Classification Resampling methods Linear model selection and regularization Moving beyond linearity Tree-based methods Support vector machines Deep learning Survival modeling Unsupervised learning Multiple testing|n8Nj64FyjSo
28|0|29|Statistical Learning: 4.9 Quadratic Discriminant Analysis and Naive Bayes|2022-10-07T17:05:43Z|This is an introductory-level course in supervised learning, with a focus on regression and classification methods. The syllabus includes: linear and polynomial regression, logistic regression and linear discriminant analysis; cross-validation and the bootstrap, model selection and regularization methods (ridge and lasso); nonlinear models, splines and generalized additive models; tree-based methods, random forests and boosting; support-vector machines; neural networks and deep learning; survival models; multiple testing. Some unsupervised learning methods are discussed: principal components and clustering (k-means and hierarchical).  This is not a math-heavy class, so we try and describe the methods without heavy reliance on formulas and complex mathematics. We focus on what we consider to be the important elements of modern data science. Computing is done in Python. There are lectures devoted to Python, giving tutorials from the ground up, and progressing with more detailed sessions that implement the techniques in each chapter.  The lectures cover all the material in An Introduction to Statistical Learning, with Applications in Python by James, Witten, Hastie, Tibshirani and Taylor (Springer, 2023). The pdf for this book is available for free on the book website.  What You'll Learn: Overview of statistical learning Linear regression Classification Resampling methods Linear model selection and regularization Moving beyond linearity Tree-based methods Support vector machines Deep learning Survival modeling Unsupervised learning Multiple testing|giCZkipHEmA
29|0|30|Statistical Learning: 4.Py Logistic Regression I 2023|2023-12-05T18:59:22Z|This is an introductory-level course in supervised learning, with a focus on regression and classification methods. The syllabus includes: linear and polynomial regression, logistic regression and linear discriminant analysis; cross-validation and the bootstrap, model selection and regularization methods (ridge and lasso); nonlinear models, splines and generalized additive models; tree-based methods, random forests and boosting; support-vector machines; neural networks and deep learning; survival models; multiple testing. Some unsupervised learning methods are discussed: principal components and clustering (k-means and hierarchical).  This is not a math-heavy class, so we try and describe the methods without heavy reliance on formulas and complex mathematics. We focus on what we consider to be the important elements of modern data science. Computing is done in Python. There are lectures devoted to Python, giving tutorials from the ground up, and progressing with more detailed sessions that implement the techniques in each chapter.  The lectures cover all the material in An Introduction to Statistical Learning, with Applications in Python by James, Witten, Hastie, Tibshirani and Taylor (Springer, 2023). The pdf for this book is available for free on the book website.  What You'll Learn: Overview of statistical learning Linear regression Classification Resampling methods Linear model selection and regularization Moving beyond linearity Tree-based methods Support vector machines Deep learning Survival modeling Unsupervised learning Multiple testing|wOGBlLLuc4I
30|0|31|Statistical Learning: 4.Py Linear Discriminant Analysis (LDA) I 2023|2023-12-05T18:59:36Z|This is an introductory-level course in supervised learning, with a focus on regression and classification methods. The syllabus includes: linear and polynomial regression, logistic regression and linear discriminant analysis; cross-validation and the bootstrap, model selection and regularization methods (ridge and lasso); nonlinear models, splines and generalized additive models; tree-based methods, random forests and boosting; support-vector machines; neural networks and deep learning; survival models; multiple testing. Some unsupervised learning methods are discussed: principal components and clustering (k-means and hierarchical).  This is not a math-heavy class, so we try and describe the methods without heavy reliance on formulas and complex mathematics. We focus on what we consider to be the important elements of modern data science. Computing is done in Python. There are lectures devoted to Python, giving tutorials from the ground up, and progressing with more detailed sessions that implement the techniques in each chapter.  The lectures cover all the material in An Introduction to Statistical Learning, with Applications in Python by James, Witten, Hastie, Tibshirani and Taylor (Springer, 2023). The pdf for this book is available for free on the book website.  What You'll Learn: Overview of statistical learning Linear regression Classification Resampling methods Linear model selection and regularization Moving beyond linearity Tree-based methods Support vector machines Deep learning Survival modeling Unsupervised learning Multiple testing|9pfRgzM1oCQ
31|0|32|Statistical Learning: 4.Py K-Nearest Neighbors (KNN) I 2023|2023-12-05T18:59:43Z|This is an introductory-level course in supervised learning, with a focus on regression and classification methods. The syllabus includes: linear and polynomial regression, logistic regression and linear discriminant analysis; cross-validation and the bootstrap, model selection and regularization methods (ridge and lasso); nonlinear models, splines and generalized additive models; tree-based methods, random forests and boosting; support-vector machines; neural networks and deep learning; survival models; multiple testing. Some unsupervised learning methods are discussed: principal components and clustering (k-means and hierarchical).  This is not a math-heavy class, so we try and describe the methods without heavy reliance on formulas and complex mathematics. We focus on what we consider to be the important elements of modern data science. Computing is done in Python. There are lectures devoted to Python, giving tutorials from the ground up, and progressing with more detailed sessions that implement the techniques in each chapter.  The lectures cover all the material in An Introduction to Statistical Learning, with Applications in Python by James, Witten, Hastie, Tibshirani and Taylor (Springer, 2023). The pdf for this book is available for free on the book website.  What You'll Learn: Overview of statistical learning Linear regression Classification Resampling methods Linear model selection and regularization Moving beyond linearity Tree-based methods Support vector machines Deep learning Survival modeling Unsupervised learning Multiple testing|yLEx1FnYyOo
32|0|33|Statistical Learning: 5.1 Cross Validation|2022-10-07T17:06:50Z|This is an introductory-level course in supervised learning, with a focus on regression and classification methods. The syllabus includes: linear and polynomial regression, logistic regression and linear discriminant analysis; cross-validation and the bootstrap, model selection and regularization methods (ridge and lasso); nonlinear models, splines and generalized additive models; tree-based methods, random forests and boosting; support-vector machines; neural networks and deep learning; survival models; multiple testing. Some unsupervised learning methods are discussed: principal components and clustering (k-means and hierarchical).  This is not a math-heavy class, so we try and describe the methods without heavy reliance on formulas and complex mathematics. We focus on what we consider to be the important elements of modern data science. Computing is done in Python. There are lectures devoted to Python, giving tutorials from the ground up, and progressing with more detailed sessions that implement the techniques in each chapter.  The lectures cover all the material in An Introduction to Statistical Learning, with Applications in Python by James, Witten, Hastie, Tibshirani and Taylor (Springer, 2023). The pdf for this book is available for free on the book website.  What You'll Learn: Overview of statistical learning Linear regression Classification Resampling methods Linear model selection and regularization Moving beyond linearity Tree-based methods Support vector machines Deep learning Survival modeling Unsupervised learning Multiple testing|6eWODQJrMKs
33|0|34|Statistical Learning: 5.2 K-fold Cross Validation|2022-10-07T17:07:06Z|This is an introductory-level course in supervised learning, with a focus on regression and classification methods. The syllabus includes: linear and polynomial regression, logistic regression and linear discriminant analysis; cross-validation and the bootstrap, model selection and regularization methods (ridge and lasso); nonlinear models, splines and generalized additive models; tree-based methods, random forests and boosting; support-vector machines; neural networks and deep learning; survival models; multiple testing. Some unsupervised learning methods are discussed: principal components and clustering (k-means and hierarchical).  This is not a math-heavy class, so we try and describe the methods without heavy reliance on formulas and complex mathematics. We focus on what we consider to be the important elements of modern data science. Computing is done in Python. There are lectures devoted to Python, giving tutorials from the ground up, and progressing with more detailed sessions that implement the techniques in each chapter.  The lectures cover all the material in An Introduction to Statistical Learning, with Applications in Python by James, Witten, Hastie, Tibshirani and Taylor (Springer, 2023). The pdf for this book is available for free on the book website.  What You'll Learn: Overview of statistical learning Linear regression Classification Resampling methods Linear model selection and regularization Moving beyond linearity Tree-based methods Support vector machines Deep learning Survival modeling Unsupervised learning Multiple testing|AMfvd_hLssE
34|0|35|Statistical Learning: 5.3 Cross Validation the wrong and right way|2022-10-07T17:07:22Z|This is an introductory-level course in supervised learning, with a focus on regression and classification methods. The syllabus includes: linear and polynomial regression, logistic regression and linear discriminant analysis; cross-validation and the bootstrap, model selection and regularization methods (ridge and lasso); nonlinear models, splines and generalized additive models; tree-based methods, random forests and boosting; support-vector machines; neural networks and deep learning; survival models; multiple testing. Some unsupervised learning methods are discussed: principal components and clustering (k-means and hierarchical).  This is not a math-heavy class, so we try and describe the methods without heavy reliance on formulas and complex mathematics. We focus on what we consider to be the important elements of modern data science. Computing is done in Python. There are lectures devoted to Python, giving tutorials from the ground up, and progressing with more detailed sessions that implement the techniques in each chapter.  The lectures cover all the material in An Introduction to Statistical Learning, with Applications in Python by James, Witten, Hastie, Tibshirani and Taylor (Springer, 2023). The pdf for this book is available for free on the book website.  What You'll Learn: Overview of statistical learning Linear regression Classification Resampling methods Linear model selection and regularization Moving beyond linearity Tree-based methods Support vector machines Deep learning Survival modeling Unsupervised learning Multiple testing|jgoa28FR__Y
35|0|36|Statistical Learning: 5.4 The Bootstrap|2022-10-07T17:07:37Z|This is an introductory-level course in supervised learning, with a focus on regression and classification methods. The syllabus includes: linear and polynomial regression, logistic regression and linear discriminant analysis; cross-validation and the bootstrap, model selection and regularization methods (ridge and lasso); nonlinear models, splines and generalized additive models; tree-based methods, random forests and boosting; support-vector machines; neural networks and deep learning; survival models; multiple testing. Some unsupervised learning methods are discussed: principal components and clustering (k-means and hierarchical).  This is not a math-heavy class, so we try and describe the methods without heavy reliance on formulas and complex mathematics. We focus on what we consider to be the important elements of modern data science. Computing is done in Python. There are lectures devoted to Python, giving tutorials from the ground up, and progressing with more detailed sessions that implement the techniques in each chapter.  The lectures cover all the material in An Introduction to Statistical Learning, with Applications in Python by James, Witten, Hastie, Tibshirani and Taylor (Springer, 2023). The pdf for this book is available for free on the book website.  What You'll Learn: Overview of statistical learning Linear regression Classification Resampling methods Linear model selection and regularization Moving beyond linearity Tree-based methods Support vector machines Deep learning Survival modeling Unsupervised learning Multiple testing|h_LweqiIotE
36|0|37|Statistical Learning: 5.5 More on the Bootstrap|2022-10-07T17:07:52Z|This is an introductory-level course in supervised learning, with a focus on regression and classification methods. The syllabus includes: linear and polynomial regression, logistic regression and linear discriminant analysis; cross-validation and the bootstrap, model selection and regularization methods (ridge and lasso); nonlinear models, splines and generalized additive models; tree-based methods, random forests and boosting; support-vector machines; neural networks and deep learning; survival models; multiple testing. Some unsupervised learning methods are discussed: principal components and clustering (k-means and hierarchical).  This is not a math-heavy class, so we try and describe the methods without heavy reliance on formulas and complex mathematics. We focus on what we consider to be the important elements of modern data science. Computing is done in Python. There are lectures devoted to Python, giving tutorials from the ground up, and progressing with more detailed sessions that implement the techniques in each chapter.  The lectures cover all the material in An Introduction to Statistical Learning, with Applications in Python by James, Witten, Hastie, Tibshirani and Taylor (Springer, 2023). The pdf for this book is available for free on the book website.  What You'll Learn: Overview of statistical learning Linear regression Classification Resampling methods Linear model selection and regularization Moving beyond linearity Tree-based methods Support vector machines Deep learning Survival modeling Unsupervised learning Multiple testing|OKREmw6YP64
37|0|38|Statistical Learning: 5.Py Cross-Validation I 2023|2023-12-05T18:59:48Z|This is an introductory-level course in supervised learning, with a focus on regression and classification methods. The syllabus includes: linear and polynomial regression, logistic regression and linear discriminant analysis; cross-validation and the bootstrap, model selection and regularization methods (ridge and lasso); nonlinear models, splines and generalized additive models; tree-based methods, random forests and boosting; support-vector machines; neural networks and deep learning; survival models; multiple testing. Some unsupervised learning methods are discussed: principal components and clustering (k-means and hierarchical).  This is not a math-heavy class, so we try and describe the methods without heavy reliance on formulas and complex mathematics. We focus on what we consider to be the important elements of modern data science. Computing is done in Python. There are lectures devoted to Python, giving tutorials from the ground up, and progressing with more detailed sessions that implement the techniques in each chapter.  The lectures cover all the material in An Introduction to Statistical Learning, with Applications in Python by James, Witten, Hastie, Tibshirani and Taylor (Springer, 2023). The pdf for this book is available for free on the book website.  What You'll Learn: Overview of statistical learning Linear regression Classification Resampling methods Linear model selection and regularization Moving beyond linearity Tree-based methods Support vector machines Deep learning Survival modeling Unsupervised learning Multiple testing|oYzo95sFoSY
38|0|39|Statistical Learning: 5.Py Bootstrap I 2023|2023-12-05T18:59:54Z|This is an introductory-level course in supervised learning, with a focus on regression and classification methods. The syllabus includes: linear and polynomial regression, logistic regression and linear discriminant analysis; cross-validation and the bootstrap, model selection and regularization methods (ridge and lasso); nonlinear models, splines and generalized additive models; tree-based methods, random forests and boosting; support-vector machines; neural networks and deep learning; survival models; multiple testing. Some unsupervised learning methods are discussed: principal components and clustering (k-means and hierarchical).  This is not a math-heavy class, so we try and describe the methods without heavy reliance on formulas and complex mathematics. We focus on what we consider to be the important elements of modern data science. Computing is done in Python. There are lectures devoted to Python, giving tutorials from the ground up, and progressing with more detailed sessions that implement the techniques in each chapter.  The lectures cover all the material in An Introduction to Statistical Learning, with Applications in Python by James, Witten, Hastie, Tibshirani and Taylor (Springer, 2023). The pdf for this book is available for free on the book website.  What You'll Learn: Overview of statistical learning Linear regression Classification Resampling methods Linear model selection and regularization Moving beyond linearity Tree-based methods Support vector machines Deep learning Survival modeling Unsupervised learning Multiple testing|1BxtC6ZOvJQ
39|0|40|Statistical Learning: 6.1 Introduction and Best Subset Selection|2022-10-07T17:08:37Z|This is an introductory-level course in supervised learning, with a focus on regression and classification methods. The syllabus includes: linear and polynomial regression, logistic regression and linear discriminant analysis; cross-validation and the bootstrap, model selection and regularization methods (ridge and lasso); nonlinear models, splines and generalized additive models; tree-based methods, random forests and boosting; support-vector machines; neural networks and deep learning; survival models; multiple testing. Some unsupervised learning methods are discussed: principal components and clustering (k-means and hierarchical).  This is not a math-heavy class, so we try and describe the methods without heavy reliance on formulas and complex mathematics. We focus on what we consider to be the important elements of modern data science. Computing is done in Python. There are lectures devoted to Python, giving tutorials from the ground up, and progressing with more detailed sessions that implement the techniques in each chapter.  The lectures cover all the material in An Introduction to Statistical Learning, with Applications in Python by James, Witten, Hastie, Tibshirani and Taylor (Springer, 2023). The pdf for this book is available for free on the book website.  What You'll Learn: Overview of statistical learning Linear regression Classification Resampling methods Linear model selection and regularization Moving beyond linearity Tree-based methods Support vector machines Deep learning Survival modeling Unsupervised learning Multiple testing|nsv5rEV3mVI
40|0|41|Statistical Learning: 6.2 Stepwise Selection|2022-10-07T17:09:03Z|This is an introductory-level course in supervised learning, with a focus on regression and classification methods. The syllabus includes: linear and polynomial regression, logistic regression and linear discriminant analysis; cross-validation and the bootstrap, model selection and regularization methods (ridge and lasso); nonlinear models, splines and generalized additive models; tree-based methods, random forests and boosting; support-vector machines; neural networks and deep learning; survival models; multiple testing. Some unsupervised learning methods are discussed: principal components and clustering (k-means and hierarchical).  This is not a math-heavy class, so we try and describe the methods without heavy reliance on formulas and complex mathematics. We focus on what we consider to be the important elements of modern data science. Computing is done in Python. There are lectures devoted to Python, giving tutorials from the ground up, and progressing with more detailed sessions that implement the techniques in each chapter.  The lectures cover all the material in An Introduction to Statistical Learning, with Applications in Python by James, Witten, Hastie, Tibshirani and Taylor (Springer, 2023). The pdf for this book is available for free on the book website.  What You'll Learn: Overview of statistical learning Linear regression Classification Resampling methods Linear model selection and regularization Moving beyond linearity Tree-based methods Support vector machines Deep learning Survival modeling Unsupervised learning Multiple testing|ynXq-Gw1xfE
41|0|42|Statistical Learning: 6.3 Backward stepwise selection|2022-10-07T17:09:21Z|This is an introductory-level course in supervised learning, with a focus on regression and classification methods. The syllabus includes: linear and polynomial regression, logistic regression and linear discriminant analysis; cross-validation and the bootstrap, model selection and regularization methods (ridge and lasso); nonlinear models, splines and generalized additive models; tree-based methods, random forests and boosting; support-vector machines; neural networks and deep learning; survival models; multiple testing. Some unsupervised learning methods are discussed: principal components and clustering (k-means and hierarchical).  This is not a math-heavy class, so we try and describe the methods without heavy reliance on formulas and complex mathematics. We focus on what we consider to be the important elements of modern data science. Computing is done in Python. There are lectures devoted to Python, giving tutorials from the ground up, and progressing with more detailed sessions that implement the techniques in each chapter.  The lectures cover all the material in An Introduction to Statistical Learning, with Applications in Python by James, Witten, Hastie, Tibshirani and Taylor (Springer, 2023). The pdf for this book is available for free on the book website.  What You'll Learn: Overview of statistical learning Linear regression Classification Resampling methods Linear model selection and regularization Moving beyond linearity Tree-based methods Support vector machines Deep learning Survival modeling Unsupervised learning Multiple testing|c5aI9cowjRI
42|0|43|Statistical Learning: 6.4 Estimating test error|2022-10-07T17:09:36Z|This is an introductory-level course in supervised learning, with a focus on regression and classification methods. The syllabus includes: linear and polynomial regression, logistic regression and linear discriminant analysis; cross-validation and the bootstrap, model selection and regularization methods (ridge and lasso); nonlinear models, splines and generalized additive models; tree-based methods, random forests and boosting; support-vector machines; neural networks and deep learning; survival models; multiple testing. Some unsupervised learning methods are discussed: principal components and clustering (k-means and hierarchical).  This is not a math-heavy class, so we try and describe the methods without heavy reliance on formulas and complex mathematics. We focus on what we consider to be the important elements of modern data science. Computing is done in Python. There are lectures devoted to Python, giving tutorials from the ground up, and progressing with more detailed sessions that implement the techniques in each chapter.  The lectures cover all the material in An Introduction to Statistical Learning, with Applications in Python by James, Witten, Hastie, Tibshirani and Taylor (Springer, 2023). The pdf for this book is available for free on the book website.  What You'll Learn: Overview of statistical learning Linear regression Classification Resampling methods Linear model selection and regularization Moving beyond linearity Tree-based methods Support vector machines Deep learning Survival modeling Unsupervised learning Multiple testing|48P-oV6cH44
43|0|44|Statistical Learning: 6.5 Validation and cross validation|2022-10-07T17:10:56Z|This is an introductory-level course in supervised learning, with a focus on regression and classification methods. The syllabus includes: linear and polynomial regression, logistic regression and linear discriminant analysis; cross-validation and the bootstrap, model selection and regularization methods (ridge and lasso); nonlinear models, splines and generalized additive models; tree-based methods, random forests and boosting; support-vector machines; neural networks and deep learning; survival models; multiple testing. Some unsupervised learning methods are discussed: principal components and clustering (k-means and hierarchical).  This is not a math-heavy class, so we try and describe the methods without heavy reliance on formulas and complex mathematics. We focus on what we consider to be the important elements of modern data science. Computing is done in Python. There are lectures devoted to Python, giving tutorials from the ground up, and progressing with more detailed sessions that implement the techniques in each chapter.  The lectures cover all the material in An Introduction to Statistical Learning, with Applications in Python by James, Witten, Hastie, Tibshirani and Taylor (Springer, 2023). The pdf for this book is available for free on the book website.  What You'll Learn: Overview of statistical learning Linear regression Classification Resampling methods Linear model selection and regularization Moving beyond linearity Tree-based methods Support vector machines Deep learning Survival modeling Unsupervised learning Multiple testing|mzb5Xs58bb0
44|0|45|Statistical Learning: 6.6 Shrinkage methods and ridge regression|2022-10-07T17:11:16Z|This is an introductory-level course in supervised learning, with a focus on regression and classification methods. The syllabus includes: linear and polynomial regression, logistic regression and linear discriminant analysis; cross-validation and the bootstrap, model selection and regularization methods (ridge and lasso); nonlinear models, splines and generalized additive models; tree-based methods, random forests and boosting; support-vector machines; neural networks and deep learning; survival models; multiple testing. Some unsupervised learning methods are discussed: principal components and clustering (k-means and hierarchical).  This is not a math-heavy class, so we try and describe the methods without heavy reliance on formulas and complex mathematics. We focus on what we consider to be the important elements of modern data science. Computing is done in Python. There are lectures devoted to Python, giving tutorials from the ground up, and progressing with more detailed sessions that implement the techniques in each chapter.  The lectures cover all the material in An Introduction to Statistical Learning, with Applications in Python by James, Witten, Hastie, Tibshirani and Taylor (Springer, 2023). The pdf for this book is available for free on the book website.  What You'll Learn: Overview of statistical learning Linear regression Classification Resampling methods Linear model selection and regularization Moving beyond linearity Tree-based methods Support vector machines Deep learning Survival modeling Unsupervised learning Multiple testing|lLlG5xkyqIA
45|0|46|Statistical Learning: 6.7 The Lasso|2022-10-07T17:11:31Z|This is an introductory-level course in supervised learning, with a focus on regression and classification methods. The syllabus includes: linear and polynomial regression, logistic regression and linear discriminant analysis; cross-validation and the bootstrap, model selection and regularization methods (ridge and lasso); nonlinear models, splines and generalized additive models; tree-based methods, random forests and boosting; support-vector machines; neural networks and deep learning; survival models; multiple testing. Some unsupervised learning methods are discussed: principal components and clustering (k-means and hierarchical).  This is not a math-heavy class, so we try and describe the methods without heavy reliance on formulas and complex mathematics. We focus on what we consider to be the important elements of modern data science. Computing is done in Python. There are lectures devoted to Python, giving tutorials from the ground up, and progressing with more detailed sessions that implement the techniques in each chapter.  The lectures cover all the material in An Introduction to Statistical Learning, with Applications in Python by James, Witten, Hastie, Tibshirani and Taylor (Springer, 2023). The pdf for this book is available for free on the book website.  What You'll Learn: Overview of statistical learning Linear regression Classification Resampling methods Linear model selection and regularization Moving beyond linearity Tree-based methods Support vector machines Deep learning Survival modeling Unsupervised learning Multiple testing|0tfPuddPhEY
46|0|47|Statistical Learning: 6.8 Tuning parameter selection|2022-10-07T17:11:51Z|This is an introductory-level course in supervised learning, with a focus on regression and classification methods. The syllabus includes: linear and polynomial regression, logistic regression and linear discriminant analysis; cross-validation and the bootstrap, model selection and regularization methods (ridge and lasso); nonlinear models, splines and generalized additive models; tree-based methods, random forests and boosting; support-vector machines; neural networks and deep learning; survival models; multiple testing. Some unsupervised learning methods are discussed: principal components and clustering (k-means and hierarchical).  This is not a math-heavy class, so we try and describe the methods without heavy reliance on formulas and complex mathematics. We focus on what we consider to be the important elements of modern data science. Computing is done in Python. There are lectures devoted to Python, giving tutorials from the ground up, and progressing with more detailed sessions that implement the techniques in each chapter.  The lectures cover all the material in An Introduction to Statistical Learning, with Applications in Python by James, Witten, Hastie, Tibshirani and Taylor (Springer, 2023). The pdf for this book is available for free on the book website.  What You'll Learn: Overview of statistical learning Linear regression Classification Resampling methods Linear model selection and regularization Moving beyond linearity Tree-based methods Support vector machines Deep learning Survival modeling Unsupervised learning Multiple testing|KV1Kt6I8rYs
47|0|48|Statistical Learning: 6.9 Dimension Reduction Methods|2022-10-07T17:12:08Z|This is an introductory-level course in supervised learning, with a focus on regression and classification methods. The syllabus includes: linear and polynomial regression, logistic regression and linear discriminant analysis; cross-validation and the bootstrap, model selection and regularization methods (ridge and lasso); nonlinear models, splines and generalized additive models; tree-based methods, random forests and boosting; support-vector machines; neural networks and deep learning; survival models; multiple testing. Some unsupervised learning methods are discussed: principal components and clustering (k-means and hierarchical).  This is not a math-heavy class, so we try and describe the methods without heavy reliance on formulas and complex mathematics. We focus on what we consider to be the important elements of modern data science. Computing is done in Python. There are lectures devoted to Python, giving tutorials from the ground up, and progressing with more detailed sessions that implement the techniques in each chapter.  The lectures cover all the material in An Introduction to Statistical Learning, with Applications in Python by James, Witten, Hastie, Tibshirani and Taylor (Springer, 2023). The pdf for this book is available for free on the book website.  What You'll Learn: Overview of statistical learning Linear regression Classification Resampling methods Linear model selection and regularization Moving beyond linearity Tree-based methods Support vector machines Deep learning Survival modeling Unsupervised learning Multiple testing|bpto4g5l_go
48|0|49|Statistical Learning: 6.10 Principal Components Regression and Partial Least Squares|2022-10-07T17:12:23Z|This is an introductory-level course in supervised learning, with a focus on regression and classification methods. The syllabus includes: linear and polynomial regression, logistic regression and linear discriminant analysis; cross-validation and the bootstrap, model selection and regularization methods (ridge and lasso); nonlinear models, splines and generalized additive models; tree-based methods, random forests and boosting; support-vector machines; neural networks and deep learning; survival models; multiple testing. Some unsupervised learning methods are discussed: principal components and clustering (k-means and hierarchical).  This is not a math-heavy class, so we try and describe the methods without heavy reliance on formulas and complex mathematics. We focus on what we consider to be the important elements of modern data science. Computing is done in Python. There are lectures devoted to Python, giving tutorials from the ground up, and progressing with more detailed sessions that implement the techniques in each chapter.  The lectures cover all the material in An Introduction to Statistical Learning, with Applications in Python by James, Witten, Hastie, Tibshirani and Taylor (Springer, 2023). The pdf for this book is available for free on the book website.  What You'll Learn: Overview of statistical learning Linear regression Classification Resampling methods Linear model selection and regularization Moving beyond linearity Tree-based methods Support vector machines Deep learning Survival modeling Unsupervised learning Multiple testing|Uo19ST0IEZI
49|0|50|Statistical Learning: 6.Py Stepwise Regression I 2023|2023-12-05T19:00:02Z|This is an introductory-level course in supervised learning, with a focus on regression and classification methods. The syllabus includes: linear and polynomial regression, logistic regression and linear discriminant analysis; cross-validation and the bootstrap, model selection and regularization methods (ridge and lasso); nonlinear models, splines and generalized additive models; tree-based methods, random forests and boosting; support-vector machines; neural networks and deep learning; survival models; multiple testing. Some unsupervised learning methods are discussed: principal components and clustering (k-means and hierarchical).  This is not a math-heavy class, so we try and describe the methods without heavy reliance on formulas and complex mathematics. We focus on what we consider to be the important elements of modern data science. Computing is done in Python. There are lectures devoted to Python, giving tutorials from the ground up, and progressing with more detailed sessions that implement the techniques in each chapter.  The lectures cover all the material in An Introduction to Statistical Learning, with Applications in Python by James, Witten, Hastie, Tibshirani and Taylor (Springer, 2023). The pdf for this book is available for free on the book website.  What You'll Learn: Overview of statistical learning Linear regression Classification Resampling methods Linear model selection and regularization Moving beyond linearity Tree-based methods Support vector machines Deep learning Survival modeling Unsupervised learning Multiple testing|EJ-haQArGzU
50|0|51|Statistical Learning: 6.Py Ridge Regression and the Lasso I 2023|2023-12-05T19:00:12Z|This is an introductory-level course in supervised learning, with a focus on regression and classification methods. The syllabus includes: linear and polynomial regression, logistic regression and linear discriminant analysis; cross-validation and the bootstrap, model selection and regularization methods (ridge and lasso); nonlinear models, splines and generalized additive models; tree-based methods, random forests and boosting; support-vector machines; neural networks and deep learning; survival models; multiple testing. Some unsupervised learning methods are discussed: principal components and clustering (k-means and hierarchical).  This is not a math-heavy class, so we try and describe the methods without heavy reliance on formulas and complex mathematics. We focus on what we consider to be the important elements of modern data science. Computing is done in Python. There are lectures devoted to Python, giving tutorials from the ground up, and progressing with more detailed sessions that implement the techniques in each chapter.  The lectures cover all the material in An Introduction to Statistical Learning, with Applications in Python by James, Witten, Hastie, Tibshirani and Taylor (Springer, 2023). The pdf for this book is available for free on the book website.  What You'll Learn: Overview of statistical learning Linear regression Classification Resampling methods Linear model selection and regularization Moving beyond linearity Tree-based methods Support vector machines Deep learning Survival modeling Unsupervised learning Multiple testing|dI7xi5kRwwM
51|0|52|Statistical Learning: 7.1 Polynomials and Step Functions|2022-10-07T17:13:43Z|This is an introductory-level course in supervised learning, with a focus on regression and classification methods. The syllabus includes: linear and polynomial regression, logistic regression and linear discriminant analysis; cross-validation and the bootstrap, model selection and regularization methods (ridge and lasso); nonlinear models, splines and generalized additive models; tree-based methods, random forests and boosting; support-vector machines; neural networks and deep learning; survival models; multiple testing. Some unsupervised learning methods are discussed: principal components and clustering (k-means and hierarchical).  This is not a math-heavy class, so we try and describe the methods without heavy reliance on formulas and complex mathematics. We focus on what we consider to be the important elements of modern data science. Computing is done in Python. There are lectures devoted to Python, giving tutorials from the ground up, and progressing with more detailed sessions that implement the techniques in each chapter.  The lectures cover all the material in An Introduction to Statistical Learning, with Applications in Python by James, Witten, Hastie, Tibshirani and Taylor (Springer, 2023). The pdf for this book is available for free on the book website.  What You'll Learn: Overview of statistical learning Linear regression Classification Resampling methods Linear model selection and regularization Moving beyond linearity Tree-based methods Support vector machines Deep learning Survival modeling Unsupervised learning Multiple testing|F-D3lZzYn50
52|0|53|Statistical Learning: 7.2 Piecewise Polynomials and Splines|2022-10-07T17:14:00Z|This is an introductory-level course in supervised learning, with a focus on regression and classification methods. The syllabus includes: linear and polynomial regression, logistic regression and linear discriminant analysis; cross-validation and the bootstrap, model selection and regularization methods (ridge and lasso); nonlinear models, splines and generalized additive models; tree-based methods, random forests and boosting; support-vector machines; neural networks and deep learning; survival models; multiple testing. Some unsupervised learning methods are discussed: principal components and clustering (k-means and hierarchical).  This is not a math-heavy class, so we try and describe the methods without heavy reliance on formulas and complex mathematics. We focus on what we consider to be the important elements of modern data science. Computing is done in Python. There are lectures devoted to Python, giving tutorials from the ground up, and progressing with more detailed sessions that implement the techniques in each chapter.  The lectures cover all the material in An Introduction to Statistical Learning, with Applications in Python by James, Witten, Hastie, Tibshirani and Taylor (Springer, 2023). The pdf for this book is available for free on the book website.  What You'll Learn: Overview of statistical learning Linear regression Classification Resampling methods Linear model selection and regularization Moving beyond linearity Tree-based methods Support vector machines Deep learning Survival modeling Unsupervised learning Multiple testing|FrVaxvwCLYM
53|0|54|Statistical Learning: 7.3 Smoothing Splines|2022-10-07T17:14:15Z|This is an introductory-level course in supervised learning, with a focus on regression and classification methods. The syllabus includes: linear and polynomial regression, logistic regression and linear discriminant analysis; cross-validation and the bootstrap, model selection and regularization methods (ridge and lasso); nonlinear models, splines and generalized additive models; tree-based methods, random forests and boosting; support-vector machines; neural networks and deep learning; survival models; multiple testing. Some unsupervised learning methods are discussed: principal components and clustering (k-means and hierarchical).  This is not a math-heavy class, so we try and describe the methods without heavy reliance on formulas and complex mathematics. We focus on what we consider to be the important elements of modern data science. Computing is done in Python. There are lectures devoted to Python, giving tutorials from the ground up, and progressing with more detailed sessions that implement the techniques in each chapter.  The lectures cover all the material in An Introduction to Statistical Learning, with Applications in Python by James, Witten, Hastie, Tibshirani and Taylor (Springer, 2023). The pdf for this book is available for free on the book website.  What You'll Learn: Overview of statistical learning Linear regression Classification Resampling methods Linear model selection and regularization Moving beyond linearity Tree-based methods Support vector machines Deep learning Survival modeling Unsupervised learning Multiple testing|b_HSFOnrGLI
54|0|55|Statistical Learning: 7.4 Generalized Additive Models and Local Regression|2022-10-07T17:14:30Z|This is an introductory-level course in supervised learning, with a focus on regression and classification methods. The syllabus includes: linear and polynomial regression, logistic regression and linear discriminant analysis; cross-validation and the bootstrap, model selection and regularization methods (ridge and lasso); nonlinear models, splines and generalized additive models; tree-based methods, random forests and boosting; support-vector machines; neural networks and deep learning; survival models; multiple testing. Some unsupervised learning methods are discussed: principal components and clustering (k-means and hierarchical).  This is not a math-heavy class, so we try and describe the methods without heavy reliance on formulas and complex mathematics. We focus on what we consider to be the important elements of modern data science. Computing is done in Python. There are lectures devoted to Python, giving tutorials from the ground up, and progressing with more detailed sessions that implement the techniques in each chapter.  The lectures cover all the material in An Introduction to Statistical Learning, with Applications in Python by James, Witten, Hastie, Tibshirani and Taylor (Springer, 2023). The pdf for this book is available for free on the book website.  What You'll Learn: Overview of statistical learning Linear regression Classification Resampling methods Linear model selection and regularization Moving beyond linearity Tree-based methods Support vector machines Deep learning Survival modeling Unsupervised learning Multiple testing|3aMB51GMUyQ
55|0|56|Statistical Learning: 7.Py Polynomial Regressions and Step Functions I 2023|2023-12-05T19:00:33Z|This is an introductory-level course in supervised learning, with a focus on regression and classification methods. The syllabus includes: linear and polynomial regression, logistic regression and linear discriminant analysis; cross-validation and the bootstrap, model selection and regularization methods (ridge and lasso); nonlinear models, splines and generalized additive models; tree-based methods, random forests and boosting; support-vector machines; neural networks and deep learning; survival models; multiple testing. Some unsupervised learning methods are discussed: principal components and clustering (k-means and hierarchical).  This is not a math-heavy class, so we try and describe the methods without heavy reliance on formulas and complex mathematics. We focus on what we consider to be the important elements of modern data science. Computing is done in Python. There are lectures devoted to Python, giving tutorials from the ground up, and progressing with more detailed sessions that implement the techniques in each chapter.  The lectures cover all the material in An Introduction to Statistical Learning, with Applications in Python by James, Witten, Hastie, Tibshirani and Taylor (Springer, 2023). The pdf for this book is available for free on the book website.  What You'll Learn: Overview of statistical learning Linear regression Classification Resampling methods Linear model selection and regularization Moving beyond linearity Tree-based methods Support vector machines Deep learning Survival modeling Unsupervised learning Multiple testing|_0omkfNiU2c
56|0|57|Statistical Learning: 7.Py Splines I 2023|2023-12-05T19:00:40Z|This is an introductory-level course in supervised learning, with a focus on regression and classification methods. The syllabus includes: linear and polynomial regression, logistic regression and linear discriminant analysis; cross-validation and the bootstrap, model selection and regularization methods (ridge and lasso); nonlinear models, splines and generalized additive models; tree-based methods, random forests and boosting; support-vector machines; neural networks and deep learning; survival models; multiple testing. Some unsupervised learning methods are discussed: principal components and clustering (k-means and hierarchical).  This is not a math-heavy class, so we try and describe the methods without heavy reliance on formulas and complex mathematics. We focus on what we consider to be the important elements of modern data science. Computing is done in Python. There are lectures devoted to Python, giving tutorials from the ground up, and progressing with more detailed sessions that implement the techniques in each chapter.  The lectures cover all the material in An Introduction to Statistical Learning, with Applications in Python by James, Witten, Hastie, Tibshirani and Taylor (Springer, 2023). The pdf for this book is available for free on the book website.  What You'll Learn: Overview of statistical learning Linear regression Classification Resampling methods Linear model selection and regularization Moving beyond linearity Tree-based methods Support vector machines Deep learning Survival modeling Unsupervised learning Multiple testing|C9h-o6AfNX0
57|0|58|Statistical Learning: 7.Py Generalized Additive Models (GAMs) I 2023|2023-12-05T18:59:31Z|This is an introductory-level course in supervised learning, with a focus on regression and classification methods. The syllabus includes: linear and polynomial regression, logistic regression and linear discriminant analysis; cross-validation and the bootstrap, model selection and regularization methods (ridge and lasso); nonlinear models, splines and generalized additive models; tree-based methods, random forests and boosting; support-vector machines; neural networks and deep learning; survival models; multiple testing. Some unsupervised learning methods are discussed: principal components and clustering (k-means and hierarchical).  This is not a math-heavy class, so we try and describe the methods without heavy reliance on formulas and complex mathematics. We focus on what we consider to be the important elements of modern data science. Computing is done in Python. There are lectures devoted to Python, giving tutorials from the ground up, and progressing with more detailed sessions that implement the techniques in each chapter.  The lectures cover all the material in An Introduction to Statistical Learning, with Applications in Python by James, Witten, Hastie, Tibshirani and Taylor (Springer, 2023). The pdf for this book is available for free on the book website.  What You'll Learn: Overview of statistical learning Linear regression Classification Resampling methods Linear model selection and regularization Moving beyond linearity Tree-based methods Support vector machines Deep learning Survival modeling Unsupervised learning Multiple testing|hQx84r2maaE
58|0|59|Statistical Learning: 8.1 Tree based methods|2022-10-07T17:15:18Z|This is an introductory-level course in supervised learning, with a focus on regression and classification methods. The syllabus includes: linear and polynomial regression, logistic regression and linear discriminant analysis; cross-validation and the bootstrap, model selection and regularization methods (ridge and lasso); nonlinear models, splines and generalized additive models; tree-based methods, random forests and boosting; support-vector machines; neural networks and deep learning; survival models; multiple testing. Some unsupervised learning methods are discussed: principal components and clustering (k-means and hierarchical).  This is not a math-heavy class, so we try and describe the methods without heavy reliance on formulas and complex mathematics. We focus on what we consider to be the important elements of modern data science. Computing is done in Python. There are lectures devoted to Python, giving tutorials from the ground up, and progressing with more detailed sessions that implement the techniques in each chapter.  The lectures cover all the material in An Introduction to Statistical Learning, with Applications in Python by James, Witten, Hastie, Tibshirani and Taylor (Springer, 2023). The pdf for this book is available for free on the book website.  What You'll Learn: Overview of statistical learning Linear regression Classification Resampling methods Linear model selection and regularization Moving beyond linearity Tree-based methods Support vector machines Deep learning Survival modeling Unsupervised learning Multiple testing|QNnayf--_yk
59|0|60|Statistical Learning: 8.2 More details on Trees|2022-10-07T17:16:11Z|This is an introductory-level course in supervised learning, with a focus on regression and classification methods. The syllabus includes: linear and polynomial regression, logistic regression and linear discriminant analysis; cross-validation and the bootstrap, model selection and regularization methods (ridge and lasso); nonlinear models, splines and generalized additive models; tree-based methods, random forests and boosting; support-vector machines; neural networks and deep learning; survival models; multiple testing. Some unsupervised learning methods are discussed: principal components and clustering (k-means and hierarchical).  This is not a math-heavy class, so we try and describe the methods without heavy reliance on formulas and complex mathematics. We focus on what we consider to be the important elements of modern data science. Computing is done in Python. There are lectures devoted to Python, giving tutorials from the ground up, and progressing with more detailed sessions that implement the techniques in each chapter.  The lectures cover all the material in An Introduction to Statistical Learning, with Applications in Python by James, Witten, Hastie, Tibshirani and Taylor (Springer, 2023). The pdf for this book is available for free on the book website.  What You'll Learn: Overview of statistical learning Linear regression Classification Resampling methods Linear model selection and regularization Moving beyond linearity Tree-based methods Support vector machines Deep learning Survival modeling Unsupervised learning Multiple testing|JaoTOfTNOVk
60|0|61|Statistical Learning: 8.3 Classification Trees|2022-10-07T17:16:37Z|This is an introductory-level course in supervised learning, with a focus on regression and classification methods. The syllabus includes: linear and polynomial regression, logistic regression and linear discriminant analysis; cross-validation and the bootstrap, model selection and regularization methods (ridge and lasso); nonlinear models, splines and generalized additive models; tree-based methods, random forests and boosting; support-vector machines; neural networks and deep learning; survival models; multiple testing. Some unsupervised learning methods are discussed: principal components and clustering (k-means and hierarchical).  This is not a math-heavy class, so we try and describe the methods without heavy reliance on formulas and complex mathematics. We focus on what we consider to be the important elements of modern data science. Computing is done in Python. There are lectures devoted to Python, giving tutorials from the ground up, and progressing with more detailed sessions that implement the techniques in each chapter.  The lectures cover all the material in An Introduction to Statistical Learning, with Applications in Python by James, Witten, Hastie, Tibshirani and Taylor (Springer, 2023). The pdf for this book is available for free on the book website.  What You'll Learn: Overview of statistical learning Linear regression Classification Resampling methods Linear model selection and regularization Moving beyond linearity Tree-based methods Support vector machines Deep learning Survival modeling Unsupervised learning Multiple testing|gLcfKSMKOb0
61|0|62|Statistical Learning: 8.4 Bagging|2022-10-07T17:17:00Z|This is an introductory-level course in supervised learning, with a focus on regression and classification methods. The syllabus includes: linear and polynomial regression, logistic regression and linear discriminant analysis; cross-validation and the bootstrap, model selection and regularization methods (ridge and lasso); nonlinear models, splines and generalized additive models; tree-based methods, random forests and boosting; support-vector machines; neural networks and deep learning; survival models; multiple testing. Some unsupervised learning methods are discussed: principal components and clustering (k-means and hierarchical).  This is not a math-heavy class, so we try and describe the methods without heavy reliance on formulas and complex mathematics. We focus on what we consider to be the important elements of modern data science. Computing is done in Python. There are lectures devoted to Python, giving tutorials from the ground up, and progressing with more detailed sessions that implement the techniques in each chapter.  The lectures cover all the material in An Introduction to Statistical Learning, with Applications in Python by James, Witten, Hastie, Tibshirani and Taylor (Springer, 2023). The pdf for this book is available for free on the book website.  What You'll Learn: Overview of statistical learning Linear regression Classification Resampling methods Linear model selection and regularization Moving beyond linearity Tree-based methods Support vector machines Deep learning Survival modeling Unsupervised learning Multiple testing|_cKAxjnInfA
62|0|63|Statistical Learning: 8.5 Boosting|2022-10-07T17:17:26Z|This is an introductory-level course in supervised learning, with a focus on regression and classification methods. The syllabus includes: linear and polynomial regression, logistic regression and linear discriminant analysis; cross-validation and the bootstrap, model selection and regularization methods (ridge and lasso); nonlinear models, splines and generalized additive models; tree-based methods, random forests and boosting; support-vector machines; neural networks and deep learning; survival models; multiple testing. Some unsupervised learning methods are discussed: principal components and clustering (k-means and hierarchical).  This is not a math-heavy class, so we try and describe the methods without heavy reliance on formulas and complex mathematics. We focus on what we consider to be the important elements of modern data science. Computing is done in Python. There are lectures devoted to Python, giving tutorials from the ground up, and progressing with more detailed sessions that implement the techniques in each chapter.  The lectures cover all the material in An Introduction to Statistical Learning, with Applications in Python by James, Witten, Hastie, Tibshirani and Taylor (Springer, 2023). The pdf for this book is available for free on the book website.  What You'll Learn: Overview of statistical learning Linear regression Classification Resampling methods Linear model selection and regularization Moving beyond linearity Tree-based methods Support vector machines Deep learning Survival modeling Unsupervised learning Multiple testing|cdl4C2eCOHk
63|0|64|Statistical Learning: 8.6 Bayesian Additive Regression Trees|2022-10-07T17:17:44Z|This is an introductory-level course in supervised learning, with a focus on regression and classification methods. The syllabus includes: linear and polynomial regression, logistic regression and linear discriminant analysis; cross-validation and the bootstrap, model selection and regularization methods (ridge and lasso); nonlinear models, splines and generalized additive models; tree-based methods, random forests and boosting; support-vector machines; neural networks and deep learning; survival models; multiple testing. Some unsupervised learning methods are discussed: principal components and clustering (k-means and hierarchical).  This is not a math-heavy class, so we try and describe the methods without heavy reliance on formulas and complex mathematics. We focus on what we consider to be the important elements of modern data science. Computing is done in Python. There are lectures devoted to Python, giving tutorials from the ground up, and progressing with more detailed sessions that implement the techniques in each chapter.  The lectures cover all the material in An Introduction to Statistical Learning, with Applications in Python by James, Witten, Hastie, Tibshirani and Taylor (Springer, 2023). The pdf for this book is available for free on the book website.  What You'll Learn: Overview of statistical learning Linear regression Classification Resampling methods Linear model selection and regularization Moving beyond linearity Tree-based methods Support vector machines Deep learning Survival modeling Unsupervised learning Multiple testing|xWhPwHZF4c0
64|0|65|Statistical Learning: 8.Py Tree-Based Methods I 2023|2023-12-05T19:00:53Z|This is an introductory-level course in supervised learning, with a focus on regression and classification methods. The syllabus includes: linear and polynomial regression, logistic regression and linear discriminant analysis; cross-validation and the bootstrap, model selection and regularization methods (ridge and lasso); nonlinear models, splines and generalized additive models; tree-based methods, random forests and boosting; support-vector machines; neural networks and deep learning; survival models; multiple testing. Some unsupervised learning methods are discussed: principal components and clustering (k-means and hierarchical).  This is not a math-heavy class, so we try and describe the methods without heavy reliance on formulas and complex mathematics. We focus on what we consider to be the important elements of modern data science. Computing is done in Python. There are lectures devoted to Python, giving tutorials from the ground up, and progressing with more detailed sessions that implement the techniques in each chapter.  The lectures cover all the material in An Introduction to Statistical Learning, with Applications in Python by James, Witten, Hastie, Tibshirani and Taylor (Springer, 2023). The pdf for this book is available for free on the book website.  What You'll Learn: Overview of statistical learning Linear regression Classification Resampling methods Linear model selection and regularization Moving beyond linearity Tree-based methods Support vector machines Deep learning Survival modeling Unsupervised learning Multiple testing|AVTfC5WnDTo
65|0|66|Statistical Learning: 9.1 Optimal Separating Hyperplane|2022-10-07T17:18:36Z|This is an introductory-level course in supervised learning, with a focus on regression and classification methods. The syllabus includes: linear and polynomial regression, logistic regression and linear discriminant analysis; cross-validation and the bootstrap, model selection and regularization methods (ridge and lasso); nonlinear models, splines and generalized additive models; tree-based methods, random forests and boosting; support-vector machines; neural networks and deep learning; survival models; multiple testing. Some unsupervised learning methods are discussed: principal components and clustering (k-means and hierarchical).  This is not a math-heavy class, so we try and describe the methods without heavy reliance on formulas and complex mathematics. We focus on what we consider to be the important elements of modern data science. Computing is done in Python. There are lectures devoted to Python, giving tutorials from the ground up, and progressing with more detailed sessions that implement the techniques in each chapter.  The lectures cover all the material in An Introduction to Statistical Learning, with Applications in Python by James, Witten, Hastie, Tibshirani and Taylor (Springer, 2023). The pdf for this book is available for free on the book website.  What You'll Learn: Overview of statistical learning Linear regression Classification Resampling methods Linear model selection and regularization Moving beyond linearity Tree-based methods Support vector machines Deep learning Survival modeling Unsupervised learning Multiple testing|Op0OyOuDjcQ
66|0|67|Statistical Learning: 9.2.Support Vector Classifier|2022-10-07T17:18:51Z|This is an introductory-level course in supervised learning, with a focus on regression and classification methods. The syllabus includes: linear and polynomial regression, logistic regression and linear discriminant analysis; cross-validation and the bootstrap, model selection and regularization methods (ridge and lasso); nonlinear models, splines and generalized additive models; tree-based methods, random forests and boosting; support-vector machines; neural networks and deep learning; survival models; multiple testing. Some unsupervised learning methods are discussed: principal components and clustering (k-means and hierarchical).  This is not a math-heavy class, so we try and describe the methods without heavy reliance on formulas and complex mathematics. We focus on what we consider to be the important elements of modern data science. Computing is done in Python. There are lectures devoted to Python, giving tutorials from the ground up, and progressing with more detailed sessions that implement the techniques in each chapter.  The lectures cover all the material in An Introduction to Statistical Learning, with Applications in Python by James, Witten, Hastie, Tibshirani and Taylor (Springer, 2023). The pdf for this book is available for free on the book website.  What You'll Learn: Overview of statistical learning Linear regression Classification Resampling methods Linear model selection and regularization Moving beyond linearity Tree-based methods Support vector machines Deep learning Survival modeling Unsupervised learning Multiple testing|pjvnCEfAswc
67|0|68|Statistical Learning: 9.3 Feature Expansion and the SVM|2022-10-07T17:19:08Z|This is an introductory-level course in supervised learning, with a focus on regression and classification methods. The syllabus includes: linear and polynomial regression, logistic regression and linear discriminant analysis; cross-validation and the bootstrap, model selection and regularization methods (ridge and lasso); nonlinear models, splines and generalized additive models; tree-based methods, random forests and boosting; support-vector machines; neural networks and deep learning; survival models; multiple testing. Some unsupervised learning methods are discussed: principal components and clustering (k-means and hierarchical).  This is not a math-heavy class, so we try and describe the methods without heavy reliance on formulas and complex mathematics. We focus on what we consider to be the important elements of modern data science. Computing is done in Python. There are lectures devoted to Python, giving tutorials from the ground up, and progressing with more detailed sessions that implement the techniques in each chapter.  The lectures cover all the material in An Introduction to Statistical Learning, with Applications in Python by James, Witten, Hastie, Tibshirani and Taylor (Springer, 2023). The pdf for this book is available for free on the book website.  What You'll Learn: Overview of statistical learning Linear regression Classification Resampling methods Linear model selection and regularization Moving beyond linearity Tree-based methods Support vector machines Deep learning Survival modeling Unsupervised learning Multiple testing|02icdqOJsH4
68|0|69|Statistical Learning: 9.4 Example and Comparison with Logistic Regression|2022-10-07T17:19:25Z|This is an introductory-level course in supervised learning, with a focus on regression and classification methods. The syllabus includes: linear and polynomial regression, logistic regression and linear discriminant analysis; cross-validation and the bootstrap, model selection and regularization methods (ridge and lasso); nonlinear models, splines and generalized additive models; tree-based methods, random forests and boosting; support-vector machines; neural networks and deep learning; survival models; multiple testing. Some unsupervised learning methods are discussed: principal components and clustering (k-means and hierarchical).  This is not a math-heavy class, so we try and describe the methods without heavy reliance on formulas and complex mathematics. We focus on what we consider to be the important elements of modern data science. Computing is done in Python. There are lectures devoted to Python, giving tutorials from the ground up, and progressing with more detailed sessions that implement the techniques in each chapter.  The lectures cover all the material in An Introduction to Statistical Learning, with Applications in Python by James, Witten, Hastie, Tibshirani and Taylor (Springer, 2023). The pdf for this book is available for free on the book website.  What You'll Learn: Overview of statistical learning Linear regression Classification Resampling methods Linear model selection and regularization Moving beyond linearity Tree-based methods Support vector machines Deep learning Survival modeling Unsupervised learning Multiple testing|m5d7-URGnVY
69|0|70|Statistical Learning: 9.Py Support Vector Machines I 2023|2023-12-05T19:01:29Z|This is an introductory-level course in supervised learning, with a focus on regression and classification methods. The syllabus includes: linear and polynomial regression, logistic regression and linear discriminant analysis; cross-validation and the bootstrap, model selection and regularization methods (ridge and lasso); nonlinear models, splines and generalized additive models; tree-based methods, random forests and boosting; support-vector machines; neural networks and deep learning; survival models; multiple testing. Some unsupervised learning methods are discussed: principal components and clustering (k-means and hierarchical).  This is not a math-heavy class, so we try and describe the methods without heavy reliance on formulas and complex mathematics. We focus on what we consider to be the important elements of modern data science. Computing is done in Python. There are lectures devoted to Python, giving tutorials from the ground up, and progressing with more detailed sessions that implement the techniques in each chapter.  The lectures cover all the material in An Introduction to Statistical Learning, with Applications in Python by James, Witten, Hastie, Tibshirani and Taylor (Springer, 2023). The pdf for this book is available for free on the book website.  What You'll Learn: Overview of statistical learning Linear regression Classification Resampling methods Linear model selection and regularization Moving beyond linearity Tree-based methods Support vector machines Deep learning Survival modeling Unsupervised learning Multiple testing|94wKFGS4Zm4
70|0|71|Statistical Learning: 9.Py ROC Curves I 2023|2023-12-05T19:01:07Z|This is an introductory-level course in supervised learning, with a focus on regression and classification methods. The syllabus includes: linear and polynomial regression, logistic regression and linear discriminant analysis; cross-validation and the bootstrap, model selection and regularization methods (ridge and lasso); nonlinear models, splines and generalized additive models; tree-based methods, random forests and boosting; support-vector machines; neural networks and deep learning; survival models; multiple testing. Some unsupervised learning methods are discussed: principal components and clustering (k-means and hierarchical).  This is not a math-heavy class, so we try and describe the methods without heavy reliance on formulas and complex mathematics. We focus on what we consider to be the important elements of modern data science. Computing is done in Python. There are lectures devoted to Python, giving tutorials from the ground up, and progressing with more detailed sessions that implement the techniques in each chapter.  The lectures cover all the material in An Introduction to Statistical Learning, with Applications in Python by James, Witten, Hastie, Tibshirani and Taylor (Springer, 2023). The pdf for this book is available for free on the book website.  What You'll Learn: Overview of statistical learning Linear regression Classification Resampling methods Linear model selection and regularization Moving beyond linearity Tree-based methods Support vector machines Deep learning Survival modeling Unsupervised learning Multiple testing|1fmHMmoa47g
71|0|72|Statistical Learning: 10.1 Introduction to Neural Networks|2022-10-07T17:20:11Z|This is an introductory-level course in supervised learning, with a focus on regression and classification methods. The syllabus includes: linear and polynomial regression, logistic regression and linear discriminant analysis; cross-validation and the bootstrap, model selection and regularization methods (ridge and lasso); nonlinear models, splines and generalized additive models; tree-based methods, random forests and boosting; support-vector machines; neural networks and deep learning; survival models; multiple testing. Some unsupervised learning methods are discussed: principal components and clustering (k-means and hierarchical).  This is not a math-heavy class, so we try and describe the methods without heavy reliance on formulas and complex mathematics. We focus on what we consider to be the important elements of modern data science. Computing is done in Python. There are lectures devoted to Python, giving tutorials from the ground up, and progressing with more detailed sessions that implement the techniques in each chapter.  The lectures cover all the material in An Introduction to Statistical Learning, with Applications in Python by James, Witten, Hastie, Tibshirani and Taylor (Springer, 2023). The pdf for this book is available for free on the book website.  What You'll Learn: Overview of statistical learning Linear regression Classification Resampling methods Linear model selection and regularization Moving beyond linearity Tree-based methods Support vector machines Deep learning Survival modeling Unsupervised learning Multiple testing|jJb2qytbcNg
72|0|73|Statistical Learning: 10.2 Convolutional Neural Networks|2022-10-07T17:20:27Z|This is an introductory-level course in supervised learning, with a focus on regression and classification methods. The syllabus includes: linear and polynomial regression, logistic regression and linear discriminant analysis; cross-validation and the bootstrap, model selection and regularization methods (ridge and lasso); nonlinear models, splines and generalized additive models; tree-based methods, random forests and boosting; support-vector machines; neural networks and deep learning; survival models; multiple testing. Some unsupervised learning methods are discussed: principal components and clustering (k-means and hierarchical).  This is not a math-heavy class, so we try and describe the methods without heavy reliance on formulas and complex mathematics. We focus on what we consider to be the important elements of modern data science. Computing is done in Python. There are lectures devoted to Python, giving tutorials from the ground up, and progressing with more detailed sessions that implement the techniques in each chapter.  The lectures cover all the material in An Introduction to Statistical Learning, with Applications in Python by James, Witten, Hastie, Tibshirani and Taylor (Springer, 2023). The pdf for this book is available for free on the book website.  What You'll Learn: Overview of statistical learning Linear regression Classification Resampling methods Linear model selection and regularization Moving beyond linearity Tree-based methods Support vector machines Deep learning Survival modeling Unsupervised learning Multiple testing|ggOZuZnA6is
73|0|74|Statistical Learning: 10.3 Document Classification|2022-10-07T17:20:42Z|This is an introductory-level course in supervised learning, with a focus on regression and classification methods. The syllabus includes: linear and polynomial regression, logistic regression and linear discriminant analysis; cross-validation and the bootstrap, model selection and regularization methods (ridge and lasso); nonlinear models, splines and generalized additive models; tree-based methods, random forests and boosting; support-vector machines; neural networks and deep learning; survival models; multiple testing. Some unsupervised learning methods are discussed: principal components and clustering (k-means and hierarchical).  This is not a math-heavy class, so we try and describe the methods without heavy reliance on formulas and complex mathematics. We focus on what we consider to be the important elements of modern data science. Computing is done in Python. There are lectures devoted to Python, giving tutorials from the ground up, and progressing with more detailed sessions that implement the techniques in each chapter.  The lectures cover all the material in An Introduction to Statistical Learning, with Applications in Python by James, Witten, Hastie, Tibshirani and Taylor (Springer, 2023). The pdf for this book is available for free on the book website.  What You'll Learn: Overview of statistical learning Linear regression Classification Resampling methods Linear model selection and regularization Moving beyond linearity Tree-based methods Support vector machines Deep learning Survival modeling Unsupervised learning Multiple testing|Zw3L-0ZP_DA
74|0|75|Statistical Learning: 10.4 Recurrent Neural Networks|2022-10-07T17:20:58Z|This is an introductory-level course in supervised learning, with a focus on regression and classification methods. The syllabus includes: linear and polynomial regression, logistic regression and linear discriminant analysis; cross-validation and the bootstrap, model selection and regularization methods (ridge and lasso); nonlinear models, splines and generalized additive models; tree-based methods, random forests and boosting; support-vector machines; neural networks and deep learning; survival models; multiple testing. Some unsupervised learning methods are discussed: principal components and clustering (k-means and hierarchical).  This is not a math-heavy class, so we try and describe the methods without heavy reliance on formulas and complex mathematics. We focus on what we consider to be the important elements of modern data science. Computing is done in Python. There are lectures devoted to Python, giving tutorials from the ground up, and progressing with more detailed sessions that implement the techniques in each chapter.  The lectures cover all the material in An Introduction to Statistical Learning, with Applications in Python by James, Witten, Hastie, Tibshirani and Taylor (Springer, 2023). The pdf for this book is available for free on the book website.  What You'll Learn: Overview of statistical learning Linear regression Classification Resampling methods Linear model selection and regularization Moving beyond linearity Tree-based methods Support vector machines Deep learning Survival modeling Unsupervised learning Multiple testing|MexNVKPwu7g
75|0|76|Statistical Learning: 10.5 Time Series Forecasting|2022-10-07T17:21:16Z|This is an introductory-level course in supervised learning, with a focus on regression and classification methods. The syllabus includes: linear and polynomial regression, logistic regression and linear discriminant analysis; cross-validation and the bootstrap, model selection and regularization methods (ridge and lasso); nonlinear models, splines and generalized additive models; tree-based methods, random forests and boosting; support-vector machines; neural networks and deep learning; survival models; multiple testing. Some unsupervised learning methods are discussed: principal components and clustering (k-means and hierarchical).  This is not a math-heavy class, so we try and describe the methods without heavy reliance on formulas and complex mathematics. We focus on what we consider to be the important elements of modern data science. Computing is done in Python. There are lectures devoted to Python, giving tutorials from the ground up, and progressing with more detailed sessions that implement the techniques in each chapter.  The lectures cover all the material in An Introduction to Statistical Learning, with Applications in Python by James, Witten, Hastie, Tibshirani and Taylor (Springer, 2023). The pdf for this book is available for free on the book website.  What You'll Learn: Overview of statistical learning Linear regression Classification Resampling methods Linear model selection and regularization Moving beyond linearity Tree-based methods Support vector machines Deep learning Survival modeling Unsupervised learning Multiple testing|ogx1q2xBHkc
76|0|77|Statistical Learning: 10.6 Fitting Neural Networks|2022-10-07T17:21:31Z|This is an introductory-level course in supervised learning, with a focus on regression and classification methods. The syllabus includes: linear and polynomial regression, logistic regression and linear discriminant analysis; cross-validation and the bootstrap, model selection and regularization methods (ridge and lasso); nonlinear models, splines and generalized additive models; tree-based methods, random forests and boosting; support-vector machines; neural networks and deep learning; survival models; multiple testing. Some unsupervised learning methods are discussed: principal components and clustering (k-means and hierarchical).  This is not a math-heavy class, so we try and describe the methods without heavy reliance on formulas and complex mathematics. We focus on what we consider to be the important elements of modern data science. Computing is done in Python. There are lectures devoted to Python, giving tutorials from the ground up, and progressing with more detailed sessions that implement the techniques in each chapter.  The lectures cover all the material in An Introduction to Statistical Learning, with Applications in Python by James, Witten, Hastie, Tibshirani and Taylor (Springer, 2023). The pdf for this book is available for free on the book website.  What You'll Learn: Overview of statistical learning Linear regression Classification Resampling methods Linear model selection and regularization Moving beyond linearity Tree-based methods Support vector machines Deep learning Survival modeling Unsupervised learning Multiple testing|07zslA8BXZY
77|0|78|Statistical Learning: 10.7 Interpolation and Double Descent|2022-10-07T17:21:49Z|This is an introductory-level course in supervised learning, with a focus on regression and classification methods. The syllabus includes: linear and polynomial regression, logistic regression and linear discriminant analysis; cross-validation and the bootstrap, model selection and regularization methods (ridge and lasso); nonlinear models, splines and generalized additive models; tree-based methods, random forests and boosting; support-vector machines; neural networks and deep learning; survival models; multiple testing. Some unsupervised learning methods are discussed: principal components and clustering (k-means and hierarchical).  This is not a math-heavy class, so we try and describe the methods without heavy reliance on formulas and complex mathematics. We focus on what we consider to be the important elements of modern data science. Computing is done in Python. There are lectures devoted to Python, giving tutorials from the ground up, and progressing with more detailed sessions that implement the techniques in each chapter.  The lectures cover all the material in An Introduction to Statistical Learning, with Applications in Python by James, Witten, Hastie, Tibshirani and Taylor (Springer, 2023). The pdf for this book is available for free on the book website.  What You'll Learn: Overview of statistical learning Linear regression Classification Resampling methods Linear model selection and regularization Moving beyond linearity Tree-based methods Support vector machines Deep learning Survival modeling Unsupervised learning Multiple testing|qRHdQz_P_Lo
78|0|79|Statistical Learning: 10.Py Single Layer Model: Hitters Data I 2023|2023-12-05T19:01:01Z|This is an introductory-level course in supervised learning, with a focus on regression and classification methods. The syllabus includes: linear and polynomial regression, logistic regression and linear discriminant analysis; cross-validation and the bootstrap, model selection and regularization methods (ridge and lasso); nonlinear models, splines and generalized additive models; tree-based methods, random forests and boosting; support-vector machines; neural networks and deep learning; survival models; multiple testing. Some unsupervised learning methods are discussed: principal components and clustering (k-means and hierarchical).  This is not a math-heavy class, so we try and describe the methods without heavy reliance on formulas and complex mathematics. We focus on what we consider to be the important elements of modern data science. Computing is done in Python. There are lectures devoted to Python, giving tutorials from the ground up, and progressing with more detailed sessions that implement the techniques in each chapter.  The lectures cover all the material in An Introduction to Statistical Learning, with Applications in Python by James, Witten, Hastie, Tibshirani and Taylor (Springer, 2023). The pdf for this book is available for free on the book website.  What You'll Learn: Overview of statistical learning Linear regression Classification Resampling methods Linear model selection and regularization Moving beyond linearity Tree-based methods Support vector machines Deep learning Survival modeling Unsupervised learning Multiple testing|-ZHEnz-emcQ
79|0|80|Statistical Learning: 10.Py Multilayer Model: MNIST Digit Data I 2023|2023-12-05T19:01:38Z|This is an introductory-level course in supervised learning, with a focus on regression and classification methods. The syllabus includes: linear and polynomial regression, logistic regression and linear discriminant analysis; cross-validation and the bootstrap, model selection and regularization methods (ridge and lasso); nonlinear models, splines and generalized additive models; tree-based methods, random forests and boosting; support-vector machines; neural networks and deep learning; survival models; multiple testing. Some unsupervised learning methods are discussed: principal components and clustering (k-means and hierarchical).  This is not a math-heavy class, so we try and describe the methods without heavy reliance on formulas and complex mathematics. We focus on what we consider to be the important elements of modern data science. Computing is done in Python. There are lectures devoted to Python, giving tutorials from the ground up, and progressing with more detailed sessions that implement the techniques in each chapter.  The lectures cover all the material in An Introduction to Statistical Learning, with Applications in Python by James, Witten, Hastie, Tibshirani and Taylor (Springer, 2023). The pdf for this book is available for free on the book website.  What You'll Learn: Overview of statistical learning Linear regression Classification Resampling methods Linear model selection and regularization Moving beyond linearity Tree-based methods Support vector machines Deep learning Survival modeling Unsupervised learning Multiple testing|csmHZGEsONU
80|0|81|Statistical Learning: 10.Py Convolutional Neural Network: CIFAR Image Data I 2023|2023-12-05T19:01:12Z|This is an introductory-level course in supervised learning, with a focus on regression and classification methods. The syllabus includes: linear and polynomial regression, logistic regression and linear discriminant analysis; cross-validation and the bootstrap, model selection and regularization methods (ridge and lasso); nonlinear models, splines and generalized additive models; tree-based methods, random forests and boosting; support-vector machines; neural networks and deep learning; survival models; multiple testing. Some unsupervised learning methods are discussed: principal components and clustering (k-means and hierarchical).  This is not a math-heavy class, so we try and describe the methods without heavy reliance on formulas and complex mathematics. We focus on what we consider to be the important elements of modern data science. Computing is done in Python. There are lectures devoted to Python, giving tutorials from the ground up, and progressing with more detailed sessions that implement the techniques in each chapter.  The lectures cover all the material in An Introduction to Statistical Learning, with Applications in Python by James, Witten, Hastie, Tibshirani and Taylor (Springer, 2023). The pdf for this book is available for free on the book website.  What You'll Learn: Overview of statistical learning Linear regression Classification Resampling methods Linear model selection and regularization Moving beyond linearity Tree-based methods Support vector machines Deep learning Survival modeling Unsupervised learning Multiple testing|NJ0gKDGdchQ
81|0|82|Statistical Learning: 10.Py Document Classification and Recurrent Neural Networks I 2023|2023-12-05T19:15:06Z|This is an introductory-level course in supervised learning, with a focus on regression and classification methods. The syllabus includes: linear and polynomial regression, logistic regression and linear discriminant analysis; cross-validation and the bootstrap, model selection and regularization methods (ridge and lasso); nonlinear models, splines and generalized additive models; tree-based methods, random forests and boosting; support-vector machines; neural networks and deep learning; survival models; multiple testing. Some unsupervised learning methods are discussed: principal components and clustering (k-means and hierarchical).  This is not a math-heavy class, so we try and describe the methods without heavy reliance on formulas and complex mathematics. We focus on what we consider to be the important elements of modern data science. Computing is done in Python. There are lectures devoted to Python, giving tutorials from the ground up, and progressing with more detailed sessions that implement the techniques in each chapter.  The lectures cover all the material in An Introduction to Statistical Learning, with Applications in Python by James, Witten, Hastie, Tibshirani and Taylor (Springer, 2023). The pdf for this book is available for free on the book website.  What You'll Learn: Overview of statistical learning Linear regression Classification Resampling methods Linear model selection and regularization Moving beyond linearity Tree-based methods Support vector machines Deep learning Survival modeling Unsupervised learning Multiple testing|RM-MkwkGUSI
82|0|83|Statistical Learning: 11.1 Introduction to Survival Data and Censoring|2022-10-07T17:38:31Z|This is an introductory-level course in supervised learning, with a focus on regression and classification methods. The syllabus includes: linear and polynomial regression, logistic regression and linear discriminant analysis; cross-validation and the bootstrap, model selection and regularization methods (ridge and lasso); nonlinear models, splines and generalized additive models; tree-based methods, random forests and boosting; support-vector machines; neural networks and deep learning; survival models; multiple testing. Some unsupervised learning methods are discussed: principal components and clustering (k-means and hierarchical).  This is not a math-heavy class, so we try and describe the methods without heavy reliance on formulas and complex mathematics. We focus on what we consider to be the important elements of modern data science. Computing is done in Python. There are lectures devoted to Python, giving tutorials from the ground up, and progressing with more detailed sessions that implement the techniques in each chapter.  The lectures cover all the material in An Introduction to Statistical Learning, with Applications in Python by James, Witten, Hastie, Tibshirani and Taylor (Springer, 2023). The pdf for this book is available for free on the book website.  What You'll Learn: Overview of statistical learning Linear regression Classification Resampling methods Linear model selection and regularization Moving beyond linearity Tree-based methods Support vector machines Deep learning Survival modeling Unsupervised learning Multiple testing|7_XK7mGMm1E
83|0|84|Statistical Learning: 11.2 Proportional Hazards Model|2022-10-07T17:38:55Z|This is an introductory-level course in supervised learning, with a focus on regression and classification methods. The syllabus includes: linear and polynomial regression, logistic regression and linear discriminant analysis; cross-validation and the bootstrap, model selection and regularization methods (ridge and lasso); nonlinear models, splines and generalized additive models; tree-based methods, random forests and boosting; support-vector machines; neural networks and deep learning; survival models; multiple testing. Some unsupervised learning methods are discussed: principal components and clustering (k-means and hierarchical).  This is not a math-heavy class, so we try and describe the methods without heavy reliance on formulas and complex mathematics. We focus on what we consider to be the important elements of modern data science. Computing is done in Python. There are lectures devoted to Python, giving tutorials from the ground up, and progressing with more detailed sessions that implement the techniques in each chapter.  The lectures cover all the material in An Introduction to Statistical Learning, with Applications in Python by James, Witten, Hastie, Tibshirani and Taylor (Springer, 2023). The pdf for this book is available for free on the book website.  What You'll Learn: Overview of statistical learning Linear regression Classification Resampling methods Linear model selection and regularization Moving beyond linearity Tree-based methods Support vector machines Deep learning Survival modeling Unsupervised learning Multiple testing|lP42Vly2MVg
84|0|85|Statistical Learning: 11.3 Estimation of Cox Model with Examples|2022-10-07T17:39:11Z|This is an introductory-level course in supervised learning, with a focus on regression and classification methods. The syllabus includes: linear and polynomial regression, logistic regression and linear discriminant analysis; cross-validation and the bootstrap, model selection and regularization methods (ridge and lasso); nonlinear models, splines and generalized additive models; tree-based methods, random forests and boosting; support-vector machines; neural networks and deep learning; survival models; multiple testing. Some unsupervised learning methods are discussed: principal components and clustering (k-means and hierarchical).  This is not a math-heavy class, so we try and describe the methods without heavy reliance on formulas and complex mathematics. We focus on what we consider to be the important elements of modern data science. Computing is done in Python. There are lectures devoted to Python, giving tutorials from the ground up, and progressing with more detailed sessions that implement the techniques in each chapter.  The lectures cover all the material in An Introduction to Statistical Learning, with Applications in Python by James, Witten, Hastie, Tibshirani and Taylor (Springer, 2023). The pdf for this book is available for free on the book website.  What You'll Learn: Overview of statistical learning Linear regression Classification Resampling methods Linear model selection and regularization Moving beyond linearity Tree-based methods Support vector machines Deep learning Survival modeling Unsupervised learning Multiple testing|ujIMPpl2Tr0
85|0|86|Statistical Learning: 11.4 Model Evaluation and Further Topics|2022-10-07T17:39:27Z|This is an introductory-level course in supervised learning, with a focus on regression and classification methods. The syllabus includes: linear and polynomial regression, logistic regression and linear discriminant analysis; cross-validation and the bootstrap, model selection and regularization methods (ridge and lasso); nonlinear models, splines and generalized additive models; tree-based methods, random forests and boosting; support-vector machines; neural networks and deep learning; survival models; multiple testing. Some unsupervised learning methods are discussed: principal components and clustering (k-means and hierarchical).  This is not a math-heavy class, so we try and describe the methods without heavy reliance on formulas and complex mathematics. We focus on what we consider to be the important elements of modern data science. Computing is done in Python. There are lectures devoted to Python, giving tutorials from the ground up, and progressing with more detailed sessions that implement the techniques in each chapter.  The lectures cover all the material in An Introduction to Statistical Learning, with Applications in Python by James, Witten, Hastie, Tibshirani and Taylor (Springer, 2023). The pdf for this book is available for free on the book website.  What You'll Learn: Overview of statistical learning Linear regression Classification Resampling methods Linear model selection and regularization Moving beyond linearity Tree-based methods Support vector machines Deep learning Survival modeling Unsupervised learning Multiple testing|rRYfWAsG4RI
86|0|87|Statistical Learning: 11.Py Cox Model: Brain Cancer Data I 2023|2023-12-05T19:01:19Z|This is an introductory-level course in supervised learning, with a focus on regression and classification methods. The syllabus includes: linear and polynomial regression, logistic regression and linear discriminant analysis; cross-validation and the bootstrap, model selection and regularization methods (ridge and lasso); nonlinear models, splines and generalized additive models; tree-based methods, random forests and boosting; support-vector machines; neural networks and deep learning; survival models; multiple testing. Some unsupervised learning methods are discussed: principal components and clustering (k-means and hierarchical).  This is not a math-heavy class, so we try and describe the methods without heavy reliance on formulas and complex mathematics. We focus on what we consider to be the important elements of modern data science. Computing is done in Python. There are lectures devoted to Python, giving tutorials from the ground up, and progressing with more detailed sessions that implement the techniques in each chapter.  The lectures cover all the material in An Introduction to Statistical Learning, with Applications in Python by James, Witten, Hastie, Tibshirani and Taylor (Springer, 2023). The pdf for this book is available for free on the book website.  What You'll Learn: Overview of statistical learning Linear regression Classification Resampling methods Linear model selection and regularization Moving beyond linearity Tree-based methods Support vector machines Deep learning Survival modeling Unsupervised learning Multiple testing|HUBmWuxu68M
87|0|88|Statistical Learning: 11.Py Cox Model: Publication Data I 2023|2023-12-05T19:00:48Z|This is an introductory-level course in supervised learning, with a focus on regression and classification methods. The syllabus includes: linear and polynomial regression, logistic regression and linear discriminant analysis; cross-validation and the bootstrap, model selection and regularization methods (ridge and lasso); nonlinear models, splines and generalized additive models; tree-based methods, random forests and boosting; support-vector machines; neural networks and deep learning; survival models; multiple testing. Some unsupervised learning methods are discussed: principal components and clustering (k-means and hierarchical).  This is not a math-heavy class, so we try and describe the methods without heavy reliance on formulas and complex mathematics. We focus on what we consider to be the important elements of modern data science. Computing is done in Python. There are lectures devoted to Python, giving tutorials from the ground up, and progressing with more detailed sessions that implement the techniques in each chapter.  The lectures cover all the material in An Introduction to Statistical Learning, with Applications in Python by James, Witten, Hastie, Tibshirani and Taylor (Springer, 2023). The pdf for this book is available for free on the book website.  What You'll Learn: Overview of statistical learning Linear regression Classification Resampling methods Linear model selection and regularization Moving beyond linearity Tree-based methods Support vector machines Deep learning Survival modeling Unsupervised learning Multiple testing|H2AE2KS89-w
88|0|89|Statistical Learning: 12.1 Principal Components|2022-10-07T17:42:02Z|This is an introductory-level course in supervised learning, with a focus on regression and classification methods. The syllabus includes: linear and polynomial regression, logistic regression and linear discriminant analysis; cross-validation and the bootstrap, model selection and regularization methods (ridge and lasso); nonlinear models, splines and generalized additive models; tree-based methods, random forests and boosting; support-vector machines; neural networks and deep learning; survival models; multiple testing. Some unsupervised learning methods are discussed: principal components and clustering (k-means and hierarchical).  This is not a math-heavy class, so we try and describe the methods without heavy reliance on formulas and complex mathematics. We focus on what we consider to be the important elements of modern data science. Computing is done in Python. There are lectures devoted to Python, giving tutorials from the ground up, and progressing with more detailed sessions that implement the techniques in each chapter.  The lectures cover all the material in An Introduction to Statistical Learning, with Applications in Python by James, Witten, Hastie, Tibshirani and Taylor (Springer, 2023). The pdf for this book is available for free on the book website.  What You'll Learn: Overview of statistical learning Linear regression Classification Resampling methods Linear model selection and regularization Moving beyond linearity Tree-based methods Support vector machines Deep learning Survival modeling Unsupervised learning Multiple testing|kpuQqOzQXfM
89|0|90|Statistical Learning: 12.2 Higher order principal components|2022-10-07T17:42:34Z|This is an introductory-level course in supervised learning, with a focus on regression and classification methods. The syllabus includes: linear and polynomial regression, logistic regression and linear discriminant analysis; cross-validation and the bootstrap, model selection and regularization methods (ridge and lasso); nonlinear models, splines and generalized additive models; tree-based methods, random forests and boosting; support-vector machines; neural networks and deep learning; survival models; multiple testing. Some unsupervised learning methods are discussed: principal components and clustering (k-means and hierarchical).  This is not a math-heavy class, so we try and describe the methods without heavy reliance on formulas and complex mathematics. We focus on what we consider to be the important elements of modern data science. Computing is done in Python. There are lectures devoted to Python, giving tutorials from the ground up, and progressing with more detailed sessions that implement the techniques in each chapter.  The lectures cover all the material in An Introduction to Statistical Learning, with Applications in Python by James, Witten, Hastie, Tibshirani and Taylor (Springer, 2023). The pdf for this book is available for free on the book website.  What You'll Learn: Overview of statistical learning Linear regression Classification Resampling methods Linear model selection and regularization Moving beyond linearity Tree-based methods Support vector machines Deep learning Survival modeling Unsupervised learning Multiple testing|O30nHhyBiAs
90|0|91|Statistical Learning: 12.3 k means Clustering|2022-10-07T17:46:13Z|This is an introductory-level course in supervised learning, with a focus on regression and classification methods. The syllabus includes: linear and polynomial regression, logistic regression and linear discriminant analysis; cross-validation and the bootstrap, model selection and regularization methods (ridge and lasso); nonlinear models, splines and generalized additive models; tree-based methods, random forests and boosting; support-vector machines; neural networks and deep learning; survival models; multiple testing. Some unsupervised learning methods are discussed: principal components and clustering (k-means and hierarchical).  This is not a math-heavy class, so we try and describe the methods without heavy reliance on formulas and complex mathematics. We focus on what we consider to be the important elements of modern data science. Computing is done in Python. There are lectures devoted to Python, giving tutorials from the ground up, and progressing with more detailed sessions that implement the techniques in each chapter.  The lectures cover all the material in An Introduction to Statistical Learning, with Applications in Python by James, Witten, Hastie, Tibshirani and Taylor (Springer, 2023). The pdf for this book is available for free on the book website.  What You'll Learn: Overview of statistical learning Linear regression Classification Resampling methods Linear model selection and regularization Moving beyond linearity Tree-based methods Support vector machines Deep learning Survival modeling Unsupervised learning Multiple testing|ded_NQqOe7I
91|0|92|Statistical Learning: 12.4 Hierarchical Clustering|2022-10-07T17:46:08Z|This is an introductory-level course in supervised learning, with a focus on regression and classification methods. The syllabus includes: linear and polynomial regression, logistic regression and linear discriminant analysis; cross-validation and the bootstrap, model selection and regularization methods (ridge and lasso); nonlinear models, splines and generalized additive models; tree-based methods, random forests and boosting; support-vector machines; neural networks and deep learning; survival models; multiple testing. Some unsupervised learning methods are discussed: principal components and clustering (k-means and hierarchical).  This is not a math-heavy class, so we try and describe the methods without heavy reliance on formulas and complex mathematics. We focus on what we consider to be the important elements of modern data science. Computing is done in Python. There are lectures devoted to Python, giving tutorials from the ground up, and progressing with more detailed sessions that implement the techniques in each chapter.  The lectures cover all the material in An Introduction to Statistical Learning, with Applications in Python by James, Witten, Hastie, Tibshirani and Taylor (Springer, 2023). The pdf for this book is available for free on the book website.  What You'll Learn: Overview of statistical learning Linear regression Classification Resampling methods Linear model selection and regularization Moving beyond linearity Tree-based methods Support vector machines Deep learning Survival modeling Unsupervised learning Multiple testing|yktzn-Mr2Nw
92|0|93|Statistical Learning: 12.5 Matrix Completion|2022-10-07T17:46:06Z|This is an introductory-level course in supervised learning, with a focus on regression and classification methods. The syllabus includes: linear and polynomial regression, logistic regression and linear discriminant analysis; cross-validation and the bootstrap, model selection and regularization methods (ridge and lasso); nonlinear models, splines and generalized additive models; tree-based methods, random forests and boosting; support-vector machines; neural networks and deep learning; survival models; multiple testing. Some unsupervised learning methods are discussed: principal components and clustering (k-means and hierarchical).  This is not a math-heavy class, so we try and describe the methods without heavy reliance on formulas and complex mathematics. We focus on what we consider to be the important elements of modern data science. Computing is done in Python. There are lectures devoted to Python, giving tutorials from the ground up, and progressing with more detailed sessions that implement the techniques in each chapter.  The lectures cover all the material in An Introduction to Statistical Learning, with Applications in Python by James, Witten, Hastie, Tibshirani and Taylor (Springer, 2023). The pdf for this book is available for free on the book website.  What You'll Learn: Overview of statistical learning Linear regression Classification Resampling methods Linear model selection and regularization Moving beyond linearity Tree-based methods Support vector machines Deep learning Survival modeling Unsupervised learning Multiple testing|MYKb5KcI55s
93|0|94|Statistical Learning: 12.6 Breast Cancer Example|2022-10-07T17:46:03Z|This is an introductory-level course in supervised learning, with a focus on regression and classification methods. The syllabus includes: linear and polynomial regression, logistic regression and linear discriminant analysis; cross-validation and the bootstrap, model selection and regularization methods (ridge and lasso); nonlinear models, splines and generalized additive models; tree-based methods, random forests and boosting; support-vector machines; neural networks and deep learning; survival models; multiple testing. Some unsupervised learning methods are discussed: principal components and clustering (k-means and hierarchical).  This is not a math-heavy class, so we try and describe the methods without heavy reliance on formulas and complex mathematics. We focus on what we consider to be the important elements of modern data science. Computing is done in Python. There are lectures devoted to Python, giving tutorials from the ground up, and progressing with more detailed sessions that implement the techniques in each chapter.  The lectures cover all the material in An Introduction to Statistical Learning, with Applications in Python by James, Witten, Hastie, Tibshirani and Taylor (Springer, 2023). The pdf for this book is available for free on the book website.  What You'll Learn: Overview of statistical learning Linear regression Classification Resampling methods Linear model selection and regularization Moving beyond linearity Tree-based methods Support vector machines Deep learning Survival modeling Unsupervised learning Multiple testing|InBhMLEx6sU
94|0|95|Statistical Learning: 12.Py Principal Components I 2023|2023-12-05T19:13:52Z|This is an introductory-level course in supervised learning, with a focus on regression and classification methods. The syllabus includes: linear and polynomial regression, logistic regression and linear discriminant analysis; cross-validation and the bootstrap, model selection and regularization methods (ridge and lasso); nonlinear models, splines and generalized additive models; tree-based methods, random forests and boosting; support-vector machines; neural networks and deep learning; survival models; multiple testing. Some unsupervised learning methods are discussed: principal components and clustering (k-means and hierarchical).  This is not a math-heavy class, so we try and describe the methods without heavy reliance on formulas and complex mathematics. We focus on what we consider to be the important elements of modern data science. Computing is done in Python. There are lectures devoted to Python, giving tutorials from the ground up, and progressing with more detailed sessions that implement the techniques in each chapter.  The lectures cover all the material in An Introduction to Statistical Learning, with Applications in Python by James, Witten, Hastie, Tibshirani and Taylor (Springer, 2023). The pdf for this book is available for free on the book website.  What You'll Learn: Overview of statistical learning Linear regression Classification Resampling methods Linear model selection and regularization Moving beyond linearity Tree-based methods Support vector machines Deep learning Survival modeling Unsupervised learning Multiple testing|3zpISnhksqQ
95|0|96|Statistical Learning: 12.Py Clustering I 2023|2023-12-05T19:13:57Z|This is an introductory-level course in supervised learning, with a focus on regression and classification methods. The syllabus includes: linear and polynomial regression, logistic regression and linear discriminant analysis; cross-validation and the bootstrap, model selection and regularization methods (ridge and lasso); nonlinear models, splines and generalized additive models; tree-based methods, random forests and boosting; support-vector machines; neural networks and deep learning; survival models; multiple testing. Some unsupervised learning methods are discussed: principal components and clustering (k-means and hierarchical).  This is not a math-heavy class, so we try and describe the methods without heavy reliance on formulas and complex mathematics. We focus on what we consider to be the important elements of modern data science. Computing is done in Python. There are lectures devoted to Python, giving tutorials from the ground up, and progressing with more detailed sessions that implement the techniques in each chapter.  The lectures cover all the material in An Introduction to Statistical Learning, with Applications in Python by James, Witten, Hastie, Tibshirani and Taylor (Springer, 2023). The pdf for this book is available for free on the book website.  What You'll Learn: Overview of statistical learning Linear regression Classification Resampling methods Linear model selection and regularization Moving beyond linearity Tree-based methods Support vector machines Deep learning Survival modeling Unsupervised learning Multiple testing|xhLD4nEEVlE
96|0|97|Statistical Learning: 12.Py Application: NCI60 Data I 2023|2023-12-05T19:14:07Z|This is an introductory-level course in supervised learning, with a focus on regression and classification methods. The syllabus includes: linear and polynomial regression, logistic regression and linear discriminant analysis; cross-validation and the bootstrap, model selection and regularization methods (ridge and lasso); nonlinear models, splines and generalized additive models; tree-based methods, random forests and boosting; support-vector machines; neural networks and deep learning; survival models; multiple testing. Some unsupervised learning methods are discussed: principal components and clustering (k-means and hierarchical).  This is not a math-heavy class, so we try and describe the methods without heavy reliance on formulas and complex mathematics. We focus on what we consider to be the important elements of modern data science. Computing is done in Python. There are lectures devoted to Python, giving tutorials from the ground up, and progressing with more detailed sessions that implement the techniques in each chapter.  The lectures cover all the material in An Introduction to Statistical Learning, with Applications in Python by James, Witten, Hastie, Tibshirani and Taylor (Springer, 2023). The pdf for this book is available for free on the book website.  What You'll Learn: Overview of statistical learning Linear regression Classification Resampling methods Linear model selection and regularization Moving beyond linearity Tree-based methods Support vector machines Deep learning Survival modeling Unsupervised learning Multiple testing|aygQSHAbMFQ
97|0|98|Statistical Learning: 13.1 Introduction to Hypothesis Testing|2022-10-07T17:48:53Z|This is an introductory-level course in supervised learning, with a focus on regression and classification methods. The syllabus includes: linear and polynomial regression, logistic regression and linear discriminant analysis; cross-validation and the bootstrap, model selection and regularization methods (ridge and lasso); nonlinear models, splines and generalized additive models; tree-based methods, random forests and boosting; support-vector machines; neural networks and deep learning; survival models; multiple testing. Some unsupervised learning methods are discussed: principal components and clustering (k-means and hierarchical).  This is not a math-heavy class, so we try and describe the methods without heavy reliance on formulas and complex mathematics. We focus on what we consider to be the important elements of modern data science. Computing is done in Python. There are lectures devoted to Python, giving tutorials from the ground up, and progressing with more detailed sessions that implement the techniques in each chapter.  The lectures cover all the material in An Introduction to Statistical Learning, with Applications in Python by James, Witten, Hastie, Tibshirani and Taylor (Springer, 2023). The pdf for this book is available for free on the book website.  What You'll Learn: Overview of statistical learning Linear regression Classification Resampling methods Linear model selection and regularization Moving beyond linearity Tree-based methods Support vector machines Deep learning Survival modeling Unsupervised learning Multiple testing|ti9NFdjf3sM
98|0|99|Statistical Learning: 13.1 Introduction to Hypothesis Testing II|2022-10-07T17:45:36Z|This is an introductory-level course in supervised learning, with a focus on regression and classification methods. The syllabus includes: linear and polynomial regression, logistic regression and linear discriminant analysis; cross-validation and the bootstrap, model selection and regularization methods (ridge and lasso); nonlinear models, splines and generalized additive models; tree-based methods, random forests and boosting; support-vector machines; neural networks and deep learning; survival models; multiple testing. Some unsupervised learning methods are discussed: principal components and clustering (k-means and hierarchical).  This is not a math-heavy class, so we try and describe the methods without heavy reliance on formulas and complex mathematics. We focus on what we consider to be the important elements of modern data science. Computing is done in Python. There are lectures devoted to Python, giving tutorials from the ground up, and progressing with more detailed sessions that implement the techniques in each chapter.  The lectures cover all the material in An Introduction to Statistical Learning, with Applications in Python by James, Witten, Hastie, Tibshirani and Taylor (Springer, 2023). The pdf for this book is available for free on the book website.  What You'll Learn: Overview of statistical learning Linear regression Classification Resampling methods Linear model selection and regularization Moving beyond linearity Tree-based methods Support vector machines Deep learning Survival modeling Unsupervised learning Multiple testing|klFG10_XajI
99|0|100|Statistical Learning: 13.2 Introduction to Multiple Testing and Family Wise Error Rate|2022-10-07T17:45:31Z|This is an introductory-level course in supervised learning, with a focus on regression and classification methods. The syllabus includes: linear and polynomial regression, logistic regression and linear discriminant analysis; cross-validation and the bootstrap, model selection and regularization methods (ridge and lasso); nonlinear models, splines and generalized additive models; tree-based methods, random forests and boosting; support-vector machines; neural networks and deep learning; survival models; multiple testing. Some unsupervised learning methods are discussed: principal components and clustering (k-means and hierarchical).  This is not a math-heavy class, so we try and describe the methods without heavy reliance on formulas and complex mathematics. We focus on what we consider to be the important elements of modern data science. Computing is done in Python. There are lectures devoted to Python, giving tutorials from the ground up, and progressing with more detailed sessions that implement the techniques in each chapter.  The lectures cover all the material in An Introduction to Statistical Learning, with Applications in Python by James, Witten, Hastie, Tibshirani and Taylor (Springer, 2023). The pdf for this book is available for free on the book website.  What You'll Learn: Overview of statistical learning Linear regression Classification Resampling methods Linear model selection and regularization Moving beyond linearity Tree-based methods Support vector machines Deep learning Survival modeling Unsupervised learning Multiple testing|-6zM6mydlfA
100|0|101|Statistical Learning: 13.3 Bonferroni Method for Controlling FWER|2022-10-07T17:45:28Z|This is an introductory-level course in supervised learning, with a focus on regression and classification methods. The syllabus includes: linear and polynomial regression, logistic regression and linear discriminant analysis; cross-validation and the bootstrap, model selection and regularization methods (ridge and lasso); nonlinear models, splines and generalized additive models; tree-based methods, random forests and boosting; support-vector machines; neural networks and deep learning; survival models; multiple testing. Some unsupervised learning methods are discussed: principal components and clustering (k-means and hierarchical).  This is not a math-heavy class, so we try and describe the methods without heavy reliance on formulas and complex mathematics. We focus on what we consider to be the important elements of modern data science. Computing is done in Python. There are lectures devoted to Python, giving tutorials from the ground up, and progressing with more detailed sessions that implement the techniques in each chapter.  The lectures cover all the material in An Introduction to Statistical Learning, with Applications in Python by James, Witten, Hastie, Tibshirani and Taylor (Springer, 2023). The pdf for this book is available for free on the book website.  What You'll Learn: Overview of statistical learning Linear regression Classification Resampling methods Linear model selection and regularization Moving beyond linearity Tree-based methods Support vector machines Deep learning Survival modeling Unsupervised learning Multiple testing|xML6pCgPv2c
101|0|102|Statistical Learning: 13.4 Holm's Method for Controlling FWER|2022-10-07T17:45:25Z|This is an introductory-level course in supervised learning, with a focus on regression and classification methods. The syllabus includes: linear and polynomial regression, logistic regression and linear discriminant analysis; cross-validation and the bootstrap, model selection and regularization methods (ridge and lasso); nonlinear models, splines and generalized additive models; tree-based methods, random forests and boosting; support-vector machines; neural networks and deep learning; survival models; multiple testing. Some unsupervised learning methods are discussed: principal components and clustering (k-means and hierarchical).  This is not a math-heavy class, so we try and describe the methods without heavy reliance on formulas and complex mathematics. We focus on what we consider to be the important elements of modern data science. Computing is done in Python. There are lectures devoted to Python, giving tutorials from the ground up, and progressing with more detailed sessions that implement the techniques in each chapter.  The lectures cover all the material in An Introduction to Statistical Learning, with Applications in Python by James, Witten, Hastie, Tibshirani and Taylor (Springer, 2023). The pdf for this book is available for free on the book website.  What You'll Learn: Overview of statistical learning Linear regression Classification Resampling methods Linear model selection and regularization Moving beyond linearity Tree-based methods Support vector machines Deep learning Survival modeling Unsupervised learning Multiple testing|8r_pMRnG97s
102|0|103|Statistical Learning: 13.5 False Discovery Rate and Benjamini Hochberg Method|2022-10-07T17:42:59Z|This is an introductory-level course in supervised learning, with a focus on regression and classification methods. The syllabus includes: linear and polynomial regression, logistic regression and linear discriminant analysis; cross-validation and the bootstrap, model selection and regularization methods (ridge and lasso); nonlinear models, splines and generalized additive models; tree-based methods, random forests and boosting; support-vector machines; neural networks and deep learning; survival models; multiple testing. Some unsupervised learning methods are discussed: principal components and clustering (k-means and hierarchical).  This is not a math-heavy class, so we try and describe the methods without heavy reliance on formulas and complex mathematics. We focus on what we consider to be the important elements of modern data science. Computing is done in Python. There are lectures devoted to Python, giving tutorials from the ground up, and progressing with more detailed sessions that implement the techniques in each chapter.  The lectures cover all the material in An Introduction to Statistical Learning, with Applications in Python by James, Witten, Hastie, Tibshirani and Taylor (Springer, 2023). The pdf for this book is available for free on the book website.  What You'll Learn: Overview of statistical learning Linear regression Classification Resampling methods Linear model selection and regularization Moving beyond linearity Tree-based methods Support vector machines Deep learning Survival modeling Unsupervised learning Multiple testing|4RPUrwzgO6c
103|0|104|Statistical Learning: 13.6 Resampling Approaches|2022-10-07T17:42:56Z|This is an introductory-level course in supervised learning, with a focus on regression and classification methods. The syllabus includes: linear and polynomial regression, logistic regression and linear discriminant analysis; cross-validation and the bootstrap, model selection and regularization methods (ridge and lasso); nonlinear models, splines and generalized additive models; tree-based methods, random forests and boosting; support-vector machines; neural networks and deep learning; survival models; multiple testing. Some unsupervised learning methods are discussed: principal components and clustering (k-means and hierarchical).  This is not a math-heavy class, so we try and describe the methods without heavy reliance on formulas and complex mathematics. We focus on what we consider to be the important elements of modern data science. Computing is done in Python. There are lectures devoted to Python, giving tutorials from the ground up, and progressing with more detailed sessions that implement the techniques in each chapter.  The lectures cover all the material in An Introduction to Statistical Learning, with Applications in Python by James, Witten, Hastie, Tibshirani and Taylor (Springer, 2023). The pdf for this book is available for free on the book website.  What You'll Learn: Overview of statistical learning Linear regression Classification Resampling methods Linear model selection and regularization Moving beyond linearity Tree-based methods Support vector machines Deep learning Survival modeling Unsupervised learning Multiple testing|BwjVTbU6is0
104|0|105|Statistical Learning: 13.6 Resampling Approaches II|2022-10-07T17:42:53Z|This is an introductory-level course in supervised learning, with a focus on regression and classification methods. The syllabus includes: linear and polynomial regression, logistic regression and linear discriminant analysis; cross-validation and the bootstrap, model selection and regularization methods (ridge and lasso); nonlinear models, splines and generalized additive models; tree-based methods, random forests and boosting; support-vector machines; neural networks and deep learning; survival models; multiple testing. Some unsupervised learning methods are discussed: principal components and clustering (k-means and hierarchical).  This is not a math-heavy class, so we try and describe the methods without heavy reliance on formulas and complex mathematics. We focus on what we consider to be the important elements of modern data science. Computing is done in Python. There are lectures devoted to Python, giving tutorials from the ground up, and progressing with more detailed sessions that implement the techniques in each chapter.  The lectures cover all the material in An Introduction to Statistical Learning, with Applications in Python by James, Witten, Hastie, Tibshirani and Taylor (Springer, 2023). The pdf for this book is available for free on the book website.  What You'll Learn: Overview of statistical learning Linear regression Classification Resampling methods Linear model selection and regularization Moving beyond linearity Tree-based methods Support vector machines Deep learning Survival modeling Unsupervised learning Multiple testing|Zylc7K3hZoA
105|0|106|Statistical Learning: 13.Py Multiple Testing I 2023|2023-12-05T19:14:39Z|This is an introductory-level course in supervised learning, with a focus on regression and classification methods. The syllabus includes: linear and polynomial regression, logistic regression and linear discriminant analysis; cross-validation and the bootstrap, model selection and regularization methods (ridge and lasso); nonlinear models, splines and generalized additive models; tree-based methods, random forests and boosting; support-vector machines; neural networks and deep learning; survival models; multiple testing. Some unsupervised learning methods are discussed: principal components and clustering (k-means and hierarchical).  This is not a math-heavy class, so we try and describe the methods without heavy reliance on formulas and complex mathematics. We focus on what we consider to be the important elements of modern data science. Computing is done in Python. There are lectures devoted to Python, giving tutorials from the ground up, and progressing with more detailed sessions that implement the techniques in each chapter.  The lectures cover all the material in An Introduction to Statistical Learning, with Applications in Python by James, Witten, Hastie, Tibshirani and Taylor (Springer, 2023). The pdf for this book is available for free on the book website.  What You'll Learn: Overview of statistical learning Linear regression Classification Resampling methods Linear model selection and regularization Moving beyond linearity Tree-based methods Support vector machines Deep learning Survival modeling Unsupervised learning Multiple testing|SBeMlJvijbQ
106|0|107|Statistical Learning: 13.Py False Discovery Rate I 2023|2023-12-05T19:14:45Z|This is an introductory-level course in supervised learning, with a focus on regression and classification methods. The syllabus includes: linear and polynomial regression, logistic regression and linear discriminant analysis; cross-validation and the bootstrap, model selection and regularization methods (ridge and lasso); nonlinear models, splines and generalized additive models; tree-based methods, random forests and boosting; support-vector machines; neural networks and deep learning; survival models; multiple testing. Some unsupervised learning methods are discussed: principal components and clustering (k-means and hierarchical).  This is not a math-heavy class, so we try and describe the methods without heavy reliance on formulas and complex mathematics. We focus on what we consider to be the important elements of modern data science. Computing is done in Python. There are lectures devoted to Python, giving tutorials from the ground up, and progressing with more detailed sessions that implement the techniques in each chapter.  The lectures cover all the material in An Introduction to Statistical Learning, with Applications in Python by James, Witten, Hastie, Tibshirani and Taylor (Springer, 2023). The pdf for this book is available for free on the book website.  What You'll Learn: Overview of statistical learning Linear regression Classification Resampling methods Linear model selection and regularization Moving beyond linearity Tree-based methods Support vector machines Deep learning Survival modeling Unsupervised learning Multiple testing|c9-zvw3AY3Q
107|0|108|Statistical Learning: 13.Py Multiple Testing and Resampling I 2023|2023-12-05T19:13:46Z|This is an introductory-level course in supervised learning, with a focus on regression and classification methods. The syllabus includes: linear and polynomial regression, logistic regression and linear discriminant analysis; cross-validation and the bootstrap, model selection and regularization methods (ridge and lasso); nonlinear models, splines and generalized additive models; tree-based methods, random forests and boosting; support-vector machines; neural networks and deep learning; survival models; multiple testing. Some unsupervised learning methods are discussed: principal components and clustering (k-means and hierarchical).  This is not a math-heavy class, so we try and describe the methods without heavy reliance on formulas and complex mathematics. We focus on what we consider to be the important elements of modern data science. Computing is done in Python. There are lectures devoted to Python, giving tutorials from the ground up, and progressing with more detailed sessions that implement the techniques in each chapter.  The lectures cover all the material in An Introduction to Statistical Learning, with Applications in Python by James, Witten, Hastie, Tibshirani and Taylor (Springer, 2023). The pdf for this book is available for free on the book website.  What You'll Learn: Overview of statistical learning Linear regression Classification Resampling methods Linear model selection and regularization Moving beyond linearity Tree-based methods Support vector machines Deep learning Survival modeling Unsupervised learning Multiple testing|mMFT44ra-Rg
108|1|1|Stanford CS224W: Machine Learning with Graphs , 2021 , Lecture 1.1 - Why Graphs|2021-04-13T16:16:28Z|Complex data can be represented as a graph of relationships between objects. Such networks are a fundamental tool for modeling social, technological, and biological systems. This course focuses on the computational, algorithmic, and modeling challenges specific to the analysis of massive graphs. By means of studying the underlying graph structure and its features, students are introduced to machine learning techniques and data mining tools apt to reveal insights on a variety of networks. Topics include: representation learning and Graph Neural Networks; algorithms for the World Wide Web; reasoning over Knowledge Graphs; influence maximization; disease outbreak detection, social network analysis.  View the course website: https://snap.stanford.edu/class/cs224w-2023/|JAB_plj2rbA
109|1|2|Stanford CS224W: Machine Learning with Graphs , 2021 , Lecture 1.2 - Applications of Graph ML|2021-04-13T16:17:46Z|Complex data can be represented as a graph of relationships between objects. Such networks are a fundamental tool for modeling social, technological, and biological systems. This course focuses on the computational, algorithmic, and modeling challenges specific to the analysis of massive graphs. By means of studying the underlying graph structure and its features, students are introduced to machine learning techniques and data mining tools apt to reveal insights on a variety of networks. Topics include: representation learning and Graph Neural Networks; algorithms for the World Wide Web; reasoning over Knowledge Graphs; influence maximization; disease outbreak detection, social network analysis.  View the course website: https://snap.stanford.edu/class/cs224w-2023/|aBHC6xzx9YI
110|1|3|Stanford CS224W: Machine Learning with Graphs , 2021 , Lecture 1.3 - Choice of Graph Representation​|2021-04-13T16:18:33Z|Complex data can be represented as a graph of relationships between objects. Such networks are a fundamental tool for modeling social, technological, and biological systems. This course focuses on the computational, algorithmic, and modeling challenges specific to the analysis of massive graphs. By means of studying the underlying graph structure and its features, students are introduced to machine learning techniques and data mining tools apt to reveal insights on a variety of networks. Topics include: representation learning and Graph Neural Networks; algorithms for the World Wide Web; reasoning over Knowledge Graphs; influence maximization; disease outbreak detection, social network analysis.  View the course website: https://snap.stanford.edu/class/cs224w-2023/|P-m1Qv6-8cI
111|1|4|Stanford CS224W: ML with Graphs , 2021 , Lecture 2.1 - Traditional Feature-based Methods: Node|2021-04-15T18:04:49Z|Complex data can be represented as a graph of relationships between objects. Such networks are a fundamental tool for modeling social, technological, and biological systems. This course focuses on the computational, algorithmic, and modeling challenges specific to the analysis of massive graphs. By means of studying the underlying graph structure and its features, students are introduced to machine learning techniques and data mining tools apt to reveal insights on a variety of networks. Topics include: representation learning and Graph Neural Networks; algorithms for the World Wide Web; reasoning over Knowledge Graphs; influence maximization; disease outbreak detection, social network analysis.  View the course website: https://snap.stanford.edu/class/cs224w-2023/|3IS7UhNMQ3U
112|1|5|Stanford CS224W: ML with Graphs , 2021 , Lecture 2.2 - Traditional Feature-based Methods: Link|2021-04-15T18:12:11Z|Complex data can be represented as a graph of relationships between objects. Such networks are a fundamental tool for modeling social, technological, and biological systems. This course focuses on the computational, algorithmic, and modeling challenges specific to the analysis of massive graphs. By means of studying the underlying graph structure and its features, students are introduced to machine learning techniques and data mining tools apt to reveal insights on a variety of networks. Topics include: representation learning and Graph Neural Networks; algorithms for the World Wide Web; reasoning over Knowledge Graphs; influence maximization; disease outbreak detection, social network analysis.  View the course website: https://snap.stanford.edu/class/cs224w-2023/|4dVwlE9jYxY
113|1|6|Stanford CS224W: ML with Graphs , 2021 , Lecture 2.3 - Traditional Feature-based Methods: Graph|2021-04-15T18:25:49Z|Complex data can be represented as a graph of relationships between objects. Such networks are a fundamental tool for modeling social, technological, and biological systems. This course focuses on the computational, algorithmic, and modeling challenges specific to the analysis of massive graphs. By means of studying the underlying graph structure and its features, students are introduced to machine learning techniques and data mining tools apt to reveal insights on a variety of networks. Topics include: representation learning and Graph Neural Networks; algorithms for the World Wide Web; reasoning over Knowledge Graphs; influence maximization; disease outbreak detection, social network analysis.  View the course website: https://snap.stanford.edu/class/cs224w-2023/|buzsHTa4Hgs
114|1|7|Stanford CS224W: Machine Learning with Graphs , 2021 , Lecture 3.1 - Node Embeddings|2021-04-20T16:23:01Z|Complex data can be represented as a graph of relationships between objects. Such networks are a fundamental tool for modeling social, technological, and biological systems. This course focuses on the computational, algorithmic, and modeling challenges specific to the analysis of massive graphs. By means of studying the underlying graph structure and its features, students are introduced to machine learning techniques and data mining tools apt to reveal insights on a variety of networks. Topics include: representation learning and Graph Neural Networks; algorithms for the World Wide Web; reasoning over Knowledge Graphs; influence maximization; disease outbreak detection, social network analysis.  View the course website: https://snap.stanford.edu/class/cs224w-2023/|rMq21iY61SE
115|1|8|Stanford CS224W: ML with Graphs , 2021 , Lecture 3.2-Random Walk Approaches for Node Embeddings|2021-04-20T16:27:26Z|Complex data can be represented as a graph of relationships between objects. Such networks are a fundamental tool for modeling social, technological, and biological systems. This course focuses on the computational, algorithmic, and modeling challenges specific to the analysis of massive graphs. By means of studying the underlying graph structure and its features, students are introduced to machine learning techniques and data mining tools apt to reveal insights on a variety of networks. Topics include: representation learning and Graph Neural Networks; algorithms for the World Wide Web; reasoning over Knowledge Graphs; influence maximization; disease outbreak detection, social network analysis.  View the course website: https://snap.stanford.edu/class/cs224w-2023/|Xv0wRy66Big
116|1|9|Stanford CS224W: Machine Learning with Graphs , 2021 , Lecture 3.3 - Embedding Entire Graphs|2021-04-20T16:38:56Z|Complex data can be represented as a graph of relationships between objects. Such networks are a fundamental tool for modeling social, technological, and biological systems. This course focuses on the computational, algorithmic, and modeling challenges specific to the analysis of massive graphs. By means of studying the underlying graph structure and its features, students are introduced to machine learning techniques and data mining tools apt to reveal insights on a variety of networks. Topics include: representation learning and Graph Neural Networks; algorithms for the World Wide Web; reasoning over Knowledge Graphs; influence maximization; disease outbreak detection, social network analysis.  View the course website: https://snap.stanford.edu/class/cs224w-2023/|eliMLfJeu7A
117|1|10|Stanford CS224W: Machine Learning with Graphs , 2021 , Lecture 4.1 - PageRank|2021-04-22T18:00:24Z|Complex data can be represented as a graph of relationships between objects. Such networks are a fundamental tool for modeling social, technological, and biological systems. This course focuses on the computational, algorithmic, and modeling challenges specific to the analysis of massive graphs. By means of studying the underlying graph structure and its features, students are introduced to machine learning techniques and data mining tools apt to reveal insights on a variety of networks. Topics include: representation learning and Graph Neural Networks; algorithms for the World Wide Web; reasoning over Knowledge Graphs; influence maximization; disease outbreak detection, social network analysis.  View the course website: https://snap.stanford.edu/class/cs224w-2023/|TU0ankRcHmo
118|1|11|Stanford CS224W: Machine Learning with Graphs , 2021 , Lecture 4.2 - PageRank: How to Solve?|2021-04-22T18:05:05Z|Complex data can be represented as a graph of relationships between objects. Such networks are a fundamental tool for modeling social, technological, and biological systems. This course focuses on the computational, algorithmic, and modeling challenges specific to the analysis of massive graphs. By means of studying the underlying graph structure and its features, students are introduced to machine learning techniques and data mining tools apt to reveal insights on a variety of networks. Topics include: representation learning and Graph Neural Networks; algorithms for the World Wide Web; reasoning over Knowledge Graphs; influence maximization; disease outbreak detection, social network analysis.  View the course website: https://snap.stanford.edu/class/cs224w-2023/|rK2ZBmQHVVs
119|1|12|Stanford CS224W: Machine Learning with Graphs , 2021 , Lecture 4.3 - Random Walk with Restarts|2021-04-22T18:11:20Z|Complex data can be represented as a graph of relationships between objects. Such networks are a fundamental tool for modeling social, technological, and biological systems. This course focuses on the computational, algorithmic, and modeling challenges specific to the analysis of massive graphs. By means of studying the underlying graph structure and its features, students are introduced to machine learning techniques and data mining tools apt to reveal insights on a variety of networks. Topics include: representation learning and Graph Neural Networks; algorithms for the World Wide Web; reasoning over Knowledge Graphs; influence maximization; disease outbreak detection, social network analysis.  View the course website: https://snap.stanford.edu/class/cs224w-2023/|HbzQzUaJ_9I
120|1|13|Stanford CS224W: ML with Graphs , 2021 , Lecture 4.4 - Matrix Factorization and Node Embeddings|2021-04-22T18:14:51Z|Complex data can be represented as a graph of relationships between objects. Such networks are a fundamental tool for modeling social, technological, and biological systems. This course focuses on the computational, algorithmic, and modeling challenges specific to the analysis of massive graphs. By means of studying the underlying graph structure and its features, students are introduced to machine learning techniques and data mining tools apt to reveal insights on a variety of networks. Topics include: representation learning and Graph Neural Networks; algorithms for the World Wide Web; reasoning over Knowledge Graphs; influence maximization; disease outbreak detection, social network analysis.  View the course website: https://snap.stanford.edu/class/cs224w-2023/|r12qJZZVtqc
121|1|14|Stanford CS224W: Machine Learning w/ Graphs I 2023 I Graph Neural Networks|2023-12-07T18:18:51Z|Complex data can be represented as a graph of relationships between objects. Such networks are a fundamental tool for modeling social, technological, and biological systems. This course focuses on the computational, algorithmic, and modeling challenges specific to the analysis of massive graphs. By means of studying the underlying graph structure and its features, students are introduced to machine learning techniques and data mining tools apt to reveal insights on a variety of networks. Topics include: representation learning and Graph Neural Networks; algorithms for the World Wide Web; reasoning over Knowledge Graphs; influence maximization; disease outbreak detection, social network analysis.  View the course website: https://snap.stanford.edu/class/cs224w-2023/|ZfK4FDk9uy8
122|1|15|Stanford CS224W: Machine Learning with Graphs , 2021 , Lecture 7.1 - A general Perspective on GNNs|2021-05-04T06:00:04Z|Complex data can be represented as a graph of relationships between objects. Such networks are a fundamental tool for modeling social, technological, and biological systems. This course focuses on the computational, algorithmic, and modeling challenges specific to the analysis of massive graphs. By means of studying the underlying graph structure and its features, students are introduced to machine learning techniques and data mining tools apt to reveal insights on a variety of networks. Topics include: representation learning and Graph Neural Networks; algorithms for the World Wide Web; reasoning over Knowledge Graphs; influence maximization; disease outbreak detection, social network analysis.  View the course website: https://snap.stanford.edu/class/cs224w-2023/|RU9uTa_-ZOw
123|1|16|Stanford CS224W: Machine Learning with Graphs , 2021 , Lecture 7.2 - A Single Layer of a GNN|2021-05-04T06:00:00Z|Complex data can be represented as a graph of relationships between objects. Such networks are a fundamental tool for modeling social, technological, and biological systems. This course focuses on the computational, algorithmic, and modeling challenges specific to the analysis of massive graphs. By means of studying the underlying graph structure and its features, students are introduced to machine learning techniques and data mining tools apt to reveal insights on a variety of networks. Topics include: representation learning and Graph Neural Networks; algorithms for the World Wide Web; reasoning over Knowledge Graphs; influence maximization; disease outbreak detection, social network analysis.  View the course website: https://snap.stanford.edu/class/cs224w-2023/|247Mkqj_wRM
124|1|17|Stanford CS224W: Machine Learning with Graphs , 2021 , Lecture 7.3 - Stacking layers of a GNN|2021-05-04T06:00:13Z|Complex data can be represented as a graph of relationships between objects. Such networks are a fundamental tool for modeling social, technological, and biological systems. This course focuses on the computational, algorithmic, and modeling challenges specific to the analysis of massive graphs. By means of studying the underlying graph structure and its features, students are introduced to machine learning techniques and data mining tools apt to reveal insights on a variety of networks. Topics include: representation learning and Graph Neural Networks; algorithms for the World Wide Web; reasoning over Knowledge Graphs; influence maximization; disease outbreak detection, social network analysis.  View the course website: https://snap.stanford.edu/class/cs224w-2023/|ew1cnUjRgl4
125|1|18|Stanford CS224W: Machine Learning with Graphs , 2021 , Lecture 8.1 - Graph Augmentation for GNNs|2021-05-06T06:00:00Z|Complex data can be represented as a graph of relationships between objects. Such networks are a fundamental tool for modeling social, technological, and biological systems. This course focuses on the computational, algorithmic, and modeling challenges specific to the analysis of massive graphs. By means of studying the underlying graph structure and its features, students are introduced to machine learning techniques and data mining tools apt to reveal insights on a variety of networks. Topics include: representation learning and Graph Neural Networks; algorithms for the World Wide Web; reasoning over Knowledge Graphs; influence maximization; disease outbreak detection, social network analysis.  View the course website: https://snap.stanford.edu/class/cs224w-2023/|1A6VoEkQnhQ
126|1|19|Stanford CS224W: Machine Learning with Graphs , 2021 , Lecture 8.2 - Training Graph Neural Networks|2021-05-06T06:00:06Z|Complex data can be represented as a graph of relationships between objects. Such networks are a fundamental tool for modeling social, technological, and biological systems. This course focuses on the computational, algorithmic, and modeling challenges specific to the analysis of massive graphs. By means of studying the underlying graph structure and its features, students are introduced to machine learning techniques and data mining tools apt to reveal insights on a variety of networks. Topics include: representation learning and Graph Neural Networks; algorithms for the World Wide Web; reasoning over Knowledge Graphs; influence maximization; disease outbreak detection, social network analysis.  View the course website: https://snap.stanford.edu/class/cs224w-2023/|eXIIH8YVxKI
127|1|20|Stanford CS224W: Machine Learning with Graphs , 2021 , Lecture 8.3 - Setting up GNN Prediction Tasks|2021-05-06T06:00:18Z|Complex data can be represented as a graph of relationships between objects. Such networks are a fundamental tool for modeling social, technological, and biological systems. This course focuses on the computational, algorithmic, and modeling challenges specific to the analysis of massive graphs. By means of studying the underlying graph structure and its features, students are introduced to machine learning techniques and data mining tools apt to reveal insights on a variety of networks. Topics include: representation learning and Graph Neural Networks; algorithms for the World Wide Web; reasoning over Knowledge Graphs; influence maximization; disease outbreak detection, social network analysis.  View the course website: https://snap.stanford.edu/class/cs224w-2023/|ewEW_EMzRuo
128|1|21|Stanford CS224W: ML with Graphs , 2021 , Lecture 9.1 - How Expressive are Graph Neural Networks|2021-05-11T06:00:03Z|Complex data can be represented as a graph of relationships between objects. Such networks are a fundamental tool for modeling social, technological, and biological systems. This course focuses on the computational, algorithmic, and modeling challenges specific to the analysis of massive graphs. By means of studying the underlying graph structure and its features, students are introduced to machine learning techniques and data mining tools apt to reveal insights on a variety of networks. Topics include: representation learning and Graph Neural Networks; algorithms for the World Wide Web; reasoning over Knowledge Graphs; influence maximization; disease outbreak detection, social network analysis.  View the course website: https://snap.stanford.edu/class/cs224w-2023/|5vMEgYbka0A
129|1|22|Stanford CS224W: ML with Graphs , 2021 , Lecture 9.2 - Designing the Most Powerful GNNs|2021-05-11T06:00:07Z|Complex data can be represented as a graph of relationships between objects. Such networks are a fundamental tool for modeling social, technological, and biological systems. This course focuses on the computational, algorithmic, and modeling challenges specific to the analysis of massive graphs. By means of studying the underlying graph structure and its features, students are introduced to machine learning techniques and data mining tools apt to reveal insights on a variety of networks. Topics include: representation learning and Graph Neural Networks; algorithms for the World Wide Web; reasoning over Knowledge Graphs; influence maximization; disease outbreak detection, social network analysis.  View the course website: https://snap.stanford.edu/class/cs224w-2023/|B5y47gWt3co
130|1|23|Stanford CS224W: Machine Learning w/ Graphs I 2023 I Label Propagation on Graphs|2023-12-07T18:18:58Z|Complex data can be represented as a graph of relationships between objects. Such networks are a fundamental tool for modeling social, technological, and biological systems. This course focuses on the computational, algorithmic, and modeling challenges specific to the analysis of massive graphs. By means of studying the underlying graph structure and its features, students are introduced to machine learning techniques and data mining tools apt to reveal insights on a variety of networks. Topics include: representation learning and Graph Neural Networks; algorithms for the World Wide Web; reasoning over Knowledge Graphs; influence maximization; disease outbreak detection, social network analysis.  View the course website: https://snap.stanford.edu/class/cs224w-2023/|1E8fie5KSl0
131|1|24|Stanford CS224W: Machine Learning w/ Graphs I 2023 I Machine Learning with Heterogeneous Graphs|2023-12-07T18:19:06Z|Complex data can be represented as a graph of relationships between objects. Such networks are a fundamental tool for modeling social, technological, and biological systems. This course focuses on the computational, algorithmic, and modeling challenges specific to the analysis of massive graphs. By means of studying the underlying graph structure and its features, students are introduced to machine learning techniques and data mining tools apt to reveal insights on a variety of networks. Topics include: representation learning and Graph Neural Networks; algorithms for the World Wide Web; reasoning over Knowledge Graphs; influence maximization; disease outbreak detection, social network analysis.  View the course website: https://snap.stanford.edu/class/cs224w-2023/|uvrlKxj8HVU
132|1|25|Stanford CS224W: Machine Learning w/ Graphs I 2023 I Knowledge Graph Embeddings|2023-12-07T18:19:14Z|Complex data can be represented as a graph of relationships between objects. Such networks are a fundamental tool for modeling social, technological, and biological systems. This course focuses on the computational, algorithmic, and modeling challenges specific to the analysis of massive graphs. By means of studying the underlying graph structure and its features, students are introduced to machine learning techniques and data mining tools apt to reveal insights on a variety of networks. Topics include: representation learning and Graph Neural Networks; algorithms for the World Wide Web; reasoning over Knowledge Graphs; influence maximization; disease outbreak detection, social network analysis.  View the course website: https://snap.stanford.edu/class/cs224w-2023/|isI_TUMoP60
133|1|26|Stanford CS224W: Machine Learning with Graphs , 2021 , Lecture 11.1 - Reasoning in Knowledge Graphs|2021-05-18T06:00:05Z|Complex data can be represented as a graph of relationships between objects. Such networks are a fundamental tool for modeling social, technological, and biological systems. This course focuses on the computational, algorithmic, and modeling challenges specific to the analysis of massive graphs. By means of studying the underlying graph structure and its features, students are introduced to machine learning techniques and data mining tools apt to reveal insights on a variety of networks. Topics include: representation learning and Graph Neural Networks; algorithms for the World Wide Web; reasoning over Knowledge Graphs; influence maximization; disease outbreak detection, social network analysis.  View the course website: https://snap.stanford.edu/class/cs224w-2023/|X9yl0pTP9fY
134|1|27|Stanford CS224W: Machine Learning with Graphs , 2021 , Lecture 11.2 - Answering Predictive Queries|2021-05-18T06:00:08Z|Complex data can be represented as a graph of relationships between objects. Such networks are a fundamental tool for modeling social, technological, and biological systems. This course focuses on the computational, algorithmic, and modeling challenges specific to the analysis of massive graphs. By means of studying the underlying graph structure and its features, students are introduced to machine learning techniques and data mining tools apt to reveal insights on a variety of networks. Topics include: representation learning and Graph Neural Networks; algorithms for the World Wide Web; reasoning over Knowledge Graphs; influence maximization; disease outbreak detection, social network analysis.  View the course website: https://snap.stanford.edu/class/cs224w-2023/|qaRIBNE-4Ho
135|1|28|Stanford CS224W: Machine Learning with Graphs , 2021 , Lecture 11.3 - Query2box: Reasoning over KGs|2021-05-18T06:00:07Z|Complex data can be represented as a graph of relationships between objects. Such networks are a fundamental tool for modeling social, technological, and biological systems. This course focuses on the computational, algorithmic, and modeling challenges specific to the analysis of massive graphs. By means of studying the underlying graph structure and its features, students are introduced to machine learning techniques and data mining tools apt to reveal insights on a variety of networks. Topics include: representation learning and Graph Neural Networks; algorithms for the World Wide Web; reasoning over Knowledge Graphs; influence maximization; disease outbreak detection, social network analysis.  View the course website: https://snap.stanford.edu/class/cs224w-2023/|Nt66M2OsbCw
136|1|29|Stanford CS224W: ML with Graphs , 2021 , Lecture 12.1-Fast Neural Subgraph Matching & Counting|2021-05-20T06:00:07Z|Complex data can be represented as a graph of relationships between objects. Such networks are a fundamental tool for modeling social, technological, and biological systems. This course focuses on the computational, algorithmic, and modeling challenges specific to the analysis of massive graphs. By means of studying the underlying graph structure and its features, students are introduced to machine learning techniques and data mining tools apt to reveal insights on a variety of networks. Topics include: representation learning and Graph Neural Networks; algorithms for the World Wide Web; reasoning over Knowledge Graphs; influence maximization; disease outbreak detection, social network analysis.  View the course website: https://snap.stanford.edu/class/cs224w-2023/|lRCDpfJoMiE
137|1|30|Stanford CS224W: Machine Learning with Graphs , 2021 , Lecture 12.2 - Neural Subgraph Matching|2021-05-17T21:27:45Z|Complex data can be represented as a graph of relationships between objects. Such networks are a fundamental tool for modeling social, technological, and biological systems. This course focuses on the computational, algorithmic, and modeling challenges specific to the analysis of massive graphs. By means of studying the underlying graph structure and its features, students are introduced to machine learning techniques and data mining tools apt to reveal insights on a variety of networks. Topics include: representation learning and Graph Neural Networks; algorithms for the World Wide Web; reasoning over Knowledge Graphs; influence maximization; disease outbreak detection, social network analysis.  View the course website: https://snap.stanford.edu/class/cs224w-2023/|4Ia5QzQ_QNI
138|1|31|Stanford CS224W: Machine Learning with Graphs , 2021 , Lecture 12.3 - Finding Frequent Subgraphs|2021-05-20T06:00:18Z|Complex data can be represented as a graph of relationships between objects. Such networks are a fundamental tool for modeling social, technological, and biological systems. This course focuses on the computational, algorithmic, and modeling challenges specific to the analysis of massive graphs. By means of studying the underlying graph structure and its features, students are introduced to machine learning techniques and data mining tools apt to reveal insights on a variety of networks. Topics include: representation learning and Graph Neural Networks; algorithms for the World Wide Web; reasoning over Knowledge Graphs; influence maximization; disease outbreak detection, social network analysis.  View the course website: https://snap.stanford.edu/class/cs224w-2023/|kUv4gY5e0hg
139|1|32|Stanford CS224W: ML with Graphs , 2021 , Lecture 13.1 - Community Detection in Networks|2021-05-25T06:00:03Z|Complex data can be represented as a graph of relationships between objects. Such networks are a fundamental tool for modeling social, technological, and biological systems. This course focuses on the computational, algorithmic, and modeling challenges specific to the analysis of massive graphs. By means of studying the underlying graph structure and its features, students are introduced to machine learning techniques and data mining tools apt to reveal insights on a variety of networks. Topics include: representation learning and Graph Neural Networks; algorithms for the World Wide Web; reasoning over Knowledge Graphs; influence maximization; disease outbreak detection, social network analysis.  View the course website: https://snap.stanford.edu/class/cs224w-2023/|KXi4ha79o3s
140|1|33|Stanford CS224W: Machine Learning with Graphs , 2021 , Lecture 13.2 - Network Communities|2021-05-25T06:00:05Z|Complex data can be represented as a graph of relationships between objects. Such networks are a fundamental tool for modeling social, technological, and biological systems. This course focuses on the computational, algorithmic, and modeling challenges specific to the analysis of massive graphs. By means of studying the underlying graph structure and its features, students are introduced to machine learning techniques and data mining tools apt to reveal insights on a variety of networks. Topics include: representation learning and Graph Neural Networks; algorithms for the World Wide Web; reasoning over Knowledge Graphs; influence maximization; disease outbreak detection, social network analysis.  View the course website: https://snap.stanford.edu/class/cs224w-2023/|mJQrtXZT5pw
141|1|34|Stanford CS224W: Machine Learning with Graphs , 2021 , Lecture 13.3 - Louvain Algorithm|2021-05-25T06:00:00Z|Complex data can be represented as a graph of relationships between objects. Such networks are a fundamental tool for modeling social, technological, and biological systems. This course focuses on the computational, algorithmic, and modeling challenges specific to the analysis of massive graphs. By means of studying the underlying graph structure and its features, students are introduced to machine learning techniques and data mining tools apt to reveal insights on a variety of networks. Topics include: representation learning and Graph Neural Networks; algorithms for the World Wide Web; reasoning over Knowledge Graphs; influence maximization; disease outbreak detection, social network analysis.  View the course website: https://snap.stanford.edu/class/cs224w-2023/|0zuiLBOIcsw
142|1|35|Stanford CS224W: ML with Graphs , 2021 , Lecture 13.4 - Detecting Overlapping Communities|2021-05-25T06:00:04Z|Complex data can be represented as a graph of relationships between objects. Such networks are a fundamental tool for modeling social, technological, and biological systems. This course focuses on the computational, algorithmic, and modeling challenges specific to the analysis of massive graphs. By means of studying the underlying graph structure and its features, students are introduced to machine learning techniques and data mining tools apt to reveal insights on a variety of networks. Topics include: representation learning and Graph Neural Networks; algorithms for the World Wide Web; reasoning over Knowledge Graphs; influence maximization; disease outbreak detection, social network analysis.  View the course website: https://snap.stanford.edu/class/cs224w-2023/|SHcVHrsgj8w
143|1|36|Stanford CS224W: Machine Learning w/ Graphs I 2023 I GNNs for Recommender Systems|2023-12-07T18:19:31Z|Complex data can be represented as a graph of relationships between objects. Such networks are a fundamental tool for modeling social, technological, and biological systems. This course focuses on the computational, algorithmic, and modeling challenges specific to the analysis of massive graphs. By means of studying the underlying graph structure and its features, students are introduced to machine learning techniques and data mining tools apt to reveal insights on a variety of networks. Topics include: representation learning and Graph Neural Networks; algorithms for the World Wide Web; reasoning over Knowledge Graphs; influence maximization; disease outbreak detection, social network analysis.  View the course website: https://snap.stanford.edu/class/cs224w-2023/|OV2VUApLUio
144|1|37|Stanford CS224W: ML with Graphs , 2021 , Lecture 15.1 - Deep Generative Models for Graphs|2021-06-01T06:00:06Z|Complex data can be represented as a graph of relationships between objects. Such networks are a fundamental tool for modeling social, technological, and biological systems. This course focuses on the computational, algorithmic, and modeling challenges specific to the analysis of massive graphs. By means of studying the underlying graph structure and its features, students are introduced to machine learning techniques and data mining tools apt to reveal insights on a variety of networks. Topics include: representation learning and Graph Neural Networks; algorithms for the World Wide Web; reasoning over Knowledge Graphs; influence maximization; disease outbreak detection, social network analysis.  View the course website: https://snap.stanford.edu/class/cs224w-2023/|IMpkHvQ0LA4
145|1|38|Stanford CS224W: ML with Graphs , 2021 , Lecture 15.2 - Graph RNN: Generating Realistic Graphs|2021-06-01T06:00:10Z|Complex data can be represented as a graph of relationships between objects. Such networks are a fundamental tool for modeling social, technological, and biological systems. This course focuses on the computational, algorithmic, and modeling challenges specific to the analysis of massive graphs. By means of studying the underlying graph structure and its features, students are introduced to machine learning techniques and data mining tools apt to reveal insights on a variety of networks. Topics include: representation learning and Graph Neural Networks; algorithms for the World Wide Web; reasoning over Knowledge Graphs; influence maximization; disease outbreak detection, social network analysis.  View the course website: https://snap.stanford.edu/class/cs224w-2023/|xGDUYQGvRac
146|1|39|Stanford CS224W: ML with Graphs , 2021 , Lecture 15.3 - Scaling Up & Evaluating Graph Gen|2021-06-01T06:00:07Z|Complex data can be represented as a graph of relationships between objects. Such networks are a fundamental tool for modeling social, technological, and biological systems. This course focuses on the computational, algorithmic, and modeling challenges specific to the analysis of massive graphs. By means of studying the underlying graph structure and its features, students are introduced to machine learning techniques and data mining tools apt to reveal insights on a variety of networks. Topics include: representation learning and Graph Neural Networks; algorithms for the World Wide Web; reasoning over Knowledge Graphs; influence maximization; disease outbreak detection, social network analysis.  View the course website: https://snap.stanford.edu/class/cs224w-2023/|kDu5YY9d1TM
147|1|40|Stanford CS224W: ML with Graphs , 2021 , Lecture 15.4 - Applications of Deep Graph Generation|2021-06-01T06:00:14Z|Complex data can be represented as a graph of relationships between objects. Such networks are a fundamental tool for modeling social, technological, and biological systems. This course focuses on the computational, algorithmic, and modeling challenges specific to the analysis of massive graphs. By means of studying the underlying graph structure and its features, students are introduced to machine learning techniques and data mining tools apt to reveal insights on a variety of networks. Topics include: representation learning and Graph Neural Networks; algorithms for the World Wide Web; reasoning over Knowledge Graphs; influence maximization; disease outbreak detection, social network analysis.  View the course website: https://snap.stanford.edu/class/cs224w-2023/|SdBk8fMmwUU
148|1|41|Stanford CS224W: Machine Learning w/ Graphs I 2023 I Advanced Topics in GNNs|2023-12-07T18:19:39Z|Complex data can be represented as a graph of relationships between objects. Such networks are a fundamental tool for modeling social, technological, and biological systems. This course focuses on the computational, algorithmic, and modeling challenges specific to the analysis of massive graphs. By means of studying the underlying graph structure and its features, students are introduced to machine learning techniques and data mining tools apt to reveal insights on a variety of networks. Topics include: representation learning and Graph Neural Networks; algorithms for the World Wide Web; reasoning over Knowledge Graphs; influence maximization; disease outbreak detection, social network analysis.  View the course website: https://snap.stanford.edu/class/cs224w-2023/|xVQN2aSNQhU
149|1|42|Stanford CS224W: ML with Graphs , 2021 , Lecture 17.1 - Scaling up Graph Neural Networks|2021-06-08T06:00:00Z|Complex data can be represented as a graph of relationships between objects. Such networks are a fundamental tool for modeling social, technological, and biological systems. This course focuses on the computational, algorithmic, and modeling challenges specific to the analysis of massive graphs. By means of studying the underlying graph structure and its features, students are introduced to machine learning techniques and data mining tools apt to reveal insights on a variety of networks. Topics include: representation learning and Graph Neural Networks; algorithms for the World Wide Web; reasoning over Knowledge Graphs; influence maximization; disease outbreak detection, social network analysis.  View the course website: https://snap.stanford.edu/class/cs224w-2023/|2nPCw3yHlnI
150|1|43|Stanford CS224W: Machine Learning with Graphs , 2021 , Lecture 17.2 - GraphSAGE Neighbor Sampling|2021-06-08T06:00:04Z|Complex data can be represented as a graph of relationships between objects. Such networks are a fundamental tool for modeling social, technological, and biological systems. This course focuses on the computational, algorithmic, and modeling challenges specific to the analysis of massive graphs. By means of studying the underlying graph structure and its features, students are introduced to machine learning techniques and data mining tools apt to reveal insights on a variety of networks. Topics include: representation learning and Graph Neural Networks; algorithms for the World Wide Web; reasoning over Knowledge Graphs; influence maximization; disease outbreak detection, social network analysis.  View the course website: https://snap.stanford.edu/class/cs224w-2023/|LLUxwHc7O4A
151|1|44|Stanford CS224W: Machine Learning with Graphs , 2021 , Lecture 17.3 - Cluster GCN: Scaling up GNNs|2021-06-08T06:00:05Z|Complex data can be represented as a graph of relationships between objects. Such networks are a fundamental tool for modeling social, technological, and biological systems. This course focuses on the computational, algorithmic, and modeling challenges specific to the analysis of massive graphs. By means of studying the underlying graph structure and its features, students are introduced to machine learning techniques and data mining tools apt to reveal insights on a variety of networks. Topics include: representation learning and Graph Neural Networks; algorithms for the World Wide Web; reasoning over Knowledge Graphs; influence maximization; disease outbreak detection, social network analysis.  View the course website: https://snap.stanford.edu/class/cs224w-2023/|RJkR8Ig6dXI
152|1|45|Stanford CS224W: Machine Learning with Graphs , 2021 , Lecture 17.4 - Scaling up by Simplifying GNNs|2021-06-08T06:00:04Z|Complex data can be represented as a graph of relationships between objects. Such networks are a fundamental tool for modeling social, technological, and biological systems. This course focuses on the computational, algorithmic, and modeling challenges specific to the analysis of massive graphs. By means of studying the underlying graph structure and its features, students are introduced to machine learning techniques and data mining tools apt to reveal insights on a variety of networks. Topics include: representation learning and Graph Neural Networks; algorithms for the World Wide Web; reasoning over Knowledge Graphs; influence maximization; disease outbreak detection, social network analysis.  View the course website: https://snap.stanford.edu/class/cs224w-2023/|iTRW9Gh7yKI
153|1|46|Stanford CS224W: Machine Learning w/ Graphs I 2023 I Geometric Graph Learning, Minkai Xu|2023-12-07T18:19:46Z|Complex data can be represented as a graph of relationships between objects. Such networks are a fundamental tool for modeling social, technological, and biological systems. This course focuses on the computational, algorithmic, and modeling challenges specific to the analysis of massive graphs. By means of studying the underlying graph structure and its features, students are introduced to machine learning techniques and data mining tools apt to reveal insights on a variety of networks. Topics include: representation learning and Graph Neural Networks; algorithms for the World Wide Web; reasoning over Knowledge Graphs; influence maximization; disease outbreak detection, social network analysis.  View the course website: https://snap.stanford.edu/class/cs224w-2023/|hDnlfTx4dZs
154|1|47|Stanford CS224W: Machine Learning w/ Graphs I 2023 I Trustworthy Graph AI,  Rex Ying|2023-12-07T18:19:52Z|Complex data can be represented as a graph of relationships between objects. Such networks are a fundamental tool for modeling social, technological, and biological systems. This course focuses on the computational, algorithmic, and modeling challenges specific to the analysis of massive graphs. By means of studying the underlying graph structure and its features, students are introduced to machine learning techniques and data mining tools apt to reveal insights on a variety of networks. Topics include: representation learning and Graph Neural Networks; algorithms for the World Wide Web; reasoning over Knowledge Graphs; influence maximization; disease outbreak detection, social network analysis.  View the course website: https://snap.stanford.edu/class/cs224w-2023/|O6eTvxWy07A
155|2|1|Stanford CS109 Probability for Computer Scientists I Counting I 2022 I Lecture 1|2023-10-09T20:11:45Z|The class starts by providing a fundamental grounding in combinatorics, and then quickly moves into the basics of probability theory. We will then cover many essential concepts in probability theory, including particular probability distributions, properties of probabilities, and mathematical tools for analyzing probabilities. Finally, the last third of the class will focus on data analysis and Machine Learning as a means for seeing direct applications of probability in this exciting and quickly growing subfield of computer science.  Overview of Topics: Counting Theory, Core Probability, Random Variables, Probabilistic Models, Uncertainty Theory, Machine Learning|2MuDZIAzBMY
156|2|2|Stanford CS109 Probability for Computer Scientists I Combinatorics I 2022 I Lecture 2|2023-10-09T20:11:50Z|The class starts by providing a fundamental grounding in combinatorics, and then quickly moves into the basics of probability theory. We will then cover many essential concepts in probability theory, including particular probability distributions, properties of probabilities, and mathematical tools for analyzing probabilities. Finally, the last third of the class will focus on data analysis and Machine Learning as a means for seeing direct applications of probability in this exciting and quickly growing subfield of computer science.  Overview of Topics: Counting Theory, Core Probability, Random Variables, Probabilistic Models, Uncertainty Theory, Machine Learning|ag4Ei15CG0c
157|2|3|Stanford CS109 Probability for Computer Scientists I What is Probability? I 2022 I Lecture 3|2023-10-09T20:11:54Z|The class starts by providing a fundamental grounding in combinatorics, and then quickly moves into the basics of probability theory. We will then cover many essential concepts in probability theory, including particular probability distributions, properties of probabilities, and mathematical tools for analyzing probabilities. Finally, the last third of the class will focus on data analysis and Machine Learning as a means for seeing direct applications of probability in this exciting and quickly growing subfield of computer science.  Overview of Topics: Counting Theory, Core Probability, Random Variables, Probabilistic Models, Uncertainty Theory, Machine Learning|EGgMCE2AgyU
158|2|4|Stanford CS109 I Conditional Probability and Bayes I 2022 I Lecture 4|2023-10-09T20:11:57Z|The class starts by providing a fundamental grounding in combinatorics, and then quickly moves into the basics of probability theory. We will then cover many essential concepts in probability theory, including particular probability distributions, properties of probabilities, and mathematical tools for analyzing probabilities. Finally, the last third of the class will focus on data analysis and Machine Learning as a means for seeing direct applications of probability in this exciting and quickly growing subfield of computer science.  Overview of Topics: Counting Theory, Core Probability, Random Variables, Probabilistic Models, Uncertainty Theory, Machine Learning|NHRoXvPaZqY
159|2|5|Stanford CS109 Probability for Computer Scientists I Independence I 2022 I Lecture 5|2023-10-09T20:12:00Z|The class starts by providing a fundamental grounding in combinatorics, and then quickly moves into the basics of probability theory. We will then cover many essential concepts in probability theory, including particular probability distributions, properties of probabilities, and mathematical tools for analyzing probabilities. Finally, the last third of the class will focus on data analysis and Machine Learning as a means for seeing direct applications of probability in this exciting and quickly growing subfield of computer science.  Overview of Topics: Counting Theory, Core Probability, Random Variables, Probabilistic Models, Uncertainty Theory, Machine Learning|zTJDZ2wmaRU
160|2|6|Stanford CS109 I Random Variables and Expectation I 2022 I Lecture 6|2023-10-09T20:12:04Z|The class starts by providing a fundamental grounding in combinatorics, and then quickly moves into the basics of probability theory. We will then cover many essential concepts in probability theory, including particular probability distributions, properties of probabilities, and mathematical tools for analyzing probabilities. Finally, the last third of the class will focus on data analysis and Machine Learning as a means for seeing direct applications of probability in this exciting and quickly growing subfield of computer science.  Overview of Topics: Counting Theory, Core Probability, Random Variables, Probabilistic Models, Uncertainty Theory, Machine Learning|8QCg2ur-3fo
161|2|7|Stanford CS109 Probability for Computer Scientists I Variance Bernoulli Binomial I 2022 I Lecture 7|2023-10-09T20:12:07Z|The class starts by providing a fundamental grounding in combinatorics, and then quickly moves into the basics of probability theory. We will then cover many essential concepts in probability theory, including particular probability distributions, properties of probabilities, and mathematical tools for analyzing probabilities. Finally, the last third of the class will focus on data analysis and Machine Learning as a means for seeing direct applications of probability in this exciting and quickly growing subfield of computer science.  Overview of Topics: Counting Theory, Core Probability, Random Variables, Probabilistic Models, Uncertainty Theory, Machine Learning|I2UBspTNAG0
162|2|8|Stanford CS109 Probability for Computer Scientists I Poisson I 2022 I Lecture 8|2023-10-09T20:12:10Z|The class starts by providing a fundamental grounding in combinatorics, and then quickly moves into the basics of probability theory. We will then cover many essential concepts in probability theory, including particular probability distributions, properties of probabilities, and mathematical tools for analyzing probabilities. Finally, the last third of the class will focus on data analysis and Machine Learning as a means for seeing direct applications of probability in this exciting and quickly growing subfield of computer science.  Overview of Topics: Counting Theory, Core Probability, Random Variables, Probabilistic Models, Uncertainty Theory, Machine Learning|QV3IRiG6dVs
163|2|9|Stanford CS109 Probability for Computer Scientists I Continuous Random Variables I 2022 I Lecture 9|2023-10-09T20:12:14Z|The class starts by providing a fundamental grounding in combinatorics, and then quickly moves into the basics of probability theory. We will then cover many essential concepts in probability theory, including particular probability distributions, properties of probabilities, and mathematical tools for analyzing probabilities. Finally, the last third of the class will focus on data analysis and Machine Learning as a means for seeing direct applications of probability in this exciting and quickly growing subfield of computer science.  Overview of Topics: Counting Theory, Core Probability, Random Variables, Probabilistic Models, Uncertainty Theory, Machine Learning|OFgBn4rQkqc
164|2|10|Stanford CS109 Probability for Computer Scientists I Normal Distribution I 2022 I Lecture 10|2023-10-09T20:12:19Z|The class starts by providing a fundamental grounding in combinatorics, and then quickly moves into the basics of probability theory. We will then cover many essential concepts in probability theory, including particular probability distributions, properties of probabilities, and mathematical tools for analyzing probabilities. Finally, the last third of the class will focus on data analysis and Machine Learning as a means for seeing direct applications of probability in this exciting and quickly growing subfield of computer science.  Overview of Topics: Counting Theory, Core Probability, Random Variables, Probabilistic Models, Uncertainty Theory, Machine Learning|rpB_NNXiWlM
165|2|11|Stanford CS109 Probability for Computer Scientists I Joint Distributions I 2022 I Lecture 11|2023-10-09T20:12:17Z|The class starts by providing a fundamental grounding in combinatorics, and then quickly moves into the basics of probability theory. We will then cover many essential concepts in probability theory, including particular probability distributions, properties of probabilities, and mathematical tools for analyzing probabilities. Finally, the last third of the class will focus on data analysis and Machine Learning as a means for seeing direct applications of probability in this exciting and quickly growing subfield of computer science.  Overview of Topics: Counting Theory, Core Probability, Random Variables, Probabilistic Models, Uncertainty Theory, Machine Learning|8Il2M7kbQSc
166|2|12|Stanford CS109 Probability for Computer Scientists I Inference I 2022 I Lecture 12|2023-10-09T20:12:21Z|The class starts by providing a fundamental grounding in combinatorics, and then quickly moves into the basics of probability theory. We will then cover many essential concepts in probability theory, including particular probability distributions, properties of probabilities, and mathematical tools for analyzing probabilities. Finally, the last third of the class will focus on data analysis and Machine Learning as a means for seeing direct applications of probability in this exciting and quickly growing subfield of computer science.  Overview of Topics: Counting Theory, Core Probability, Random Variables, Probabilistic Models, Uncertainty Theory, Machine Learning|fvgQBAsg5Zo
167|2|13|Stanford CS109 Probability for Computer Scientists I Inference II I 2022 I Lecture 13|2023-10-09T20:12:24Z|The class starts by providing a fundamental grounding in combinatorics, and then quickly moves into the basics of probability theory. We will then cover many essential concepts in probability theory, including particular probability distributions, properties of probabilities, and mathematical tools for analyzing probabilities. Finally, the last third of the class will focus on data analysis and Machine Learning as a means for seeing direct applications of probability in this exciting and quickly growing subfield of computer science.  Overview of Topics: Counting Theory, Core Probability, Random Variables, Probabilistic Models, Uncertainty Theory, Machine Learning|d0ImA7m4BEg
168|2|14|Stanford CS109 Probability for Computer Scientists I Modelling I 2022 I Lecture 14|2023-10-09T20:12:27Z|The class starts by providing a fundamental grounding in combinatorics, and then quickly moves into the basics of probability theory. We will then cover many essential concepts in probability theory, including particular probability distributions, properties of probabilities, and mathematical tools for analyzing probabilities. Finally, the last third of the class will focus on data analysis and Machine Learning as a means for seeing direct applications of probability in this exciting and quickly growing subfield of computer science.  Overview of Topics: Counting Theory, Core Probability, Random Variables, Probabilistic Models, Uncertainty Theory, Machine Learning|q9lk8l8P-E4
169|2|15|Stanford CS109 Probability for Computer Scientists I General Inference I 2022 I Lecture 15|2023-10-09T20:12:31Z|The class starts by providing a fundamental grounding in combinatorics, and then quickly moves into the basics of probability theory. We will then cover many essential concepts in probability theory, including particular probability distributions, properties of probabilities, and mathematical tools for analyzing probabilities. Finally, the last third of the class will focus on data analysis and Machine Learning as a means for seeing direct applications of probability in this exciting and quickly growing subfield of computer science.  Overview of Topics: Counting Theory, Core Probability, Random Variables, Probabilistic Models, Uncertainty Theory, Machine Learning|c0QGjtu9GZg
170|2|16|Stanford CS109 Probability for Computer Scientists I Beta I 2022 I Lecture 16|2023-10-09T20:12:35Z|The class starts by providing a fundamental grounding in combinatorics, and then quickly moves into the basics of probability theory. We will then cover many essential concepts in probability theory, including particular probability distributions, properties of probabilities, and mathematical tools for analyzing probabilities. Finally, the last third of the class will focus on data analysis and Machine Learning as a means for seeing direct applications of probability in this exciting and quickly growing subfield of computer science.  Overview of Topics: Counting Theory, Core Probability, Random Variables, Probabilistic Models, Uncertainty Theory, Machine Learning|aOhk9mFrHdU
171|2|17|Stanford CS109 Probability for Computer Scientists I Adding Random Variables I 2022 I Lecture 17|2023-10-09T20:12:37Z|The class starts by providing a fundamental grounding in combinatorics, and then quickly moves into the basics of probability theory. We will then cover many essential concepts in probability theory, including particular probability distributions, properties of probabilities, and mathematical tools for analyzing probabilities. Finally, the last third of the class will focus on data analysis and Machine Learning as a means for seeing direct applications of probability in this exciting and quickly growing subfield of computer science.  Overview of Topics: Counting Theory, Core Probability, Random Variables, Probabilistic Models, Uncertainty Theory, Machine Learning|UEyHbI9FRtM
172|2|18|Stanford CS109 I Central Limit Theorem I 2022 I Lecture 18|2023-10-17T19:55:59Z|The class starts by providing a fundamental grounding in combinatorics, and then quickly moves into the basics of probability theory. We will then cover many essential concepts in probability theory, including particular probability distributions, properties of probabilities, and mathematical tools for analyzing probabilities. Finally, the last third of the class will focus on data analysis and Machine Learning as a means for seeing direct applications of probability in this exciting and quickly growing subfield of computer science.  Overview of Topics: Counting Theory, Core Probability, Random Variables, Probabilistic Models, Uncertainty Theory, Machine Learning|6Q9wT6JGMMM
173|2|19|Stanford CS109 Probability for Computer Scientists I Bootstraping and P-Values  I 2022 I Lecture 19|2023-10-09T20:12:40Z|The class starts by providing a fundamental grounding in combinatorics, and then quickly moves into the basics of probability theory. We will then cover many essential concepts in probability theory, including particular probability distributions, properties of probabilities, and mathematical tools for analyzing probabilities. Finally, the last third of the class will focus on data analysis and Machine Learning as a means for seeing direct applications of probability in this exciting and quickly growing subfield of computer science.  Overview of Topics: Counting Theory, Core Probability, Random Variables, Probabilistic Models, Uncertainty Theory, Machine Learning|NXJwyPT1vsc
174|2|20|Stanford CS109 I Algorithmic Analysis I 2022 I Lecture 20|2023-10-17T19:56:23Z|The class starts by providing a fundamental grounding in combinatorics, and then quickly moves into the basics of probability theory. We will then cover many essential concepts in probability theory, including particular probability distributions, properties of probabilities, and mathematical tools for analyzing probabilities. Finally, the last third of the class will focus on data analysis and Machine Learning as a means for seeing direct applications of probability in this exciting and quickly growing subfield of computer science.  Overview of Topics: Counting Theory, Core Probability, Random Variables, Probabilistic Models, Uncertainty Theory, Machine Learning|Ht9yUPtppwY
175|2|21|Stanford CS109 Probability for Computer Scientists I M.L.E. I 2022 I Lecture 21|2023-10-09T20:12:43Z|The class starts by providing a fundamental grounding in combinatorics, and then quickly moves into the basics of probability theory. We will then cover many essential concepts in probability theory, including particular probability distributions, properties of probabilities, and mathematical tools for analyzing probabilities. Finally, the last third of the class will focus on data analysis and Machine Learning as a means for seeing direct applications of probability in this exciting and quickly growing subfield of computer science.  Overview of Topics: Counting Theory, Core Probability, Random Variables, Probabilistic Models, Uncertainty Theory, Machine Learning|utFEufMXHgw
176|2|22|Stanford CS109 Probability for Computer Scientists I  M.A.P. I 2022 I Lecture 22|2023-10-09T20:12:46Z|The class starts by providing a fundamental grounding in combinatorics, and then quickly moves into the basics of probability theory. We will then cover many essential concepts in probability theory, including particular probability distributions, properties of probabilities, and mathematical tools for analyzing probabilities. Finally, the last third of the class will focus on data analysis and Machine Learning as a means for seeing direct applications of probability in this exciting and quickly growing subfield of computer science.  Overview of Topics: Counting Theory, Core Probability, Random Variables, Probabilistic Models, Uncertainty Theory, Machine Learning|sL1zOr-P4xc
177|2|23|Stanford CS109 Probability for Computer Scientists I Naive Bayes I 2022 I Lecture 23|2023-10-09T20:12:50Z|The class starts by providing a fundamental grounding in combinatorics, and then quickly moves into the basics of probability theory. We will then cover many essential concepts in probability theory, including particular probability distributions, properties of probabilities, and mathematical tools for analyzing probabilities. Finally, the last third of the class will focus on data analysis and Machine Learning as a means for seeing direct applications of probability in this exciting and quickly growing subfield of computer science.  Overview of Topics: Counting Theory, Core Probability, Random Variables, Probabilistic Models, Uncertainty Theory, Machine Learning|yqF3DvDVpvw
178|2|24|Stanford CS109 Probability for Computer Scientists I Logistic Regression I 2022 I Lecture 24|2023-10-09T20:12:54Z|The class starts by providing a fundamental grounding in combinatorics, and then quickly moves into the basics of probability theory. We will then cover many essential concepts in probability theory, including particular probability distributions, properties of probabilities, and mathematical tools for analyzing probabilities. Finally, the last third of the class will focus on data analysis and Machine Learning as a means for seeing direct applications of probability in this exciting and quickly growing subfield of computer science.  Overview of Topics: Counting Theory, Core Probability, Random Variables, Probabilistic Models, Uncertainty Theory, Machine Learning|ILqZWvDWKEc
179|2|25|Stanford CS109 I Deep Learning I 2022 I Lecture 25|2023-10-17T19:56:05Z|The class starts by providing a fundamental grounding in combinatorics, and then quickly moves into the basics of probability theory. We will then cover many essential concepts in probability theory, including particular probability distributions, properties of probabilities, and mathematical tools for analyzing probabilities. Finally, the last third of the class will focus on data analysis and Machine Learning as a means for seeing direct applications of probability in this exciting and quickly growing subfield of computer science.  Overview of Topics: Counting Theory, Core Probability, Random Variables, Probabilistic Models, Uncertainty Theory, Machine Learning|MSfI6TTgyl4
180|2|26|Stanford CS109 I Fairness I 2022 I Lecture 26|2023-10-17T19:56:08Z|The class starts by providing a fundamental grounding in combinatorics, and then quickly moves into the basics of probability theory. We will then cover many essential concepts in probability theory, including particular probability distributions, properties of probabilities, and mathematical tools for analyzing probabilities. Finally, the last third of the class will focus on data analysis and Machine Learning as a means for seeing direct applications of probability in this exciting and quickly growing subfield of computer science.  Overview of Topics: Counting Theory, Core Probability, Random Variables, Probabilistic Models, Uncertainty Theory, Machine Learning|cbzwbr5H_LA
181|2|27|Stanford CS109 I Advanced Probability I 2022 I Lecture 27|2023-10-17T19:56:10Z|The class starts by providing a fundamental grounding in combinatorics, and then quickly moves into the basics of probability theory. We will then cover many essential concepts in probability theory, including particular probability distributions, properties of probabilities, and mathematical tools for analyzing probabilities. Finally, the last third of the class will focus on data analysis and Machine Learning as a means for seeing direct applications of probability in this exciting and quickly growing subfield of computer science.  Overview of Topics: Counting Theory, Core Probability, Random Variables, Probabilistic Models, Uncertainty Theory, Machine Learning|BquE8Z9htws
182|2|28|Stanford CS109 I Future of Probability I 2022 I Lecture 28|2023-10-17T19:56:14Z|The class starts by providing a fundamental grounding in combinatorics, and then quickly moves into the basics of probability theory. We will then cover many essential concepts in probability theory, including particular probability distributions, properties of probabilities, and mathematical tools for analyzing probabilities. Finally, the last third of the class will focus on data analysis and Machine Learning as a means for seeing direct applications of probability in this exciting and quickly growing subfield of computer science.  Overview of Topics: Counting Theory, Core Probability, Random Variables, Probabilistic Models, Uncertainty Theory, Machine Learning|SoXygq5LtiM
183|2|29|Stanford CS109 Probability for Computer Scientists I Counting I 2022 I Lecture 29|2023-10-26T23:15:45Z|The class starts by providing a fundamental grounding in combinatorics, and then quickly moves into the basics of probability theory. We will then cover many essential concepts in probability theory, including particular probability distributions, properties of probabilities, and mathematical tools for analyzing probabilities. Finally, the last third of the class will focus on data analysis and Machine Learning as a means for seeing direct applications of probability in this exciting and quickly growing subfield of computer science.  Overview of Topics: Counting Theory, Core Probability, Random Variables, Probabilistic Models, Uncertainty Theory, Machine Learning|yyKSsjRt42o
184|3|1|Stanford CS224N: NLP with Deep Learning , Winter 2021 , Lecture 1 - Intro & Word Vectors|2021-10-29T01:16:01Z|Natural language processing (NLP) is a crucial part of artificial intelligence (AI), modeling how people share information. In recent years, deep learning approaches have obtained very high performance on many NLP tasks. In this course, students gain a thorough introduction to cutting-edge neural networks for NLP.|rmVRLeJRkl4
185|3|2|Stanford CS224N NLP with Deep Learning , Winter 2021 , Lecture 2 - Neural Classifiers|2021-10-29T01:16:17Z|Natural language processing (NLP) is a crucial part of artificial intelligence (AI), modeling how people share information. In recent years, deep learning approaches have obtained very high performance on many NLP tasks. In this course, students gain a thorough introduction to cutting-edge neural networks for NLP.|gqaHkPEZAew
186|3|3|Stanford CS224N NLP with Deep Learning , Winter 2021 , Lecture 3 - Backprop and Neural Networks|2021-10-29T01:16:36Z|Natural language processing (NLP) is a crucial part of artificial intelligence (AI), modeling how people share information. In recent years, deep learning approaches have obtained very high performance on many NLP tasks. In this course, students gain a thorough introduction to cutting-edge neural networks for NLP.|X0Jw4kgaFlg
187|3|4|Stanford CS224N - NLP w/ DL , Winter 2021 , Lecture 4 - Syntactic Structure and Dependency Parsing|2021-10-29T01:16:52Z|Natural language processing (NLP) is a crucial part of artificial intelligence (AI), modeling how people share information. In recent years, deep learning approaches have obtained very high performance on many NLP tasks. In this course, students gain a thorough introduction to cutting-edge neural networks for NLP.|PSGIodTN3KE
188|3|5|Stanford CS224N -  NLP w/ DL , Winter 2021 , Lecture 5 - Recurrent Neural networks (RNNs)|2021-10-29T01:17:12Z|Natural language processing (NLP) is a crucial part of artificial intelligence (AI), modeling how people share information. In recent years, deep learning approaches have obtained very high performance on many NLP tasks. In this course, students gain a thorough introduction to cutting-edge neural networks for NLP.|PLryWeHPcBs
189|3|6|Stanford CS224N NLP with Deep Learning , Winter 2021 , Lecture 6 - Simple and LSTM RNNs|2021-10-29T01:17:28Z|Natural language processing (NLP) is a crucial part of artificial intelligence (AI), modeling how people share information. In recent years, deep learning approaches have obtained very high performance on many NLP tasks. In this course, students gain a thorough introduction to cutting-edge neural networks for NLP.|0LixFSa7yts
190|3|7|Stanford CS224N NLP with Deep Learning , Winter 2021 , Lecture 7 - Translation, Seq2Seq, Attention|2021-10-29T01:18:05Z|Natural language processing (NLP) is a crucial part of artificial intelligence (AI), modeling how people share information. In recent years, deep learning approaches have obtained very high performance on many NLP tasks. In this course, students gain a thorough introduction to cutting-edge neural networks for NLP.|wzfWHP6SXxY
191|3|8|Stanford CS224N NLP with Deep Learning , 2023 , Lecture 8 - Self-Attention and Transformers|2023-09-20T00:10:28Z|Natural language processing (NLP) is a crucial part of artificial intelligence (AI), modeling how people share information. In recent years, deep learning approaches have obtained very high performance on many NLP tasks. In this course, students gain a thorough introduction to cutting-edge neural networks for NLP.|LWMzyfvuehA
192|3|9|Stanford CS224N NLP with Deep Learning , 2023 , Lecture 9 - Pretraining|2023-09-20T00:10:30Z|Natural language processing (NLP) is a crucial part of artificial intelligence (AI), modeling how people share information. In recent years, deep learning approaches have obtained very high performance on many NLP tasks. In this course, students gain a thorough introduction to cutting-edge neural networks for NLP.|DGfCRXuNA2w
193|3|10|Stanford CS224N , 2023 , Lecture 10 - Prompting, Reinforcement Learning from Human Feedback|2023-09-20T00:10:34Z|Natural language processing (NLP) is a crucial part of artificial intelligence (AI), modeling how people share information. In recent years, deep learning approaches have obtained very high performance on many NLP tasks. In this course, students gain a thorough introduction to cutting-edge neural networks for NLP.|SXpJ9EmG3s4
194|3|11|Stanford CS224N NLP with Deep Learning , 2023 , Lecture 11 - Natural Language Generation|2023-09-20T00:10:37Z|Natural language processing (NLP) is a crucial part of artificial intelligence (AI), modeling how people share information. In recent years, deep learning approaches have obtained very high performance on many NLP tasks. In this course, students gain a thorough introduction to cutting-edge neural networks for NLP.|N9L32bFieEY
195|3|12|Stanford CS224N NLP with Deep Learning , Winter 2021 , Lecture 12 - Question Answering|2021-10-29T01:21:20Z|Natural language processing (NLP) is a crucial part of artificial intelligence (AI), modeling how people share information. In recent years, deep learning approaches have obtained very high performance on many NLP tasks. In this course, students gain a thorough introduction to cutting-edge neural networks for NLP.|NcqfHa0_YmU
196|3|13|Stanford CS224N NLP with Deep Learning , Winter 2021 , Lecture 13 - Coreference Resolution|2021-10-29T01:21:54Z|Natural language processing (NLP) is a crucial part of artificial intelligence (AI), modeling how people share information. In recent years, deep learning approaches have obtained very high performance on many NLP tasks. In this course, students gain a thorough introduction to cutting-edge neural networks for NLP.|FFRnDRcbQQU
197|3|14|Stanford CS224N NLP with Deep Learning , 2023 , Lecture 14 - Insights between NLP and Linguistics|2023-09-20T00:10:41Z|Natural language processing (NLP) is a crucial part of artificial intelligence (AI), modeling how people share information. In recent years, deep learning approaches have obtained very high performance on many NLP tasks. In this course, students gain a thorough introduction to cutting-edge neural networks for NLP.|NvIWF82Fw4E
198|3|15|Stanford CS224N NLP with Deep Learning , Winter 2021 , Lecture 15 - Add Knowledge to Language Models|2021-10-29T01:22:31Z|Natural language processing (NLP) is a crucial part of artificial intelligence (AI), modeling how people share information. In recent years, deep learning approaches have obtained very high performance on many NLP tasks. In this course, students gain a thorough introduction to cutting-edge neural networks for NLP.|y68RJVfGoto
199|3|16|Stanford CS224N NLP with Deep Learning , 2023 , Lecture 15 - Code Generation|2023-09-20T00:10:44Z|Natural language processing (NLP) is a crucial part of artificial intelligence (AI), modeling how people share information. In recent years, deep learning approaches have obtained very high performance on many NLP tasks. In this course, students gain a thorough introduction to cutting-edge neural networks for NLP.|JlK46EzImM8
200|3|17|Stanford CS224N NLP with Deep Learning , Winter 2021 , Lecture 17 - Model Analysis and Explanation|2021-10-29T01:23:57Z|Natural language processing (NLP) is a crucial part of artificial intelligence (AI), modeling how people share information. In recent years, deep learning approaches have obtained very high performance on many NLP tasks. In this course, students gain a thorough introduction to cutting-edge neural networks for NLP.|f_qmSSBWV_E
201|3|18|Stanford CS224N NLP with Deep Learning , Winter 2021 , Lecture 18 - Future of NLP + Deep Learning|2021-10-29T01:24:24Z|Natural language processing (NLP) is a crucial part of artificial intelligence (AI), modeling how people share information. In recent years, deep learning approaches have obtained very high performance on many NLP tasks. In this course, students gain a thorough introduction to cutting-edge neural networks for NLP.|2t7Q9WVUaf8
202|3|19|Stanford CS224N NLP with Deep Learning , 2023 , Python Tutorial, Manasi Sharma|2023-09-20T00:10:56Z|Natural language processing (NLP) is a crucial part of artificial intelligence (AI), modeling how people share information. In recent years, deep learning approaches have obtained very high performance on many NLP tasks. In this course, students gain a thorough introduction to cutting-edge neural networks for NLP.|8j4wpU98Q74
203|3|20|Stanford CS224N NLP with Deep Learning , 2023 , PyTorch Tutorial,  Drew Kaul|2023-09-20T00:10:59Z|Natural language processing (NLP) is a crucial part of artificial intelligence (AI), modeling how people share information. In recent years, deep learning approaches have obtained very high performance on many NLP tasks. In this course, students gain a thorough introduction to cutting-edge neural networks for NLP.|Uv0AIRr3ptg
204|3|21|Stanford CS224N NLP with Deep Learning , 2023 , Hugging Face Tutorial, Eric Frankel|2023-09-20T00:11:03Z|Natural language processing (NLP) is a crucial part of artificial intelligence (AI), modeling how people share information. In recent years, deep learning approaches have obtained very high performance on many NLP tasks. In this course, students gain a thorough introduction to cutting-edge neural networks for NLP.|b80by3Xk_A8
205|3|22|Stanford CS224N NLP with Deep Learning , 2023 , Lecture 16 - Multimodal Deep Learning, Douwe Kiela|2023-09-20T00:10:51Z|Natural language processing (NLP) is a crucial part of artificial intelligence (AI), modeling how people share information. In recent years, deep learning approaches have obtained very high performance on many NLP tasks. In this course, students gain a thorough introduction to cutting-edge neural networks for NLP.|5vfIT5LOkR0
206|3|23|Stanford CS224N NLP with Deep Learning , 2023 , Lec. 19 - Model Interpretability & Editing, Been Kim|2023-09-20T00:10:48Z|Natural language processing (NLP) is a crucial part of artificial intelligence (AI), modeling how people share information. In recent years, deep learning approaches have obtained very high performance on many NLP tasks. In this course, students gain a thorough introduction to cutting-edge neural networks for NLP.|cd3pRpEtjLs
207|4|1|Stanford CS229 Machine Learning I Introduction I 2022 I Lecture 1|2023-08-01T14:00:11Z|This course provides a broad introduction to machine learning and statistical pattern recognition. Topics include: supervised learning (generative/discriminative learning, parametric/non-parametric learning, neural networks, support vector machines); unsupervised learning (clustering, dimensionality reduction, kernel methods); learning theory (bias/variance tradeoffs, practical advice); reinforcement learning and adaptive control. The course will also discuss recent applications of machine learning, such as to robotic control, data mining, autonomous navigation, bioinformatics, speech recognition, and text and web data processing.  View the course website: https://cs229.stanford.edu/syllabus-spring2022.html|Bl4Feh_Mjvo
208|4|2|Stanford CS229 Machine Learning I Supervised learning setup, LMS I 2022 I Lecture 2|2023-08-02T14:00:24Z|This course provides a broad introduction to machine learning and statistical pattern recognition. Topics include: supervised learning (generative/discriminative learning, parametric/non-parametric learning, neural networks, support vector machines); unsupervised learning (clustering, dimensionality reduction, kernel methods); learning theory (bias/variance tradeoffs, practical advice); reinforcement learning and adaptive control. The course will also discuss recent applications of machine learning, such as to robotic control, data mining, autonomous navigation, bioinformatics, speech recognition, and text and web data processing.  View the course website: https://cs229.stanford.edu/syllabus-spring2022.html|gqKaVgQxEJ0
209|4|3|Stanford CS229 I Weighted Least Squares, Logistic regression, Newton's Method I 2022 I Lecture 3|2023-08-03T14:00:35Z|This course provides a broad introduction to machine learning and statistical pattern recognition. Topics include: supervised learning (generative/discriminative learning, parametric/non-parametric learning, neural networks, support vector machines); unsupervised learning (clustering, dimensionality reduction, kernel methods); learning theory (bias/variance tradeoffs, practical advice); reinforcement learning and adaptive control. The course will also discuss recent applications of machine learning, such as to robotic control, data mining, autonomous navigation, bioinformatics, speech recognition, and text and web data processing.  View the course website: https://cs229.stanford.edu/syllabus-spring2022.html|k_pDh_68K6c
210|4|4|Stanford CS229 Machine Learning I Exponential family, Generalized Linear Models I 2022 I Lecture 4|2023-08-04T14:00:35Z|This course provides a broad introduction to machine learning and statistical pattern recognition. Topics include: supervised learning (generative/discriminative learning, parametric/non-parametric learning, neural networks, support vector machines); unsupervised learning (clustering, dimensionality reduction, kernel methods); learning theory (bias/variance tradeoffs, practical advice); reinforcement learning and adaptive control. The course will also discuss recent applications of machine learning, such as to robotic control, data mining, autonomous navigation, bioinformatics, speech recognition, and text and web data processing.  View the course website: https://cs229.stanford.edu/syllabus-spring2022.html|goDDnBbJQ4g
211|4|5|Stanford CS229 Machine Learning I Gaussian discriminant analysis, Naive Bayes I 2022 I Lecture 5|2023-08-05T14:00:30Z|This course provides a broad introduction to machine learning and statistical pattern recognition. Topics include: supervised learning (generative/discriminative learning, parametric/non-parametric learning, neural networks, support vector machines); unsupervised learning (clustering, dimensionality reduction, kernel methods); learning theory (bias/variance tradeoffs, practical advice); reinforcement learning and adaptive control. The course will also discuss recent applications of machine learning, such as to robotic control, data mining, autonomous navigation, bioinformatics, speech recognition, and text and web data processing.  View the course website: https://cs229.stanford.edu/syllabus-spring2022.html|RMy_1mO4HLk
212|4|6|Stanford CS229 Machine Learning I Naive Bayes, Laplace Smoothing I 2022 I Lecture 6|2023-08-06T14:00:06Z|This course provides a broad introduction to machine learning and statistical pattern recognition. Topics include: supervised learning (generative/discriminative learning, parametric/non-parametric learning, neural networks, support vector machines); unsupervised learning (clustering, dimensionality reduction, kernel methods); learning theory (bias/variance tradeoffs, practical advice); reinforcement learning and adaptive control. The course will also discuss recent applications of machine learning, such as to robotic control, data mining, autonomous navigation, bioinformatics, speech recognition, and text and web data processing.  View the course website: https://cs229.stanford.edu/syllabus-spring2022.html|ADj95edZc0w
213|4|7|Stanford CS229 Machine Learning I Kernels I 2022 I Lecture 7|2023-08-07T14:00:34Z|This course provides a broad introduction to machine learning and statistical pattern recognition. Topics include: supervised learning (generative/discriminative learning, parametric/non-parametric learning, neural networks, support vector machines); unsupervised learning (clustering, dimensionality reduction, kernel methods); learning theory (bias/variance tradeoffs, practical advice); reinforcement learning and adaptive control. The course will also discuss recent applications of machine learning, such as to robotic control, data mining, autonomous navigation, bioinformatics, speech recognition, and text and web data processing.  View the course website: https://cs229.stanford.edu/syllabus-spring2022.html|dzDOqrac9Ks
214|4|8|Stanford CS229 Machine Learning I Neural Networks 1 I 2022 I Lecture 8|2023-08-08T14:00:19Z|This course provides a broad introduction to machine learning and statistical pattern recognition. Topics include: supervised learning (generative/discriminative learning, parametric/non-parametric learning, neural networks, support vector machines); unsupervised learning (clustering, dimensionality reduction, kernel methods); learning theory (bias/variance tradeoffs, practical advice); reinforcement learning and adaptive control. The course will also discuss recent applications of machine learning, such as to robotic control, data mining, autonomous navigation, bioinformatics, speech recognition, and text and web data processing.  View the course website: https://cs229.stanford.edu/syllabus-spring2022.html|ZMxfDWPXmjc
215|4|9|Stanford CS229 Machine Learning I Neural Networks 2 (backprop) I 2022 I Lecture 9|2023-08-09T14:00:31Z|This course provides a broad introduction to machine learning and statistical pattern recognition. Topics include: supervised learning (generative/discriminative learning, parametric/non-parametric learning, neural networks, support vector machines); unsupervised learning (clustering, dimensionality reduction, kernel methods); learning theory (bias/variance tradeoffs, practical advice); reinforcement learning and adaptive control. The course will also discuss recent applications of machine learning, such as to robotic control, data mining, autonomous navigation, bioinformatics, speech recognition, and text and web data processing.  View the course website: https://cs229.stanford.edu/syllabus-spring2022.html|UbtTv7j1tzU
216|4|10|Stanford CS229 Machine Learning I Bias - Variance, Regularization I 2022 I Lecture 10|2023-08-10T14:00:08Z|This course provides a broad introduction to machine learning and statistical pattern recognition. Topics include: supervised learning (generative/discriminative learning, parametric/non-parametric learning, neural networks, support vector machines); unsupervised learning (clustering, dimensionality reduction, kernel methods); learning theory (bias/variance tradeoffs, practical advice); reinforcement learning and adaptive control. The course will also discuss recent applications of machine learning, such as to robotic control, data mining, autonomous navigation, bioinformatics, speech recognition, and text and web data processing.  View the course website: https://cs229.stanford.edu/syllabus-spring2022.html|7AQYw5FOVcw
217|4|11|Stanford CS229 Machine Learning I Feature / Model selection, ML Advice I 2022 I Lecture 11|2023-08-11T14:00:13Z|This course provides a broad introduction to machine learning and statistical pattern recognition. Topics include: supervised learning (generative/discriminative learning, parametric/non-parametric learning, neural networks, support vector machines); unsupervised learning (clustering, dimensionality reduction, kernel methods); learning theory (bias/variance tradeoffs, practical advice); reinforcement learning and adaptive control. The course will also discuss recent applications of machine learning, such as to robotic control, data mining, autonomous navigation, bioinformatics, speech recognition, and text and web data processing.  View the course website: https://cs229.stanford.edu/syllabus-spring2022.html|NirZnqwYfYU
218|4|12|Stanford CS229 I K-Means, GMM (non EM), Expectation Maximization I 2022 I Lecture 12|2023-08-12T14:00:27Z|This course provides a broad introduction to machine learning and statistical pattern recognition. Topics include: supervised learning (generative/discriminative learning, parametric/non-parametric learning, neural networks, support vector machines); unsupervised learning (clustering, dimensionality reduction, kernel methods); learning theory (bias/variance tradeoffs, practical advice); reinforcement learning and adaptive control. The course will also discuss recent applications of machine learning, such as to robotic control, data mining, autonomous navigation, bioinformatics, speech recognition, and text and web data processing.  View the course website: https://cs229.stanford.edu/syllabus-spring2022.html|LMZZPneTcP4
219|4|13|Stanford CS229 Machine Learning I GMM (EM) I 2022 I Lecture 13|2023-08-13T14:00:34Z|This course provides a broad introduction to machine learning and statistical pattern recognition. Topics include: supervised learning (generative/discriminative learning, parametric/non-parametric learning, neural networks, support vector machines); unsupervised learning (clustering, dimensionality reduction, kernel methods); learning theory (bias/variance tradeoffs, practical advice); reinforcement learning and adaptive control. The course will also discuss recent applications of machine learning, such as to robotic control, data mining, autonomous navigation, bioinformatics, speech recognition, and text and web data processing.  View the course website: https://cs229.stanford.edu/syllabus-spring2022.html|khTGx7m3Y8A
220|4|14|Stanford CS229 Machine Learning I Factor Analysis/PCA I 2022 I Lecture 14|2023-08-14T14:00:43Z|This course provides a broad introduction to machine learning and statistical pattern recognition. Topics include: supervised learning (generative/discriminative learning, parametric/non-parametric learning, neural networks, support vector machines); unsupervised learning (clustering, dimensionality reduction, kernel methods); learning theory (bias/variance tradeoffs, practical advice); reinforcement learning and adaptive control. The course will also discuss recent applications of machine learning, such as to robotic control, data mining, autonomous navigation, bioinformatics, speech recognition, and text and web data processing.  View the course website: https://cs229.stanford.edu/syllabus-spring2022.html|o2KzJdbOwMc
221|4|15|Stanford CS229 Machine Learning I PCA/ICA I 2022 I Lecture 15|2023-08-15T14:00:10Z|This course provides a broad introduction to machine learning and statistical pattern recognition. Topics include: supervised learning (generative/discriminative learning, parametric/non-parametric learning, neural networks, support vector machines); unsupervised learning (clustering, dimensionality reduction, kernel methods); learning theory (bias/variance tradeoffs, practical advice); reinforcement learning and adaptive control. The course will also discuss recent applications of machine learning, such as to robotic control, data mining, autonomous navigation, bioinformatics, speech recognition, and text and web data processing.  View the course website: https://cs229.stanford.edu/syllabus-spring2022.html|FVLZG_oHUIw
222|4|16|Stanford CS229 Machine Learning I Self-supervised learning I 2022 I Lecture 16|2023-08-16T14:00:03Z|This course provides a broad introduction to machine learning and statistical pattern recognition. Topics include: supervised learning (generative/discriminative learning, parametric/non-parametric learning, neural networks, support vector machines); unsupervised learning (clustering, dimensionality reduction, kernel methods); learning theory (bias/variance tradeoffs, practical advice); reinforcement learning and adaptive control. The course will also discuss recent applications of machine learning, such as to robotic control, data mining, autonomous navigation, bioinformatics, speech recognition, and text and web data processing.  View the course website: https://cs229.stanford.edu/syllabus-spring2022.html|6N3OAWIsUOU
223|4|17|Stanford CS229 I Basic concepts in RL, Value iteration, Policy iteration I 2022 I Lecture 17|2023-08-17T14:00:04Z|This course provides a broad introduction to machine learning and statistical pattern recognition. Topics include: supervised learning (generative/discriminative learning, parametric/non-parametric learning, neural networks, support vector machines); unsupervised learning (clustering, dimensionality reduction, kernel methods); learning theory (bias/variance tradeoffs, practical advice); reinforcement learning and adaptive control. The course will also discuss recent applications of machine learning, such as to robotic control, data mining, autonomous navigation, bioinformatics, speech recognition, and text and web data processing.  View the course website: https://cs229.stanford.edu/syllabus-spring2022.html|6ZYx_1NlYbI
224|4|18|Stanford CS229 I Societal impact of ML (Guest lecture by Prof. James Zou) I 2022 I Lecture 18|2023-08-18T14:00:31Z|This course provides a broad introduction to machine learning and statistical pattern recognition. Topics include: supervised learning (generative/discriminative learning, parametric/non-parametric learning, neural networks, support vector machines); unsupervised learning (clustering, dimensionality reduction, kernel methods); learning theory (bias/variance tradeoffs, practical advice); reinforcement learning and adaptive control. The course will also discuss recent applications of machine learning, such as to robotic control, data mining, autonomous navigation, bioinformatics, speech recognition, and text and web data processing.  View the course website: https://cs229.stanford.edu/syllabus-spring2022.html|UyPn-QR8A7I
225|4|19|Stanford CS229 Machine Learning I Model-based RL, Value function approximator I 2022 I Lecture 20|2023-08-19T14:00:27Z|This course provides a broad introduction to machine learning and statistical pattern recognition. Topics include: supervised learning (generative/discriminative learning, parametric/non-parametric learning, neural networks, support vector machines); unsupervised learning (clustering, dimensionality reduction, kernel methods); learning theory (bias/variance tradeoffs, practical advice); reinforcement learning and adaptive control. The course will also discuss recent applications of machine learning, such as to robotic control, data mining, autonomous navigation, bioinformatics, speech recognition, and text and web data processing.  View the course website: https://cs229.stanford.edu/syllabus-spring2022.html|gPCd0xx_OYI
226|5|1|Stanford CS330 Deep Multi-Task & Meta Learning - What is multi-task learning? I 2022 I Lecture 1|2023-03-30T16:30:11Z|While deep learning has achieved remarkable success in many problems such as image classification, natural language processing, and speech recognition, these models are, to a large degree, specialized for the single task they are trained for. This course will cover the setting where there are multiple tasks to be solved, and study how the structure arising from multiple tasks can be leveraged to learn more efficiently or effectively. This includes:  self-supervised pre-training for downstream few-shot learning and transfer learning meta-learning methods that aim to learn efficient learning algorithms that can learn new tasks quickly curriculum and lifelong learning, where the problem requires learning a sequence of tasks, leveraging their shared structure to enable knowledge transfer  This is a graduate-level course. By the end of the course, students will be able to understand and implement the state-of-the-art multi-task learning and meta-learning algorithms and be ready to conduct research on these topics.|bkVCAk9Nsss
227|5|2|Stanford CS330 Deep Multi-Task & Meta Learning - Multi-Task Learning Basics I 2022 I Lecture 2|2023-03-31T16:30:10Z|While deep learning has achieved remarkable success in many problems such as image classification, natural language processing, and speech recognition, these models are, to a large degree, specialized for the single task they are trained for. This course will cover the setting where there are multiple tasks to be solved, and study how the structure arising from multiple tasks can be leveraged to learn more efficiently or effectively. This includes:  self-supervised pre-training for downstream few-shot learning and transfer learning meta-learning methods that aim to learn efficient learning algorithms that can learn new tasks quickly curriculum and lifelong learning, where the problem requires learning a sequence of tasks, leveraging their shared structure to enable knowledge transfer  This is a graduate-level course. By the end of the course, students will be able to understand and implement the state-of-the-art multi-task learning and meta-learning algorithms and be ready to conduct research on these topics.|vI46tzt4O7Y
228|5|3|Stanford CS330 Deep Multi-Task & Meta Learning - Transfer Learning, Meta Learning l 2022 I Lecture 3|2023-04-01T16:30:07Z|While deep learning has achieved remarkable success in many problems such as image classification, natural language processing, and speech recognition, these models are, to a large degree, specialized for the single task they are trained for. This course will cover the setting where there are multiple tasks to be solved, and study how the structure arising from multiple tasks can be leveraged to learn more efficiently or effectively. This includes:  self-supervised pre-training for downstream few-shot learning and transfer learning meta-learning methods that aim to learn efficient learning algorithms that can learn new tasks quickly curriculum and lifelong learning, where the problem requires learning a sequence of tasks, leveraging their shared structure to enable knowledge transfer  This is a graduate-level course. By the end of the course, students will be able to understand and implement the state-of-the-art multi-task learning and meta-learning algorithms and be ready to conduct research on these topics.|bVjCjdq06R4
229|5|4|Stanford CS330 Deep Multi-Task & Meta Learning - Black Box Meta Learning l 2022 I Lecture 4|2023-04-02T16:30:06Z|While deep learning has achieved remarkable success in many problems such as image classification, natural language processing, and speech recognition, these models are, to a large degree, specialized for the single task they are trained for. This course will cover the setting where there are multiple tasks to be solved, and study how the structure arising from multiple tasks can be leveraged to learn more efficiently or effectively. This includes:  self-supervised pre-training for downstream few-shot learning and transfer learning meta-learning methods that aim to learn efficient learning algorithms that can learn new tasks quickly curriculum and lifelong learning, where the problem requires learning a sequence of tasks, leveraging their shared structure to enable knowledge transfer  This is a graduate-level course. By the end of the course, students will be able to understand and implement the state-of-the-art multi-task learning and meta-learning algorithms and be ready to conduct research on these topics.|cjTs7lH5eUs
230|5|5|Stanford CS330 Deep Multi-Task & Meta Learning - Optimization-Based Meta-Learning l 2022 I Lecture 5|2023-04-03T16:30:02Z|While deep learning has achieved remarkable success in many problems such as image classification, natural language processing, and speech recognition, these models are, to a large degree, specialized for the single task they are trained for. This course will cover the setting where there are multiple tasks to be solved, and study how the structure arising from multiple tasks can be leveraged to learn more efficiently or effectively. This includes:  self-supervised pre-training for downstream few-shot learning and transfer learning meta-learning methods that aim to learn efficient learning algorithms that can learn new tasks quickly curriculum and lifelong learning, where the problem requires learning a sequence of tasks, leveraging their shared structure to enable knowledge transfer  This is a graduate-level course. By the end of the course, students will be able to understand and implement the state-of-the-art multi-task learning and meta-learning algorithms and be ready to conduct research on these topics.|Gj5SEpFIv8I
231|5|6|Stanford CS330 Deep Multi-Task & Meta Learning - Non-Parametric Few-Shot Learning l 2022 I Lecture 6|2023-04-04T16:30:04Z|While deep learning has achieved remarkable success in many problems such as image classification, natural language processing, and speech recognition, these models are, to a large degree, specialized for the single task they are trained for. This course will cover the setting where there are multiple tasks to be solved, and study how the structure arising from multiple tasks can be leveraged to learn more efficiently or effectively. This includes:  self-supervised pre-training for downstream few-shot learning and transfer learning meta-learning methods that aim to learn efficient learning algorithms that can learn new tasks quickly curriculum and lifelong learning, where the problem requires learning a sequence of tasks, leveraging their shared structure to enable knowledge transfer  This is a graduate-level course. By the end of the course, students will be able to understand and implement the state-of-the-art multi-task learning and meta-learning algorithms and be ready to conduct research on these topics.|SUJCgisKW1Y
232|5|7|Stanford CS330 I Unsupervised Pre-Training:Contrastive Learning l 2022 I Lecture 7|2023-04-05T16:30:08Z|While deep learning has achieved remarkable success in many problems such as image classification, natural language processing, and speech recognition, these models are, to a large degree, specialized for the single task they are trained for. This course will cover the setting where there are multiple tasks to be solved, and study how the structure arising from multiple tasks can be leveraged to learn more efficiently or effectively. This includes:  self-supervised pre-training for downstream few-shot learning and transfer learning meta-learning methods that aim to learn efficient learning algorithms that can learn new tasks quickly curriculum and lifelong learning, where the problem requires learning a sequence of tasks, leveraging their shared structure to enable knowledge transfer  This is a graduate-level course. By the end of the course, students will be able to understand and implement the state-of-the-art multi-task learning and meta-learning algorithms and be ready to conduct research on these topics.|jDzuGEcnRkA
233|5|8|Stanford CS330 I Unsupervised Pre-training for Few-shot Learning l 2022 I Lecture 8|2023-04-06T16:30:04Z|While deep learning has achieved remarkable success in many problems such as image classification, natural language processing, and speech recognition, these models are, to a large degree, specialized for the single task they are trained for. This course will cover the setting where there are multiple tasks to be solved, and study how the structure arising from multiple tasks can be leveraged to learn more efficiently or effectively. This includes:  self-supervised pre-training for downstream few-shot learning and transfer learning meta-learning methods that aim to learn efficient learning algorithms that can learn new tasks quickly curriculum and lifelong learning, where the problem requires learning a sequence of tasks, leveraging their shared structure to enable knowledge transfer  This is a graduate-level course. By the end of the course, students will be able to understand and implement the state-of-the-art multi-task learning and meta-learning algorithms and be ready to conduct research on these topics.|Vs85VCzug0I
234|5|9|Stanford CS330 I Advanced Meta-Learning TopicsTask Construction l 2022 I Lecture 9|2023-04-07T16:30:00Z|While deep learning has achieved remarkable success in many problems such as image classification, natural language processing, and speech recognition, these models are, to a large degree, specialized for the single task they are trained for. This course will cover the setting where there are multiple tasks to be solved, and study how the structure arising from multiple tasks can be leveraged to learn more efficiently or effectively. This includes:  self-supervised pre-training for downstream few-shot learning and transfer learning meta-learning methods that aim to learn efficient learning algorithms that can learn new tasks quickly curriculum and lifelong learning, where the problem requires learning a sequence of tasks, leveraging their shared structure to enable knowledge transfer  This is a graduate-level course. By the end of the course, students will be able to understand and implement the state-of-the-art multi-task learning and meta-learning algorithms and be ready to conduct research on these topics.|4z3ijI9QacE
235|5|10|Stanford CS330 I Advanced Meta-Learning 2: Large-Scale Meta-Optimization l 2022 I Lecture 10|2023-04-08T16:30:15Z|While deep learning has achieved remarkable success in many problems such as image classification, natural language processing, and speech recognition, these models are, to a large degree, specialized for the single task they are trained for. This course will cover the setting where there are multiple tasks to be solved, and study how the structure arising from multiple tasks can be leveraged to learn more efficiently or effectively. This includes:  self-supervised pre-training for downstream few-shot learning and transfer learning meta-learning methods that aim to learn efficient learning algorithms that can learn new tasks quickly curriculum and lifelong learning, where the problem requires learning a sequence of tasks, leveraging their shared structure to enable knowledge transfer  This is a graduate-level course. By the end of the course, students will be able to understand and implement the state-of-the-art multi-task learning and meta-learning algorithms and be ready to conduct research on these topics.|tCGS5I9ow0k
236|5|11|Stanford CS330 I Variational Inference and Generative Models l 2022 I Lecture 11|2023-04-09T16:30:07Z|While deep learning has achieved remarkable success in many problems such as image classification, natural language processing, and speech recognition, these models are, to a large degree, specialized for the single task they are trained for. This course will cover the setting where there are multiple tasks to be solved, and study how the structure arising from multiple tasks can be leveraged to learn more efficiently or effectively. This includes:  self-supervised pre-training for downstream few-shot learning and transfer learning meta-learning methods that aim to learn efficient learning algorithms that can learn new tasks quickly curriculum and lifelong learning, where the problem requires learning a sequence of tasks, leveraging their shared structure to enable knowledge transfer  This is a graduate-level course. By the end of the course, students will be able to understand and implement the state-of-the-art multi-task learning and meta-learning algorithms and be ready to conduct research on these topics.|iL1c1KmYPM0
237|5|12|Stanford CS330 Deep Multi-Task & Meta Learning - Bayesian Meta-Learning l 2022 I Lecture 12|2023-04-10T16:30:00Z|While deep learning has achieved remarkable success in many problems such as image classification, natural language processing, and speech recognition, these models are, to a large degree, specialized for the single task they are trained for. This course will cover the setting where there are multiple tasks to be solved, and study how the structure arising from multiple tasks can be leveraged to learn more efficiently or effectively. This includes:  self-supervised pre-training for downstream few-shot learning and transfer learning meta-learning methods that aim to learn efficient learning algorithms that can learn new tasks quickly curriculum and lifelong learning, where the problem requires learning a sequence of tasks, leveraging their shared structure to enable knowledge transfer  This is a graduate-level course. By the end of the course, students will be able to understand and implement the state-of-the-art multi-task learning and meta-learning algorithms and be ready to conduct research on these topics.|-y3ufzjgmlY
238|5|13|Stanford CS330 Deep Multi-Task & Meta Learning - Domain Adaptation l 2022 I Lecture 13|2023-04-11T16:30:07Z|While deep learning has achieved remarkable success in many problems such as image classification, natural language processing, and speech recognition, these models are, to a large degree, specialized for the single task they are trained for. This course will cover the setting where there are multiple tasks to be solved, and study how the structure arising from multiple tasks can be leveraged to learn more efficiently or effectively. This includes:  self-supervised pre-training for downstream few-shot learning and transfer learning meta-learning methods that aim to learn efficient learning algorithms that can learn new tasks quickly curriculum and lifelong learning, where the problem requires learning a sequence of tasks, leveraging their shared structure to enable knowledge transfer  This is a graduate-level course. By the end of the course, students will be able to understand and implement the state-of-the-art multi-task learning and meta-learning algorithms and be ready to conduct research on these topics.|Uk6MU_PLDMs
239|5|14|Stanford CS330 Deep Multi-Task & Meta Learning - Domain Generalization l 2022 I Lecture 14|2023-04-12T16:30:01Z|While deep learning has achieved remarkable success in many problems such as image classification, natural language processing, and speech recognition, these models are, to a large degree, specialized for the single task they are trained for. This course will cover the setting where there are multiple tasks to be solved, and study how the structure arising from multiple tasks can be leveraged to learn more efficiently or effectively. This includes:  self-supervised pre-training for downstream few-shot learning and transfer learning meta-learning methods that aim to learn efficient learning algorithms that can learn new tasks quickly curriculum and lifelong learning, where the problem requires learning a sequence of tasks, leveraging their shared structure to enable knowledge transfer  This is a graduate-level course. By the end of the course, students will be able to understand and implement the state-of-the-art multi-task learning and meta-learning algorithms and be ready to conduct research on these topics.|EK0sgHPLou8
240|5|15|Stanford CS330 Deep Multi-Task & Meta Learning - Lifelong Learning I 2022 I Lecture 15|2023-04-13T16:30:17Z|While deep learning has achieved remarkable success in many problems such as image classification, natural language processing, and speech recognition, these models are, to a large degree, specialized for the single task they are trained for. This course will cover the setting where there are multiple tasks to be solved, and study how the structure arising from multiple tasks can be leveraged to learn more efficiently or effectively. This includes:  self-supervised pre-training for downstream few-shot learning and transfer learning meta-learning methods that aim to learn efficient learning algorithms that can learn new tasks quickly curriculum and lifelong learning, where the problem requires learning a sequence of tasks, leveraging their shared structure to enable knowledge transfer  This is a graduate-level course. By the end of the course, students will be able to understand and implement the state-of-the-art multi-task learning and meta-learning algorithms and be ready to conduct research on these topics.|tQjDijHgblg
241|5|16|Stanford CS330 Deep Multi-Task & Meta Learning - Frontiers and Open Challenges I 2022 I Lecture 16|2023-04-14T16:30:10Z|While deep learning has achieved remarkable success in many problems such as image classification, natural language processing, and speech recognition, these models are, to a large degree, specialized for the single task they are trained for. This course will cover the setting where there are multiple tasks to be solved, and study how the structure arising from multiple tasks can be leveraged to learn more efficiently or effectively. This includes:  self-supervised pre-training for downstream few-shot learning and transfer learning meta-learning methods that aim to learn efficient learning algorithms that can learn new tasks quickly curriculum and lifelong learning, where the problem requires learning a sequence of tasks, leveraging their shared structure to enable knowledge transfer  This is a graduate-level course. By the end of the course, students will be able to understand and implement the state-of-the-art multi-task learning and meta-learning algorithms and be ready to conduct research on these topics.|x-Ca8mvNdPQ
242|5|17|Stanford CS330 Deep Multi-Task & Meta Learning - Percy Liang Guest Lecture I 2022 I Lecture 17|2023-04-15T16:30:02Z|While deep learning has achieved remarkable success in many problems such as image classification, natural language processing, and speech recognition, these models are, to a large degree, specialized for the single task they are trained for. This course will cover the setting where there are multiple tasks to be solved, and study how the structure arising from multiple tasks can be leveraged to learn more efficiently or effectively. This includes:  self-supervised pre-training for downstream few-shot learning and transfer learning meta-learning methods that aim to learn efficient learning algorithms that can learn new tasks quickly curriculum and lifelong learning, where the problem requires learning a sequence of tasks, leveraging their shared structure to enable knowledge transfer  This is a graduate-level course. By the end of the course, students will be able to understand and implement the state-of-the-art multi-task learning and meta-learning algorithms and be ready to conduct research on these topics.|L8YHbVZlYuw
243|6|1|Stanford Seminar - ML Explainability Part 1 I Overview and Motivation for Explainability|2022-11-02T23:51:00Z|Professor Hima Lakkaraju's day-long workshop at Stanford covered modern techniques for interpretable machine learning.   About the speaker: Himabindu (Hima) Lakkaraju is an assistant professor at Harvard University focusing on explainability, fairness, and robustness of machine learning models. She has also been working with various domain experts in policy and healthcare to understand the real-world implications of explainable and fair ML. Hima has been named as one of the world’s top innovators under 35 by both MIT Tech Review and Vanity Fair. Her research has also received best paper awards at SIAM International Conference on Data Mining (SDM) and INFORMS, and grants from NSF, Google, Amazon, and Bayer. Hima has given keynote talks at various top ML conferences and workshops including CIKM, ICML, NeurIPS, AAAI, and CVPR, and her research has also been showcased by popular media outlets including the New York Times, MIT Tech Review, TIME magazine, and Forbes. More recently, she co-founded the Trustworthy ML Initiative to enable easy access to resources on trustworthy ML and to build a community of researchers/practitioners working on the topic. Learn more on her website: https://himalakkaraju.github.io/|_DYQdP_F-LA
244|6|2|Stanford Seminar - ML Explainability Part 2 I Inherently Interpretable Models|2022-11-03T16:30:01Z|Professor Hima Lakkaraju's day-long workshop at Stanford covered modern techniques for interpretable machine learning.   About the speaker: Himabindu (Hima) Lakkaraju is an assistant professor at Harvard University focusing on explainability, fairness, and robustness of machine learning models. She has also been working with various domain experts in policy and healthcare to understand the real-world implications of explainable and fair ML. Hima has been named as one of the world’s top innovators under 35 by both MIT Tech Review and Vanity Fair. Her research has also received best paper awards at SIAM International Conference on Data Mining (SDM) and INFORMS, and grants from NSF, Google, Amazon, and Bayer. Hima has given keynote talks at various top ML conferences and workshops including CIKM, ICML, NeurIPS, AAAI, and CVPR, and her research has also been showcased by popular media outlets including the New York Times, MIT Tech Review, TIME magazine, and Forbes. More recently, she co-founded the Trustworthy ML Initiative to enable easy access to resources on trustworthy ML and to build a community of researchers/practitioners working on the topic. Learn more on her website: https://himalakkaraju.github.io/|9Th86_3Ea7o
245|6|3|Stanford Seminar - ML Explainability Part 3 I Post hoc Explanation Methods|2022-11-04T16:30:04Z|Professor Hima Lakkaraju's day-long workshop at Stanford covered modern techniques for interpretable machine learning.   About the speaker: Himabindu (Hima) Lakkaraju is an assistant professor at Harvard University focusing on explainability, fairness, and robustness of machine learning models. She has also been working with various domain experts in policy and healthcare to understand the real-world implications of explainable and fair ML. Hima has been named as one of the world’s top innovators under 35 by both MIT Tech Review and Vanity Fair. Her research has also received best paper awards at SIAM International Conference on Data Mining (SDM) and INFORMS, and grants from NSF, Google, Amazon, and Bayer. Hima has given keynote talks at various top ML conferences and workshops including CIKM, ICML, NeurIPS, AAAI, and CVPR, and her research has also been showcased by popular media outlets including the New York Times, MIT Tech Review, TIME magazine, and Forbes. More recently, she co-founded the Trustworthy ML Initiative to enable easy access to resources on trustworthy ML and to build a community of researchers/practitioners working on the topic. Learn more on her website: https://himalakkaraju.github.io/|_6n8r523QP8
246|6|4|Stanford Seminar - ML Explainability Part 4 I Evaluating Model Interpretations/Explanations|2022-11-05T16:30:05Z|Professor Hima Lakkaraju's day-long workshop at Stanford covered modern techniques for interpretable machine learning.   About the speaker: Himabindu (Hima) Lakkaraju is an assistant professor at Harvard University focusing on explainability, fairness, and robustness of machine learning models. She has also been working with various domain experts in policy and healthcare to understand the real-world implications of explainable and fair ML. Hima has been named as one of the world’s top innovators under 35 by both MIT Tech Review and Vanity Fair. Her research has also received best paper awards at SIAM International Conference on Data Mining (SDM) and INFORMS, and grants from NSF, Google, Amazon, and Bayer. Hima has given keynote talks at various top ML conferences and workshops including CIKM, ICML, NeurIPS, AAAI, and CVPR, and her research has also been showcased by popular media outlets including the New York Times, MIT Tech Review, TIME magazine, and Forbes. More recently, she co-founded the Trustworthy ML Initiative to enable easy access to resources on trustworthy ML and to build a community of researchers/practitioners working on the topic. Learn more on her website: https://himalakkaraju.github.io/|htjpbbvHJQo
247|6|5|Stanford Seminar - ML Explainability Part 5 I Future of Model Understanding|2022-11-06T17:30:00Z|Professor Hima Lakkaraju's day-long workshop at Stanford covered modern techniques for interpretable machine learning.   About the speaker: Himabindu (Hima) Lakkaraju is an assistant professor at Harvard University focusing on explainability, fairness, and robustness of machine learning models. She has also been working with various domain experts in policy and healthcare to understand the real-world implications of explainable and fair ML. Hima has been named as one of the world’s top innovators under 35 by both MIT Tech Review and Vanity Fair. Her research has also received best paper awards at SIAM International Conference on Data Mining (SDM) and INFORMS, and grants from NSF, Google, Amazon, and Bayer. Hima has given keynote talks at various top ML conferences and workshops including CIKM, ICML, NeurIPS, AAAI, and CVPR, and her research has also been showcased by popular media outlets including the New York Times, MIT Tech Review, TIME magazine, and Forbes. More recently, she co-founded the Trustworthy ML Initiative to enable easy access to resources on trustworthy ML and to build a community of researchers/practitioners working on the topic. Learn more on her website: https://himalakkaraju.github.io/|2HUYwCflwI8
248|7|1|General Intro , Stanford CS221: Artificial Intelligence: Principles and Techniques (Autumn 2021)|2022-05-31T17:52:28Z|For more information about Stanford's Artificial Intelligence professional and graduate programs visit: https://stanford.io/ai|ZiwogMtbjr4
249|7|2|AI History , Stanford CS221: AI (Autumn 2021)|2022-05-31T17:53:11Z|For more information about Stanford's Artificial Intelligence professional and graduate programs visit: https://stanford.io/ai|z8fEXuH0mu0
250|7|3|Artificial Intelligence Today , Stanford CS221: AI (Autumn 2021)|2022-05-31T17:53:47Z|For more information about Stanford's Artificial Intelligence professional and graduate programs visit: https://stanford.io/ai|C0IhR4D5KYc
251|7|4|Artificial Intelligence and Machine Learning 1 - Overview , Stanford CS221: AI (Autumn 2021)|2022-05-31T17:54:14Z|For more information about Stanford's Artificial Intelligence professional and graduate programs visit: https://stanford.io/ai|mtrYwgIrRNk
252|7|5|Artificial Intelligence & Machine Learning 2 - Linear Regression , Stanford CS221: AI (Autumn 2021)|2022-05-31T17:54:41Z|For more information about Stanford's Artificial Intelligence professional and graduate programs visit: https://stanford.io/ai|nEWNNt2KmfQ
253|7|6|Artificial Intelligence & Machine learning 3 - Linear Classification , Stanford CS221 (Autumn 2021)|2022-05-31T17:55:05Z|For more information about Stanford's Artificial Intelligence professional and graduate programs visit: https://stanford.io/ai|WcaMiqJR09s
254|7|7|Artificial Intelligence & Machine Learning 4 - Stochastic Gradient Descent , Stanford CS221 (2021)|2022-05-31T17:55:29Z|For more information about Stanford's Artificial Intelligence professional and graduate programs visit: https://stanford.io/ai|bl2WgBLH0tI
255|7|8|Artificial Intelligence and Machine Learning 5 - Group DRO , Stanford CS221: AI (Autumn 2021)|2022-05-31T17:55:49Z|For more information about Stanford's Artificial Intelligence professional and graduate programs visit: https://stanford.io/ai|ZFK2XtWqUbw
256|7|9|Artificial Intelligence & Machine Learning 6 - Non Linear Features , Stanford CS221: AI(Autumn 2021)|2022-05-31T17:56:11Z|For more information about Stanford's Artificial Intelligence professional and graduate programs visit: https://stanford.io/ai|eIxbNkB4byY
257|7|10|Artificial Intelligence & Machine Learning 7 - Feature Templates , Stanford CS221: AI (Autumn 2021)|2022-05-31T17:56:34Z|For more information about Stanford's Artificial Intelligence professional and graduate programs visit: https://stanford.io/ai|2QfSBLtvioE
258|7|11|Artificial Intelligence & Machine Learning 8 - Neural Networks , Stanford CS221: AI (Autumn 2021)|2022-05-31T17:56:57Z|For more information about Stanford's Artificial Intelligence professional and graduate programs visit: https://stanford.io/ai|pnKXgBHuN58
259|7|12|Machine Learning 9 - Backpropagation , Stanford CS221: AI (Autumn 2021)|2022-05-31T17:57:18Z|For more information about Stanford's Artificial Intelligence professional and graduate programs visit: https://stanford.io/ai|OcAF-l2xB9Y
260|7|13|Machine Learning 10 - Differentiable Programming , Stanford CS221: AI (Autumn 2021)|2022-05-31T17:57:40Z|For more information about Stanford's Artificial Intelligence professional and graduate programs visit: https://stanford.io/ai|c5btEEisp_g
261|7|14|Artificial Intelligence & Machine Learning 11 - Generalization , Stanford CS221: AI (Autumn 2021)|2022-05-31T17:58:02Z|For more information about Stanford's Artificial Intelligence professional and graduate programs visit: https://stanford.io/ai|Gq-Ah-QrOQM
262|7|15|Artificial Intelligence & Machine Learning 12 - Best Practices , Stanford CS221: AI (Autumn 2021)|2022-05-31T17:58:23Z|For more information about Stanford's Artificial Intelligence professional and graduate programs visit: https://stanford.io/ai|ouvGV2YZEEM
263|7|16|Machine Learning 13 - K-means , Stanford CS221: AI (Autumn 2021)|2022-05-31T17:58:44Z|For more information about Stanford's Artificial Intelligence professional and graduate programs visit: https://stanford.io/ai|5-Fn8R9fH7A
264|7|17|Search 1 - Dynamic Programming, Uniform Cost Search , Stanford CS221: AI (Autumn 2019)|2020-01-08T19:23:58Z|For more information about Stanford's Artificial Intelligence professional and graduate programs visit: https://stanford.io/ai|aIsgJJYrlXk
265|7|18|Search 2 - A* , Stanford CS221: Artificial Intelligence (Autumn 2019)|2020-01-08T19:24:17Z|For more information about Stanford's Artificial Intelligence professional and graduate programs visit: https://stanford.io/ai|HEs1ZCvLH2s
266|7|19|Markov Decision Processes 1 - Value Iteration , Stanford CS221: AI (Autumn 2019)|2020-01-08T19:24:34Z|For more information about Stanford's Artificial Intelligence professional and graduate programs visit: https://stanford.io/ai|9g32v7bK3Co
267|7|20|Markov Decision Processes 2 - Reinforcement Learning , Stanford CS221: AI (Autumn 2019)|2020-04-28T17:05:30Z|For more information about Stanford's Artificial Intelligence professional and graduate programs visit: https://stanford.io/ai|HpaHTfY52RQ
268|7|21|Game Playing 1 - Minimax, Alpha-beta Pruning , Stanford CS221: AI (Autumn 2019)|2020-01-08T19:25:06Z|For more information about Stanford's Artificial Intelligence professional and graduate programs visit: https://stanford.io/ai|3pU-Hrz_xy4
269|7|22|Game Playing 2 - TD Learning, Game Theory , Stanford CS221: Artificial Intelligence (Autumn 2019)|2020-01-08T19:25:19Z|For more information about Stanford's Artificial Intelligence professional and graduate programs visit: https://stanford.io/ai|WoFwXj4p4Sc
270|7|23|Constraint Satisfaction Problems (CSPs) 1 - Overview , Stanford CS221: AI (Autumn 2021)|2022-05-31T17:59:13Z|For more information about Stanford's Artificial Intelligence professional and graduate programs visit: https://stanford.io/ai|-IO4fPO0rxk
271|7|24|Constraint Satisfaction Problems (CSPs) 2 - Definitions , Stanford CS221: AI (Autumn 2021)|2022-05-31T17:59:36Z|For more information about Stanford's Artificial Intelligence professional and graduate programs visit: https://stanford.io/ai|uj5wCcHsSlA
272|7|25|Constraint Satisfaction Problems (CSPs) 3 - Examples , Stanford CS221: AI (Autumn 2021)|2022-05-31T17:59:59Z|For more information about Stanford's Artificial Intelligence professional and graduate programs visit: https://stanford.io/ai|Tu6BiZhMDCc
273|7|26|Constraint Satisfaction Problems (CSPs) 4 - Dynamic Ordering , Stanford CS221: AI (Autumn 2021)|2022-05-31T18:00:25Z|For more information about Stanford's Artificial Intelligence professional and graduate programs visit: https://stanford.io/ai|Lyu8VzbIe_A
274|7|27|Constraint Satisfaction Problems (CSPs) 5 - Arc Consistency , Stanford CS221: AI (Autumn 2021)|2022-05-31T18:00:55Z|For more information about Stanford's Artificial Intelligence professional and graduate programs visit: https://stanford.io/ai|5rlIYGJdPy4
275|7|28|Constraint Satisfaction Problems (CSPs) 6 - Beam Search , Stanford CS221: AI (Autumn 2021)|2022-05-31T18:01:20Z|For more information about Stanford's Artificial Intelligence professional and graduate programs visit: https://stanford.io/ai|XuWMeIHGkus
276|7|29|Constraint Satisfaction Problems (CSPs) 7 - Local Search , Stanford CS221: AI (Autumn 2021)|2022-05-31T18:01:44Z|For more information about Stanford's Artificial Intelligence professional and graduate programs visit: https://stanford.io/ai|VwZKPlK6jUg
277|7|30|Markov Networks 1 - Overview , Stanford CS221: Artificial Intelligence (Autumn 2021)|2022-05-31T18:02:07Z|For more information about Stanford's Artificial Intelligence professional and graduate programs visit: https://stanford.io/ai|neeaJb3wCYw
278|7|31|Markov Networks 2 - Gibbs Sampling , Stanford CS221: AI (Autumn 2021)|2022-05-31T18:02:51Z|For more information about Stanford's Artificial Intelligence professional and graduate programs visit: https://stanford.io/ai|k6aZZF2pk7k
279|7|32|Bayesian Networks 1 - Overview , Stanford CS221: AI (Autumn 2021)|2022-05-31T18:03:13Z|For more information about Stanford's Artificial Intelligence professional and graduate programs visit: https://stanford.io/ai|fA7zP6EcVdw
280|7|33|Bayesian Networks 2 - Definition , Stanford CS221: AI (Autumn 2021)|2022-05-31T18:03:33Z|For more information about Stanford's Artificial Intelligence professional and graduate programs visit: https://stanford.io/ai|xvC6XmZmR_U
281|7|34|Bayesian Networks 3 - Probabilistic Programming , Stanford CS221: AI (Autumn 2021)|2022-05-31T18:04:01Z|For more information about Stanford's Artificial Intelligence professional and graduate programs visit: https://stanford.io/ai|ZVk8y1zVoD4
282|7|35|Bayesian Networks 4 - Probabilistic Inference , Stanford CS221: AI (Autumn 2021)|2022-05-31T18:04:25Z|For more information about Stanford's Artificial Intelligence professional and graduate programs visit: https://stanford.io/ai|-dGOWB9Zh8s
283|7|36|Bayesian Networks 5 - Forward-backward Algorithm , Stanford CS221: AI (Autumn 2021)|2022-05-31T18:04:51Z|For more information about Stanford's Artificial Intelligence professional and graduate programs visit: https://stanford.io/ai|N-ZPbpJOQs0
284|7|37|Bayesian Networks 6 - Particle Filtering , Stanford CS221: AI (Autumn 2021)|2022-05-31T18:05:18Z|For more information about Stanford's Artificial Intelligence professional and graduate programs visit: https://stanford.io/ai|8sOtXbQIOuE
285|7|38|Bayesian Networks 7 - Supervised Learning , Stanford CS221: AI (Autumn 2021)|2022-05-31T18:05:38Z|For more information about Stanford's Artificial Intelligence professional and graduate programs visit: https://stanford.io/ai|_rbDjsJTgm8
286|7|39|Bayesian Networks 8 - Smoothing , Stanford CS221: AI (Autumn 2021)|2022-05-31T18:06:00Z|For more information about Stanford's Artificial Intelligence professional and graduate programs visit: https://stanford.io/ai|M7rWvN_0xbw
287|7|40|Bayesian Networks 9 - EM Algorithm , Stanford CS221: AI (Autumn 2021)|2022-05-31T18:06:21Z|For more information about Stanford's Artificial Intelligence professional and graduate programs visit: https://stanford.io/ai|CPVFJBd-Qcg
288|7|41|Logic 1 - Overview: Logic Based Models , Stanford CS221: AI (Autumn 2021)|2022-05-31T18:06:44Z|For more information about Stanford's Artificial Intelligence professional and graduate programs visit: https://stanford.io/ai|oM5LUGPO7Zk
289|7|42|Logic 2 - Propositional Logic Syntax , Stanford CS221: AI (Autumn 2021)|2022-05-31T18:07:05Z|For more information about Stanford's Artificial Intelligence professional and graduate programs visit: https://stanford.io/ai|LBjNaewGJzk
290|7|43|Logic 3 - Propositional Logic Semantics , Stanford CS221: AI (Autumn 2021)|2022-05-31T18:07:28Z|For more information about Stanford's Artificial Intelligence professional and graduate programs visit: https://stanford.io/ai|N37yIn1jX98
291|7|44|Logic 4 - Inference Rules , Stanford CS221: AI (Autumn 2021)|2022-05-31T18:08:19Z|For more information about Stanford's Artificial Intelligence professional and graduate programs visit: https://stanford.io/ai|RIk67yGMVv4
292|7|45|Logic 5 - Propositional Modus Ponens , Stanford CS221: AI (Autumn 2021)|2022-05-31T18:09:21Z|For more information about Stanford's Artificial Intelligence professional and graduate programs visit: https://stanford.io/ai|6bj4z2mt1KE
293|7|46|Logic 6 - Propositional Resolutions , Stanford CS221: AI (Autumn 2021)|2022-05-31T18:09:45Z|For more information about Stanford's Artificial Intelligence professional and graduate programs visit: https://stanford.io/ai|egLAF4dFdBo
294|7|47|Logic 7 - First Order Logic , Stanford CS221: AI (Autumn 2021)|2022-05-31T18:10:04Z|For more information about Stanford's Artificial Intelligence professional and graduate programs visit: https://stanford.io/ai|Z-O0Q3_oTJM
295|7|48|Logic 8 - First Order Modus Ponens , Stanford CS221: Artificial Intelligence (Autumn 2021)|2022-05-31T18:10:45Z|For more information about Stanford's Artificial Intelligence professional and graduate programs visit: https://stanford.io/ai|mndzhfBpyUw
296|7|49|Logic 9 - First Order Resolution , Stanford CS221: AI (Autumn 2021)|2022-05-31T18:11:06Z|For more information about Stanford's Artificial Intelligence professional and graduate programs visit: https://stanford.io/ai|iG_tz7ZjZAI
297|7|50|Logic 10 - Recap , Stanford CS221: Artificial Intelligence (Autumn 2021)|2022-05-31T18:11:28Z|For more information about Stanford's Artificial Intelligence professional and graduate programs visit: https://stanford.io/ai|LYsOjtmLpPo
298|7|51|AI and Law I Mariano-Florentino Cuéllar, President of the Carnegie Endowment for International Peace|2022-05-31T18:11:48Z|For more information about Stanford's Artificial Intelligence professional and graduate programs visit: https://stanford.io/ai|_-hBu3_Jz-0
299|7|52|Stanford Fireside Talks: Robustness in Machine Learning I Robust Machine Learning|2022-05-31T18:12:11Z|For more information about Stanford's Artificial Intelligence professional and graduate programs visit: https://stanford.io/ai|xr8AHGlieOE
300|7|53|Fireside Talks: State of Robotics I Automation and Robotics Engineering Lectures - Stanford|2022-05-31T18:12:34Z|For more information about Stanford's Artificial Intelligence professional and graduate programs visit: https://stanford.io/ai|hVsR9DdR3qE
301|7|54|Stanford Talk: Inequality in Healthcare, AI & Data Science to Reduce Inequality - Improve Healthcare|2022-05-31T18:12:58Z|For more information about Stanford's Artificial Intelligence professional and graduate programs visit: https://stanford.io/ai|0IZhDmh1dmI
302|7|55|Fireside Talks:  Artificial Intelligence (AI) and Language|2022-05-31T18:13:20Z|For more information about Stanford's Artificial Intelligence professional and graduate programs visit: https://stanford.io/ai|pI72PseZQo8
303|7|56|General Conclusion , Stanford CS221: AI (Autumn 2021)|2022-05-31T18:15:36Z|For more information about Stanford's Artificial Intelligence professional and graduate programs visit: https://stanford.io/ai|iUGmupxCdjs
304|8|1|Stanford CS105: Introduction to Computers , 2021 , Lecture 00 Introduction|2021-08-05T21:08:19Z|Because computing is so pervasive, students will be making decisions on computing whether they end up in business, education, working for NGOs, or working in government. We want to put CS105 graduates in the best position possible to make informed decisions on computing in both their personnel and professional lives.  Students will gain a good understanding of how computing devices work, what their limitations are, and what their strengths are. Students will gain practical skills that they can take forward from the course, including a thorough grounding in webpage development. Working with computers should be fun. We want to ensure that our students enjoy the class.|Qpw-udshXdM
305|8|2|Stanford CS105: Intro to Computers , 2021 , Lecture 1.1 Bits, Bytes, & Binary: It's all about 0 & 1|2021-08-05T21:08:19Z|Because computing is so pervasive, students will be making decisions on computing whether they end up in business, education, working for NGOs, or working in government. We want to put CS105 graduates in the best position possible to make informed decisions on computing in both their personnel and professional lives.  Students will gain a good understanding of how computing devices work, what their limitations are, and what their strengths are. Students will gain practical skills that they can take forward from the course, including a thorough grounding in webpage development. Working with computers should be fun. We want to ensure that our students enjoy the class.|LOV_BwQfOqI
306|8|3|Stanford CS105: Introduction to Computers , 2021 , Lecture 1.2 Bits, Bytes, and Binary: 1 + 1 = 10?|2021-08-05T21:08:19Z|Because computing is so pervasive, students will be making decisions on computing whether they end up in business, education, working for NGOs, or working in government. We want to put CS105 graduates in the best position possible to make informed decisions on computing in both their personnel and professional lives.  Students will gain a good understanding of how computing devices work, what their limitations are, and what their strengths are. Students will gain practical skills that they can take forward from the course, including a thorough grounding in webpage development. Working with computers should be fun. We want to ensure that our students enjoy the class.|CzrUquTurzw
307|8|4|Stanford CS105: Intro to Computers , 2021 , Lecture 1.3 BB&B: How to Destroy a Rocket with 16-Bits|2021-08-05T21:08:19Z|Because computing is so pervasive, students will be making decisions on computing whether they end up in business, education, working for NGOs, or working in government. We want to put CS105 graduates in the best position possible to make informed decisions on computing in both their personnel and professional lives.  Students will gain a good understanding of how computing devices work, what their limitations are, and what their strengths are. Students will gain practical skills that they can take forward from the course, including a thorough grounding in webpage development. Working with computers should be fun. We want to ensure that our students enjoy the class.|vTDCChTAnrs
308|8|5|Stanford CS105: Intro to Computers , 2021 , Lecture 1.4 BB&B: Represent Hieroglyphs on a Computer|2021-08-05T21:08:19Z|Because computing is so pervasive, students will be making decisions on computing whether they end up in business, education, working for NGOs, or working in government. We want to put CS105 graduates in the best position possible to make informed decisions on computing in both their personnel and professional lives.  Students will gain a good understanding of how computing devices work, what their limitations are, and what their strengths are. Students will gain practical skills that they can take forward from the course, including a thorough grounding in webpage development. Working with computers should be fun. We want to ensure that our students enjoy the class.|Suodr0VVHgM
309|8|6|Stanford CS105: Introduction to Computers , 2021 , Lecture 2.1 Digital Images: The Basics|2021-08-05T21:08:19Z|Because computing is so pervasive, students will be making decisions on computing whether they end up in business, education, working for NGOs, or working in government. We want to put CS105 graduates in the best position possible to make informed decisions on computing in both their personnel and professional lives.  Students will gain a good understanding of how computing devices work, what their limitations are, and what their strengths are. Students will gain practical skills that they can take forward from the course, including a thorough grounding in webpage development. Working with computers should be fun. We want to ensure that our students enjoy the class.|MNAshR_e9sk
310|8|7|Stanford CS105: Introduction to Computers , 2021 , Lecture 2.2 Digital Images: Let's get Colorful|2021-08-05T21:08:19Z|Because computing is so pervasive, students will be making decisions on computing whether they end up in business, education, working for NGOs, or working in government. We want to put CS105 graduates in the best position possible to make informed decisions on computing in both their personnel and professional lives.  Students will gain a good understanding of how computing devices work, what their limitations are, and what their strengths are. Students will gain practical skills that they can take forward from the course, including a thorough grounding in webpage development. Working with computers should be fun. We want to ensure that our students enjoy the class.|NYBAxql8-Xo
311|8|8|Stanford CS105: Introduction to Computers , 2021 , Lecture 2.3 Digital Images: Bitmaps vs. Objects|2021-08-05T21:08:19Z|Because computing is so pervasive, students will be making decisions on computing whether they end up in business, education, working for NGOs, or working in government. We want to put CS105 graduates in the best position possible to make informed decisions on computing in both their personnel and professional lives.  Students will gain a good understanding of how computing devices work, what their limitations are, and what their strengths are. Students will gain practical skills that they can take forward from the course, including a thorough grounding in webpage development. Working with computers should be fun. We want to ensure that our students enjoy the class.|qysuyTJ8FJY
312|8|9|Stanford CS105: Intro to Computers , 2021 , Lecture 2.4 Digital Images: The Right Format for the Job|2021-08-05T21:08:19Z|Because computing is so pervasive, students will be making decisions on computing whether they end up in business, education, working for NGOs, or working in government. We want to put CS105 graduates in the best position possible to make informed decisions on computing in both their personnel and professional lives.  Students will gain a good understanding of how computing devices work, what their limitations are, and what their strengths are. Students will gain practical skills that they can take forward from the course, including a thorough grounding in webpage development. Working with computers should be fun. We want to ensure that our students enjoy the class.|hFaI3a91ugo
313|8|10|Stanford CS105: Intro to Computers , 2021 , Lecture 3.1 Digital Music: Science of Sound & Recording|2021-08-05T21:08:19Z|Because computing is so pervasive, students will be making decisions on computing whether they end up in business, education, working for NGOs, or working in government. We want to put CS105 graduates in the best position possible to make informed decisions on computing in both their personnel and professional lives.  Students will gain a good understanding of how computing devices work, what their limitations are, and what their strengths are. Students will gain practical skills that they can take forward from the course, including a thorough grounding in webpage development. Working with computers should be fun. We want to ensure that our students enjoy the class.|KuLxqrdmais
314|8|11|Stanford CS105: Introduction to Computers , 2021 , Lecture 3.2 Digital Music: From Analog to Digital|2021-08-05T21:08:19Z|Because computing is so pervasive, students will be making decisions on computing whether they end up in business, education, working for NGOs, or working in government. We want to put CS105 graduates in the best position possible to make informed decisions on computing in both their personnel and professional lives.  Students will gain a good understanding of how computing devices work, what their limitations are, and what their strengths are. Students will gain practical skills that they can take forward from the course, including a thorough grounding in webpage development. Working with computers should be fun. We want to ensure that our students enjoy the class.|ejJEzdUwFjQ
315|8|12|Stanford CS105: Introduction to Computers , 2021 , Lecture 3.3 Digital Music: Compress that Music|2021-08-05T21:08:19Z|Because computing is so pervasive, students will be making decisions on computing whether they end up in business, education, working for NGOs, or working in government. We want to put CS105 graduates in the best position possible to make informed decisions on computing in both their personnel and professional lives.  Students will gain a good understanding of how computing devices work, what their limitations are, and what their strengths are. Students will gain practical skills that they can take forward from the course, including a thorough grounding in webpage development. Working with computers should be fun. We want to ensure that our students enjoy the class.|TiipidPdVw4
316|8|13|Stanford CS105: Intro to Computers , 2021 , Lecture 3.4 Digital Music: Digital Data vs. Analog World|2021-08-05T21:08:19Z|Because computing is so pervasive, students will be making decisions on computing whether they end up in business, education, working for NGOs, or working in government. We want to put CS105 graduates in the best position possible to make informed decisions on computing in both their personnel and professional lives.  Students will gain a good understanding of how computing devices work, what their limitations are, and what their strengths are. Students will gain practical skills that they can take forward from the course, including a thorough grounding in webpage development. Working with computers should be fun. We want to ensure that our students enjoy the class.|nOeh-WP4Xf0
317|8|14|Stanford CS105: Intro to Computers , 2021 , Lecture 3.5 Digital Music: Perfect Music & Alt. Formats|2021-08-05T21:08:19Z|Because computing is so pervasive, students will be making decisions on computing whether they end up in business, education, working for NGOs, or working in government. We want to put CS105 graduates in the best position possible to make informed decisions on computing in both their personnel and professional lives.  Students will gain a good understanding of how computing devices work, what their limitations are, and what their strengths are. Students will gain practical skills that they can take forward from the course, including a thorough grounding in webpage development. Working with computers should be fun. We want to ensure that our students enjoy the class.|7LJF8uzDOZg
318|8|15|Stanford CS105: Introduction to Computers , 2021 , Lecture 4.1 Computer Hardware: An Overview|2021-08-05T21:08:19Z|Because computing is so pervasive, students will be making decisions on computing whether they end up in business, education, working for NGOs, or working in government. We want to put CS105 graduates in the best position possible to make informed decisions on computing in both their personnel and professional lives.  Students will gain a good understanding of how computing devices work, what their limitations are, and what their strengths are. Students will gain practical skills that they can take forward from the course, including a thorough grounding in webpage development. Working with computers should be fun. We want to ensure that our students enjoy the class.|yIdUOFxc0mQ
319|8|16|Stanford CS105 , 2021 , Lecture 4.2 Computer Hardware: A Closer Look at Processing|2021-08-05T21:08:19Z|Because computing is so pervasive, students will be making decisions on computing whether they end up in business, education, working for NGOs, or working in government. We want to put CS105 graduates in the best position possible to make informed decisions on computing in both their personnel and professional lives.  Students will gain a good understanding of how computing devices work, what their limitations are, and what their strengths are. Students will gain practical skills that they can take forward from the course, including a thorough grounding in webpage development. Working with computers should be fun. We want to ensure that our students enjoy the class.|x0Gh7zUsEJM
320|8|17|Stanford CS105: Intro to Computers , 2021 , Lecture 4.3 Computer Hardware: Anatomy of a Laptop|2021-08-05T21:08:19Z|Because computing is so pervasive, students will be making decisions on computing whether they end up in business, education, working for NGOs, or working in government. We want to put CS105 graduates in the best position possible to make informed decisions on computing in both their personnel and professional lives.  Students will gain a good understanding of how computing devices work, what their limitations are, and what their strengths are. Students will gain practical skills that they can take forward from the course, including a thorough grounding in webpage development. Working with computers should be fun. We want to ensure that our students enjoy the class.|lOxvIPCNm3Q
321|8|18|Stanford CS105: Introduction to Computers , 2021 , Lecture 4.4 Computer Hardware: Virtual Memory|2021-08-05T21:08:19Z|Because computing is so pervasive, students will be making decisions on computing whether they end up in business, education, working for NGOs, or working in government. We want to put CS105 graduates in the best position possible to make informed decisions on computing in both their personnel and professional lives.  Students will gain a good understanding of how computing devices work, what their limitations are, and what their strengths are. Students will gain practical skills that they can take forward from the course, including a thorough grounding in webpage development. Working with computers should be fun. We want to ensure that our students enjoy the class.|xs_jmQt5D0s
322|8|19|Stanford CS105: Introduction to Computers , 2021 , Lecture 5.1 Computer Networks: Hardware|2021-08-05T21:08:19Z|Because computing is so pervasive, students will be making decisions on computing whether they end up in business, education, working for NGOs, or working in government. We want to put CS105 graduates in the best position possible to make informed decisions on computing in both their personnel and professional lives.  Students will gain a good understanding of how computing devices work, what their limitations are, and what their strengths are. Students will gain practical skills that they can take forward from the course, including a thorough grounding in webpage development. Working with computers should be fun. We want to ensure that our students enjoy the class.|ypr3W8YbaH8
323|8|20|Stanford CS105: Introduction to Computers , 2021 , Lecture 5.2 Computer Networks: Naming|2021-08-05T21:08:19Z|Because computing is so pervasive, students will be making decisions on computing whether they end up in business, education, working for NGOs, or working in government. We want to put CS105 graduates in the best position possible to make informed decisions on computing in both their personnel and professional lives.  Students will gain a good understanding of how computing devices work, what their limitations are, and what their strengths are. Students will gain practical skills that they can take forward from the course, including a thorough grounding in webpage development. Working with computers should be fun. We want to ensure that our students enjoy the class.|wt8Z9SPPz9U
324|8|21|Stanford CS105: Introduction to Computers , 2021 , Lecture 6.1 Network Protocols: What is a Protocol|2021-08-05T21:16:13Z|Because computing is so pervasive, students will be making decisions on computing whether they end up in business, education, working for NGOs, or working in government. We want to put CS105 graduates in the best position possible to make informed decisions on computing in both their personnel and professional lives.  Students will gain a good understanding of how computing devices work, what their limitations are, and what their strengths are. Students will gain practical skills that they can take forward from the course, including a thorough grounding in webpage development. Working with computers should be fun. We want to ensure that our students enjoy the class.|_7MGRAuG3kc
325|8|22|Stanford CS105: Intro to Computers , 2021 , Lecture 6.2 Network Protocols: Protocols of the Internet|2021-08-05T21:16:12Z|Because computing is so pervasive, students will be making decisions on computing whether they end up in business, education, working for NGOs, or working in government. We want to put CS105 graduates in the best position possible to make informed decisions on computing in both their personnel and professional lives.  Students will gain a good understanding of how computing devices work, what their limitations are, and what their strengths are. Students will gain practical skills that they can take forward from the course, including a thorough grounding in webpage development. Working with computers should be fun. We want to ensure that our students enjoy the class.|dxxEJbxuJOE
326|8|23|Stanford CS105: Introduction to Computers , 2021 , Lecture 7.1 Intro to HTML: Origins of the Web|2021-08-05T21:16:13Z|Because computing is so pervasive, students will be making decisions on computing whether they end up in business, education, working for NGOs, or working in government. We want to put CS105 graduates in the best position possible to make informed decisions on computing in both their personnel and professional lives.  Students will gain a good understanding of how computing devices work, what their limitations are, and what their strengths are. Students will gain practical skills that they can take forward from the course, including a thorough grounding in webpage development. Working with computers should be fun. We want to ensure that our students enjoy the class.|TtM1YJ1Z8TA
327|8|24|Stanford CS105: Intro to Computers , 2021 , Lecture 7.2 Intro to HTML: Hypertext Markup Language|2021-08-05T21:16:12Z|Because computing is so pervasive, students will be making decisions on computing whether they end up in business, education, working for NGOs, or working in government. We want to put CS105 graduates in the best position possible to make informed decisions on computing in both their personnel and professional lives.  Students will gain a good understanding of how computing devices work, what their limitations are, and what their strengths are. Students will gain practical skills that they can take forward from the course, including a thorough grounding in webpage development. Working with computers should be fun. We want to ensure that our students enjoy the class.|zymolE4q_Pk
328|8|25|Stanford CS105 , 2021 , Lecture 7.3 Intro to HTML: Creating a Webpage Step-by-Step|2021-08-05T21:16:12Z|Because computing is so pervasive, students will be making decisions on computing whether they end up in business, education, working for NGOs, or working in government. We want to put CS105 graduates in the best position possible to make informed decisions on computing in both their personnel and professional lives.  Students will gain a good understanding of how computing devices work, what their limitations are, and what their strengths are. Students will gain practical skills that they can take forward from the course, including a thorough grounding in webpage development. Working with computers should be fun. We want to ensure that our students enjoy the class.|6IgMedQysSw
329|8|26|Stanford CS105: Intro to Computers , 2021 , Lecture 7.4 Intro to HTML: Grammar & Vocabulary Rules|2021-08-05T21:16:12Z|Because computing is so pervasive, students will be making decisions on computing whether they end up in business, education, working for NGOs, or working in government. We want to put CS105 graduates in the best position possible to make informed decisions on computing in both their personnel and professional lives.  Students will gain a good understanding of how computing devices work, what their limitations are, and what their strengths are. Students will gain practical skills that they can take forward from the course, including a thorough grounding in webpage development. Working with computers should be fun. We want to ensure that our students enjoy the class.|sYGcCi5xbtI
330|8|27|Stanford CS105: Introduction to Computers , 2021 , Lecture 8.1 - Introduction to CSS|2021-08-05T21:16:13Z|Because computing is so pervasive, students will be making decisions on computing whether they end up in business, education, working for NGOs, or working in government. We want to put CS105 graduates in the best position possible to make informed decisions on computing in both their personnel and professional lives.  Students will gain a good understanding of how computing devices work, what their limitations are, and what their strengths are. Students will gain practical skills that they can take forward from the course, including a thorough grounding in webpage development. Working with computers should be fun. We want to ensure that our students enjoy the class.|gpTIWKu36vQ
331|8|28|Stanford CS105: Introduction to Computers , 2021 , Lecture 8.2 Linking Webpages: Making Links|2021-08-05T21:16:12Z|Because computing is so pervasive, students will be making decisions on computing whether they end up in business, education, working for NGOs, or working in government. We want to put CS105 graduates in the best position possible to make informed decisions on computing in both their personnel and professional lives.  Students will gain a good understanding of how computing devices work, what their limitations are, and what their strengths are. Students will gain practical skills that they can take forward from the course, including a thorough grounding in webpage development. Working with computers should be fun. We want to ensure that our students enjoy the class.|adggOmf3OEM
332|8|29|Stanford CS105: Introduction to Computers , 2021 , Lecture 8.3 Linking Webpages: Formatting Links|2021-08-05T21:16:12Z|Because computing is so pervasive, students will be making decisions on computing whether they end up in business, education, working for NGOs, or working in government. We want to put CS105 graduates in the best position possible to make informed decisions on computing in both their personnel and professional lives.  Students will gain a good understanding of how computing devices work, what their limitations are, and what their strengths are. Students will gain practical skills that they can take forward from the course, including a thorough grounding in webpage development. Working with computers should be fun. We want to ensure that our students enjoy the class.|HLdDvQym5xk
333|8|30|Stanford CS105: Introduction to Computers , 2021 , Lecture 9.1 Creating Webpages: Images|2021-08-05T21:16:12Z|Because computing is so pervasive, students will be making decisions on computing whether they end up in business, education, working for NGOs, or working in government. We want to put CS105 graduates in the best position possible to make informed decisions on computing in both their personnel and professional lives.  Students will gain a good understanding of how computing devices work, what their limitations are, and what their strengths are. Students will gain practical skills that they can take forward from the course, including a thorough grounding in webpage development. Working with computers should be fun. We want to ensure that our students enjoy the class.|eJ04XVHkr4s
334|8|31|Stanford CS105: Introduction to Computers , 2021 , Lecture: 9.2 Creating Webpages: Specifying Colors|2021-08-05T21:16:12Z|Because computing is so pervasive, students will be making decisions on computing whether they end up in business, education, working for NGOs, or working in government. We want to put CS105 graduates in the best position possible to make informed decisions on computing in both their personnel and professional lives.  Students will gain a good understanding of how computing devices work, what their limitations are, and what their strengths are. Students will gain practical skills that they can take forward from the course, including a thorough grounding in webpage development. Working with computers should be fun. We want to ensure that our students enjoy the class.|6OxyB9yuSzo
335|8|32|Stanford CS105: Introduction to Computers , 2021 , Lecture 9.3 Hexadecimal|2021-08-05T21:16:12Z|Because computing is so pervasive, students will be making decisions on computing whether they end up in business, education, working for NGOs, or working in government. We want to put CS105 graduates in the best position possible to make informed decisions on computing in both their personnel and professional lives.  Students will gain a good understanding of how computing devices work, what their limitations are, and what their strengths are. Students will gain practical skills that they can take forward from the course, including a thorough grounding in webpage development. Working with computers should be fun. We want to ensure that our students enjoy the class.|GrV6BKNlCic
336|8|33|Stanford CS105: Introduction to Computers , 2021 , Lecture 9.4 Webpage Example: Captions|2021-08-05T21:16:13Z|Because computing is so pervasive, students will be making decisions on computing whether they end up in business, education, working for NGOs, or working in government. We want to put CS105 graduates in the best position possible to make informed decisions on computing in both their personnel and professional lives.  Students will gain a good understanding of how computing devices work, what their limitations are, and what their strengths are. Students will gain practical skills that they can take forward from the course, including a thorough grounding in webpage development. Working with computers should be fun. We want to ensure that our students enjoy the class.|ro34I0AgNs0
337|8|34|Stanford CS105: Introduction to Computers , 2021 , Lecture 10.1 Creating Webpages: Adding Tables|2021-08-05T21:16:12Z|Because computing is so pervasive, students will be making decisions on computing whether they end up in business, education, working for NGOs, or working in government. We want to put CS105 graduates in the best position possible to make informed decisions on computing in both their personnel and professional lives.  Students will gain a good understanding of how computing devices work, what their limitations are, and what their strengths are. Students will gain practical skills that they can take forward from the course, including a thorough grounding in webpage development. Working with computers should be fun. We want to ensure that our students enjoy the class.|2pN-U5WpJ7o
338|8|35|Stanford CS105: Introduction to Computers , 2021 , Lecture 10.2 Webpage Example: Blog|2021-08-05T21:16:12Z|Because computing is so pervasive, students will be making decisions on computing whether they end up in business, education, working for NGOs, or working in government. We want to put CS105 graduates in the best position possible to make informed decisions on computing in both their personnel and professional lives.  Students will gain a good understanding of how computing devices work, what their limitations are, and what their strengths are. Students will gain practical skills that they can take forward from the course, including a thorough grounding in webpage development. Working with computers should be fun. We want to ensure that our students enjoy the class.|A3Q45MUUM4A
339|8|36|Stanford CS105: Intro to Computers , 2021 , Lec 10.3 Creating Webpages: Working with a Web Server|2021-08-05T21:16:13Z|Because computing is so pervasive, students will be making decisions on computing whether they end up in business, education, working for NGOs, or working in government. We want to put CS105 graduates in the best position possible to make informed decisions on computing in both their personnel and professional lives.  Students will gain a good understanding of how computing devices work, what their limitations are, and what their strengths are. Students will gain practical skills that they can take forward from the course, including a thorough grounding in webpage development. Working with computers should be fun. We want to ensure that our students enjoy the class.|pWSDCGNq3s0
340|8|37|Stanford CS105 , 2021 , Lecture 11.1 Webpage Layout: Overview of Layout Techniques|2021-08-05T21:16:12Z|Because computing is so pervasive, students will be making decisions on computing whether they end up in business, education, working for NGOs, or working in government. We want to put CS105 graduates in the best position possible to make informed decisions on computing in both their personnel and professional lives.  Students will gain a good understanding of how computing devices work, what their limitations are, and what their strengths are. Students will gain practical skills that they can take forward from the course, including a thorough grounding in webpage development. Working with computers should be fun. We want to ensure that our students enjoy the class.|rA0lHWdsNeU
341|8|38|Stanford CS105: Introduction to Computers , 2021 , Lecture 11.2 Webpage Layout: Grid-Based Layout|2021-08-05T21:16:12Z|Because computing is so pervasive, students will be making decisions on computing whether they end up in business, education, working for NGOs, or working in government. We want to put CS105 graduates in the best position possible to make informed decisions on computing in both their personnel and professional lives.  Students will gain a good understanding of how computing devices work, what their limitations are, and what their strengths are. Students will gain practical skills that they can take forward from the course, including a thorough grounding in webpage development. Working with computers should be fun. We want to ensure that our students enjoy the class.|135_QQk8cRA
342|8|39|Stanford CS105: Intro to Computers , 2021 , Lecture 12 Webpage Reproduction: The New York Times|2021-08-05T21:16:13Z|Because computing is so pervasive, students will be making decisions on computing whether they end up in business, education, working for NGOs, or working in government. We want to put CS105 graduates in the best position possible to make informed decisions on computing in both their personnel and professional lives.  Students will gain a good understanding of how computing devices work, what their limitations are, and what their strengths are. Students will gain practical skills that they can take forward from the course, including a thorough grounding in webpage development. Working with computers should be fun. We want to ensure that our students enjoy the class.|DA4jAHg8U-M
343|8|40|Stanford CS105: Introduction to Computers , 2021 , Lecture 13.1 Creating Webpages: Forms for Input|2021-08-05T21:16:12Z|Because computing is so pervasive, students will be making decisions on computing whether they end up in business, education, working for NGOs, or working in government. We want to put CS105 graduates in the best position possible to make informed decisions on computing in both their personnel and professional lives.  Students will gain a good understanding of how computing devices work, what their limitations are, and what their strengths are. Students will gain practical skills that they can take forward from the course, including a thorough grounding in webpage development. Working with computers should be fun. We want to ensure that our students enjoy the class.|_k3FBZQXR5s
344|8|41|Stanford CS105: Intro to Computers , 2021 , Lecture 13.2 Webpage Reproduction: Washington Post|2021-08-05T21:16:12Z|Because computing is so pervasive, students will be making decisions on computing whether they end up in business, education, working for NGOs, or working in government. We want to put CS105 graduates in the best position possible to make informed decisions on computing in both their personnel and professional lives.  Students will gain a good understanding of how computing devices work, what their limitations are, and what their strengths are. Students will gain practical skills that they can take forward from the course, including a thorough grounding in webpage development. Working with computers should be fun. We want to ensure that our students enjoy the class.|apC9HmI7DRk
345|8|42|Stanford CS105: Introduction to Computers , 2021 , Lecture 14.1 Advanced Image Techniques|2021-08-05T21:16:12Z|Because computing is so pervasive, students will be making decisions on computing whether they end up in business, education, working for NGOs, or working in government. We want to put CS105 graduates in the best position possible to make informed decisions on computing in both their personnel and professional lives.  Students will gain a good understanding of how computing devices work, what their limitations are, and what their strengths are. Students will gain practical skills that they can take forward from the course, including a thorough grounding in webpage development. Working with computers should be fun. We want to ensure that our students enjoy the class.|NaLLY3zKUII
346|8|43|Stanford CS105: Introduction to Computers , 2021 , Lecture 14.2 Forms: Get vs Post|2021-08-05T21:16:12Z|Because computing is so pervasive, students will be making decisions on computing whether they end up in business, education, working for NGOs, or working in government. We want to put CS105 graduates in the best position possible to make informed decisions on computing in both their personnel and professional lives.  Students will gain a good understanding of how computing devices work, what their limitations are, and what their strengths are. Students will gain practical skills that they can take forward from the course, including a thorough grounding in webpage development. Working with computers should be fun. We want to ensure that our students enjoy the class.|oSPvIhkAm1o
347|8|44|Stanford CS105: Introduction to Computers , 2021 , Lecture 14.3 Responsive Webpage Design|2021-08-05T21:16:12Z|Because computing is so pervasive, students will be making decisions on computing whether they end up in business, education, working for NGOs, or working in government. We want to put CS105 graduates in the best position possible to make informed decisions on computing in both their personnel and professional lives.  Students will gain a good understanding of how computing devices work, what their limitations are, and what their strengths are. Students will gain practical skills that they can take forward from the course, including a thorough grounding in webpage development. Working with computers should be fun. We want to ensure that our students enjoy the class.|r7daBdqV7cc
348|8|45|Stanford CS105: Introduction to Computers , 2021 , Lecture 15.1 Human-Computer Interaction|2021-08-05T21:16:12Z|Because computing is so pervasive, students will be making decisions on computing whether they end up in business, education, working for NGOs, or working in government. We want to put CS105 graduates in the best position possible to make informed decisions on computing in both their personnel and professional lives.  Students will gain a good understanding of how computing devices work, what their limitations are, and what their strengths are. Students will gain practical skills that they can take forward from the course, including a thorough grounding in webpage development. Working with computers should be fun. We want to ensure that our students enjoy the class.|ckhu-nU-s9c
349|8|46|Stanford CS105: Introduction to Computers , 2021 , Lecture 15.2 Website Design|2021-08-05T21:16:12Z|Because computing is so pervasive, students will be making decisions on computing whether they end up in business, education, working for NGOs, or working in government. We want to put CS105 graduates in the best position possible to make informed decisions on computing in both their personnel and professional lives.  Students will gain a good understanding of how computing devices work, what their limitations are, and what their strengths are. Students will gain practical skills that they can take forward from the course, including a thorough grounding in webpage development. Working with computers should be fun. We want to ensure that our students enjoy the class.|0tAtq1vlU5w
350|8|47|Stanford CS105 , 2021 , Lecture 16.1 Intro to Python: Interacting w/ Python Shell|2021-08-05T21:16:12Z|Because computing is so pervasive, students will be making decisions on computing whether they end up in business, education, working for NGOs, or working in government. We want to put CS105 graduates in the best position possible to make informed decisions on computing in both their personnel and professional lives.  Students will gain a good understanding of how computing devices work, what their limitations are, and what their strengths are. Students will gain practical skills that they can take forward from the course, including a thorough grounding in webpage development. Working with computers should be fun. We want to ensure that our students enjoy the class.|G0wGXq8kBZU
351|8|48|Stanford CS105: Intro to Computers , 2021 , Lecture 16.2 Intro to Python: Our First Python Program|2021-08-05T21:16:13Z|Because computing is so pervasive, students will be making decisions on computing whether they end up in business, education, working for NGOs, or working in government. We want to put CS105 graduates in the best position possible to make informed decisions on computing in both their personnel and professional lives.  Students will gain a good understanding of how computing devices work, what their limitations are, and what their strengths are. Students will gain practical skills that they can take forward from the course, including a thorough grounding in webpage development. Working with computers should be fun. We want to ensure that our students enjoy the class.|fF8mnKi6LTw
352|8|49|Stanford CS105: Introduction to Computers , 2021 , Lecture 17.1 About Programming|2021-08-05T21:16:12Z|Because computing is so pervasive, students will be making decisions on computing whether they end up in business, education, working for NGOs, or working in government. We want to put CS105 graduates in the best position possible to make informed decisions on computing in both their personnel and professional lives.  Students will gain a good understanding of how computing devices work, what their limitations are, and what their strengths are. Students will gain practical skills that they can take forward from the course, including a thorough grounding in webpage development. Working with computers should be fun. We want to ensure that our students enjoy the class.|vBrbyb1pwtQ
353|8|50|Stanford CS105: Introduction to Computers , 2021 , Lecture 17.2 Control Structures: Conditionals|2021-08-05T21:16:12Z|Because computing is so pervasive, students will be making decisions on computing whether they end up in business, education, working for NGOs, or working in government. We want to put CS105 graduates in the best position possible to make informed decisions on computing in both their personnel and professional lives.  Students will gain a good understanding of how computing devices work, what their limitations are, and what their strengths are. Students will gain practical skills that they can take forward from the course, including a thorough grounding in webpage development. Working with computers should be fun. We want to ensure that our students enjoy the class.|J7jQPSIReSc
354|8|51|Stanford CS105: Introduction to Computers , 2021 , Lecture 18.1 Additional Python Language Features|2021-08-05T22:29:34Z|Because computing is so pervasive, students will be making decisions on computing whether they end up in business, education, working for NGOs, or working in government. We want to put CS105 graduates in the best position possible to make informed decisions on computing in both their personnel and professional lives.  Students will gain a good understanding of how computing devices work, what their limitations are, and what their strengths are. Students will gain practical skills that they can take forward from the course, including a thorough grounding in webpage development. Working with computers should be fun. We want to ensure that our students enjoy the class.|rtgP-Pxrf2o
355|8|52|Stanford CS105: Introduction to Computers , 2021 , Lecture 18.2 Lists and Loops|2021-08-05T21:48:27Z|Because computing is so pervasive, students will be making decisions on computing whether they end up in business, education, working for NGOs, or working in government. We want to put CS105 graduates in the best position possible to make informed decisions on computing in both their personnel and professional lives.  Students will gain a good understanding of how computing devices work, what their limitations are, and what their strengths are. Students will gain practical skills that they can take forward from the course, including a thorough grounding in webpage development. Working with computers should be fun. We want to ensure that our students enjoy the class.|di0KgqNDqhA
356|8|53|Stanford CS105: Introduction to Computers , 2021 , Lecture 19.1 Strings|2021-08-05T21:48:27Z|Because computing is so pervasive, students will be making decisions on computing whether they end up in business, education, working for NGOs, or working in government. We want to put CS105 graduates in the best position possible to make informed decisions on computing in both their personnel and professional lives.  Students will gain a good understanding of how computing devices work, what their limitations are, and what their strengths are. Students will gain practical skills that they can take forward from the course, including a thorough grounding in webpage development. Working with computers should be fun. We want to ensure that our students enjoy the class.|eDh5_qZ5gNc
357|8|54|Stanford CS105: Introduction to Computers , 2021 , Lecture 19.2 Working with Files|2021-08-05T21:48:28Z|Because computing is so pervasive, students will be making decisions on computing whether they end up in business, education, working for NGOs, or working in government. We want to put CS105 graduates in the best position possible to make informed decisions on computing in both their personnel and professional lives.  Students will gain a good understanding of how computing devices work, what their limitations are, and what their strengths are. Students will gain practical skills that they can take forward from the course, including a thorough grounding in webpage development. Working with computers should be fun. We want to ensure that our students enjoy the class.|F_806eRoceA
358|8|55|Stanford CS105: Introduction to Computers , 2021 , Lecture 20.1 Computer Security: Introduction|2021-08-05T21:48:28Z|Because computing is so pervasive, students will be making decisions on computing whether they end up in business, education, working for NGOs, or working in government. We want to put CS105 graduates in the best position possible to make informed decisions on computing in both their personnel and professional lives.  Students will gain a good understanding of how computing devices work, what their limitations are, and what their strengths are. Students will gain practical skills that they can take forward from the course, including a thorough grounding in webpage development. Working with computers should be fun. We want to ensure that our students enjoy the class.|dKBRavHfhfI
359|8|56|Stanford CS105: Introduction to Computers , 2021 , Lecture 20.2 Computer Security: Mechanisms|2021-08-05T21:48:28Z|Because computing is so pervasive, students will be making decisions on computing whether they end up in business, education, working for NGOs, or working in government. We want to put CS105 graduates in the best position possible to make informed decisions on computing in both their personnel and professional lives.  Students will gain a good understanding of how computing devices work, what their limitations are, and what their strengths are. Students will gain practical skills that they can take forward from the course, including a thorough grounding in webpage development. Working with computers should be fun. We want to ensure that our students enjoy the class.|4Ziwz6d94sE
360|8|57|Stanford CS105: Introduction to Computers , 2021 , Lecture 21.1 Computer Security (Attacks): Malware|2021-08-05T21:48:28Z|Because computing is so pervasive, students will be making decisions on computing whether they end up in business, education, working for NGOs, or working in government. We want to put CS105 graduates in the best position possible to make informed decisions on computing in both their personnel and professional lives.  Students will gain a good understanding of how computing devices work, what their limitations are, and what their strengths are. Students will gain practical skills that they can take forward from the course, including a thorough grounding in webpage development. Working with computers should be fun. We want to ensure that our students enjoy the class.|EwBNTOhczok
361|8|58|Stanford CS105: Intro to Computers , 2021 , Lec 21.2 Computer Security: Attack Vectors/Techniques|2021-08-05T21:48:27Z|Because computing is so pervasive, students will be making decisions on computing whether they end up in business, education, working for NGOs, or working in government. We want to put CS105 graduates in the best position possible to make informed decisions on computing in both their personnel and professional lives.  Students will gain a good understanding of how computing devices work, what their limitations are, and what their strengths are. Students will gain practical skills that they can take forward from the course, including a thorough grounding in webpage development. Working with computers should be fun. We want to ensure that our students enjoy the class.|eKrlUVhM_5k
362|8|59|Stanford CS105: Intro to Computers , 2021 , Lecture 22.1 Computer Security: Defensive Techniques|2021-08-05T21:48:28Z|Because computing is so pervasive, students will be making decisions on computing whether they end up in business, education, working for NGOs, or working in government. We want to put CS105 graduates in the best position possible to make informed decisions on computing in both their personnel and professional lives.  Students will gain a good understanding of how computing devices work, what their limitations are, and what their strengths are. Students will gain practical skills that they can take forward from the course, including a thorough grounding in webpage development. Working with computers should be fun. We want to ensure that our students enjoy the class.|8XuUtzZVgb4
363|8|60|Stanford CS105: Intro to Computers , 2021 , Lecture 22.2 Computer Security: Practical Measures|2021-08-05T21:48:27Z|Because computing is so pervasive, students will be making decisions on computing whether they end up in business, education, working for NGOs, or working in government. We want to put CS105 graduates in the best position possible to make informed decisions on computing in both their personnel and professional lives.  Students will gain a good understanding of how computing devices work, what their limitations are, and what their strengths are. Students will gain practical skills that they can take forward from the course, including a thorough grounding in webpage development. Working with computers should be fun. We want to ensure that our students enjoy the class.|imSwkC-W8KU
364|8|61|Stanford CS105: Introduction to Computers , 2021 , Lecture 23.1 Privacy and Big Data: Privacy|2021-08-05T23:00:10Z|Because computing is so pervasive, students will be making decisions on computing whether they end up in business, education, working for NGOs, or working in government. We want to put CS105 graduates in the best position possible to make informed decisions on computing in both their personnel and professional lives.  Students will gain a good understanding of how computing devices work, what their limitations are, and what their strengths are. Students will gain practical skills that they can take forward from the course, including a thorough grounding in webpage development. Working with computers should be fun. We want to ensure that our students enjoy the class.|T9aXJrLmv9w
365|8|62|Stanford CS105: Introduction to Computers , 2021 , Lecture 23.2 Privacy and Big Data: Big Data|2021-08-05T23:00:13Z|Because computing is so pervasive, students will be making decisions on computing whether they end up in business, education, working for NGOs, or working in government. We want to put CS105 graduates in the best position possible to make informed decisions on computing in both their personnel and professional lives.  Students will gain a good understanding of how computing devices work, what their limitations are, and what their strengths are. Students will gain practical skills that they can take forward from the course, including a thorough grounding in webpage development. Working with computers should be fun. We want to ensure that our students enjoy the class.|zv5A7ofqAEo
366|8|63|Stanford CS105: Introduction to Computers , 2021 , Lecture 24.1 Artificial Intelligence: What is AI|2021-08-10T16:46:34Z|Because computing is so pervasive, students will be making decisions on computing whether they end up in business, education, working for NGOs, or working in government. We want to put CS105 graduates in the best position possible to make informed decisions on computing in both their personnel and professional lives.  Students will gain a good understanding of how computing devices work, what their limitations are, and what their strengths are. Students will gain practical skills that they can take forward from the course, including a thorough grounding in webpage development. Working with computers should be fun. We want to ensure that our students enjoy the class.|kVJKb7IHbsw
367|8|64|Stanford CS105: Intro to Computers , 2021 , Lecture 24.2 Artificial Intelligence: Subfields of AI|2021-08-10T16:46:53Z|Because computing is so pervasive, students will be making decisions on computing whether they end up in business, education, working for NGOs, or working in government. We want to put CS105 graduates in the best position possible to make informed decisions on computing in both their personnel and professional lives.  Students will gain a good understanding of how computing devices work, what their limitations are, and what their strengths are. Students will gain practical skills that they can take forward from the course, including a thorough grounding in webpage development. Working with computers should be fun. We want to ensure that our students enjoy the class.|r8J-tyDWjNg
368|8|65|Stanford CS105: Introduction to Computers , 2021 , Lecture 25 Artificial Intelligence: How It's Done|2021-08-10T16:47:19Z|Because computing is so pervasive, students will be making decisions on computing whether they end up in business, education, working for NGOs, or working in government. We want to put CS105 graduates in the best position possible to make informed decisions on computing in both their personnel and professional lives.  Students will gain a good understanding of how computing devices work, what their limitations are, and what their strengths are. Students will gain practical skills that they can take forward from the course, including a thorough grounding in webpage development. Working with computers should be fun. We want to ensure that our students enjoy the class.|vat2Mxu71VE
369|8|66|Stanford CS105: Introduction to Computers , 2021 , Lecture 26.1 - Cloud Computing|2021-08-10T20:49:27Z|Because computing is so pervasive, students will be making decisions on computing whether they end up in business, education, working for NGOs, or working in government. We want to put CS105 graduates in the best position possible to make informed decisions on computing in both their personnel and professional lives.  Students will gain a good understanding of how computing devices work, what their limitations are, and what their strengths are. Students will gain practical skills that they can take forward from the course, including a thorough grounding in webpage development. Working with computers should be fun. We want to ensure that our students enjoy the class.|1r8ClbKmEtE
370|8|67|Stanford CS105: Introduction to Computers , 2021 , Lecture  26.2 Internet of Things|2021-08-10T20:49:55Z|Because computing is so pervasive, students will be making decisions on computing whether they end up in business, education, working for NGOs, or working in government. We want to put CS105 graduates in the best position possible to make informed decisions on computing in both their personnel and professional lives.  Students will gain a good understanding of how computing devices work, what their limitations are, and what their strengths are. Students will gain practical skills that they can take forward from the course, including a thorough grounding in webpage development. Working with computers should be fun. We want to ensure that our students enjoy the class.|mR4DKWoZEM4
371|8|68|Stanford CS105: Introduction to Computers , 2021 , Lecture 27.1 Theory: Analysis of Algorithms|2021-08-10T20:50:42Z|Because computing is so pervasive, students will be making decisions on computing whether they end up in business, education, working for NGOs, or working in government. We want to put CS105 graduates in the best position possible to make informed decisions on computing in both their personnel and professional lives.  Students will gain a good understanding of how computing devices work, what their limitations are, and what their strengths are. Students will gain practical skills that they can take forward from the course, including a thorough grounding in webpage development. Working with computers should be fun. We want to ensure that our students enjoy the class.|J4OnlGsym8Y
372|8|69|Stanford CS105: Introduction to Computers , 2021 , Lecture 27.2 Theory: Undecidable Problems|2021-08-10T20:51:06Z|Because computing is so pervasive, students will be making decisions on computing whether they end up in business, education, working for NGOs, or working in government. We want to put CS105 graduates in the best position possible to make informed decisions on computing in both their personnel and professional lives.  Students will gain a good understanding of how computing devices work, what their limitations are, and what their strengths are. Students will gain practical skills that they can take forward from the course, including a thorough grounding in webpage development. Working with computers should be fun. We want to ensure that our students enjoy the class.|Sw6S48iZ6DM
373|9|1|Stanford ENGR108: Introduction to Applied Linear Algebra , 2020 , Lecture 1 - Introduction|2021-02-22T22:03:28Z|Lectures by Professor Stephen Boyd, Stanford University.  Introduction to applied linear algebra with emphasis on applications.   Topics include: Vectors, norm, and angle; linear independence and orthonormal sets; applications to document analysis. Clustering and the k-means algorithm. Matrices, left and right inverses, QR factorization. Least-squares and model fitting, regularization and cross-validation. Constrained and nonlinear least-squares.  To follow along with the course schedule and syllabus, visit: https://web.stanford.edu/class/engr108/   To view all online courses and programs offered by Stanford, visit: http://online.stanford.edu​|oR6G1MUMveE
374|9|2|Stanford ENGR108: Introduction to Applied Linear Algebra , 2020 , Lecture 2 - vector notation|2021-02-22T22:02:39Z|Lectures by Professor Stephen Boyd, Stanford University.  Introduction to applied linear algebra with emphasis on applications.   Topics include: Vectors, norm, and angle; linear independence and orthonormal sets; applications to document analysis. Clustering and the k-means algorithm. Matrices, left and right inverses, QR factorization. Least-squares and model fitting, regularization and cross-validation. Constrained and nonlinear least-squares.  To follow along with the course schedule and syllabus, visit: https://web.stanford.edu/class/engr108/   To view all online courses and programs offered by Stanford, visit: http://online.stanford.edu​|vVspolIKPgc
375|9|3|Stanford ENGR108: Introduction to Applied Linear Algebra , 2020 , Lecture 3 - vector examples|2021-02-24T22:57:40Z|Lectures by Professor Stephen Boyd, Stanford University.  Introduction to applied linear algebra with emphasis on applications.   Topics include: Vectors, norm, and angle; linear independence and orthonormal sets; applications to document analysis. Clustering and the k-means algorithm. Matrices, left and right inverses, QR factorization. Least-squares and model fitting, regularization and cross-validation. Constrained and nonlinear least-squares.  To follow along with the course schedule and syllabus, visit: https://web.stanford.edu/class/engr108/   To view all online courses and programs offered by Stanford, visit: http://online.stanford.edu​|wlPiJZ1DQjs
376|9|4|Stanford ENGR108: Introduction to Applied Linear Algebra , 2020 , Lecture 4-addition & scalar mult.|2021-02-24T00:05:12Z|Lectures by Professor Stephen Boyd, Stanford University.  Introduction to applied linear algebra with emphasis on applications.   Topics include: Vectors, norm, and angle; linear independence and orthonormal sets; applications to document analysis. Clustering and the k-means algorithm. Matrices, left and right inverses, QR factorization. Least-squares and model fitting, regularization and cross-validation. Constrained and nonlinear least-squares.  To follow along with the course schedule and syllabus, visit: https://web.stanford.edu/class/engr108/   To view all online courses and programs offered by Stanford, visit: http://online.stanford.edu​|Ih3rIcBnnSg
377|9|5|Stanford ENGR108: Introduction to Applied Linear Algebra , 2020 , Lecture 5 - VMLS inner product|2021-02-24T00:14:24Z|Lectures by Professor Stephen Boyd, Stanford University.  Introduction to applied linear algebra with emphasis on applications.   Topics include: Vectors, norm, and angle; linear independence and orthonormal sets; applications to document analysis. Clustering and the k-means algorithm. Matrices, left and right inverses, QR factorization. Least-squares and model fitting, regularization and cross-validation. Constrained and nonlinear least-squares.  To follow along with the course schedule and syllabus, visit: https://web.stanford.edu/class/engr108/   To view all online courses and programs offered by Stanford, visit: http://online.stanford.edu​|YiCAeeBEo50
378|9|6|Stanford ENGR108: Introduction to Applied Linear Algebra , 2020 , Lecture 6 - VMLS complexity|2021-02-24T00:24:35Z|Lectures by Professor Stephen Boyd, Stanford University.  Introduction to applied linear algebra with emphasis on applications.   Topics include: Vectors, norm, and angle; linear independence and orthonormal sets; applications to document analysis. Clustering and the k-means algorithm. Matrices, left and right inverses, QR factorization. Least-squares and model fitting, regularization and cross-validation. Constrained and nonlinear least-squares.  To follow along with the course schedule and syllabus, visit: https://web.stanford.edu/class/engr108/   To view all online courses and programs offered by Stanford, visit: http://online.stanford.edu​|wqcknO3fArY
379|9|7|Stanford ENGR108: Introduction to Applied Linear Algebra , 2020 , Lecture 7 - VMLS linear functions|2021-02-24T23:00:05Z|Lectures by Professor Stephen Boyd, Stanford University.  Introduction to applied linear algebra with emphasis on applications.   Topics include: Vectors, norm, and angle; linear independence and orthonormal sets; applications to document analysis. Clustering and the k-means algorithm. Matrices, left and right inverses, QR factorization. Least-squares and model fitting, regularization and cross-validation. Constrained and nonlinear least-squares.  To follow along with the course schedule and syllabus, visit: https://web.stanford.edu/class/engr108/   To view all online courses and programs offered by Stanford, visit: http://online.stanford.edu​|0rOEhJlNzhs
380|9|8|Stanford ENGR108: Introduction to Applied Linear Algebra , 2020 , Lecture 8-VMLS taylor approx & reg|2021-02-24T23:03:37Z|Lectures by Professor Stephen Boyd, Stanford University.  Introduction to applied linear algebra with emphasis on applications.   Topics include: Vectors, norm, and angle; linear independence and orthonormal sets; applications to document analysis. Clustering and the k-means algorithm. Matrices, left and right inverses, QR factorization. Least-squares and model fitting, regularization and cross-validation. Constrained and nonlinear least-squares.  To follow along with the course schedule and syllabus, visit: https://web.stanford.edu/class/engr108/   To view all online courses and programs offered by Stanford, visit: http://online.stanford.edu​|H0jR5nqueWk
381|9|9|Stanford ENGR108: Introduction to Applied Linear Algebra , 2020 , Lecture 9 - VMLS norm|2021-02-24T23:08:24Z|Lectures by Professor Stephen Boyd, Stanford University.  Introduction to applied linear algebra with emphasis on applications.   Topics include: Vectors, norm, and angle; linear independence and orthonormal sets; applications to document analysis. Clustering and the k-means algorithm. Matrices, left and right inverses, QR factorization. Least-squares and model fitting, regularization and cross-validation. Constrained and nonlinear least-squares.  To follow along with the course schedule and syllabus, visit: https://web.stanford.edu/class/engr108/   To view all online courses and programs offered by Stanford, visit: http://online.stanford.edu​|odR39SYhUH0
382|9|10|Stanford ENGR108: Introduction to Applied Linear Algebra , 2020 , Lecture 10 - VMLS distance|2021-02-24T23:18:03Z|Lectures by Professor Stephen Boyd, Stanford University.  Introduction to applied linear algebra with emphasis on applications.   Topics include: Vectors, norm, and angle; linear independence and orthonormal sets; applications to document analysis. Clustering and the k-means algorithm. Matrices, left and right inverses, QR factorization. Least-squares and model fitting, regularization and cross-validation. Constrained and nonlinear least-squares.  To follow along with the course schedule and syllabus, visit: https://web.stanford.edu/class/engr108/   To view all online courses and programs offered by Stanford, visit: http://online.stanford.edu​|w77MlgsylqY
383|9|11|Stanford ENGR108: Introduction to Applied Linear Algebra , 2020 , Lecture 11-VMLS std. deviation|2021-02-24T23:29:40Z|Lectures by Professor Stephen Boyd, Stanford University.  Introduction to applied linear algebra with emphasis on applications.   Topics include: Vectors, norm, and angle; linear independence and orthonormal sets; applications to document analysis. Clustering and the k-means algorithm. Matrices, left and right inverses, QR factorization. Least-squares and model fitting, regularization and cross-validation. Constrained and nonlinear least-squares.  To follow along with the course schedule and syllabus, visit: https://web.stanford.edu/class/engr108/   To view all online courses and programs offered by Stanford, visit: http://online.stanford.edu​|Gdl92kGR8Ho
384|9|12|Stanford ENGR108: Introduction to Applied Linear Algebra , 2020 , Lecture 12 - VMLS angle|2021-02-25T00:47:00Z|Lectures by Professor Stephen Boyd, Stanford University.  Introduction to applied linear algebra with emphasis on applications.   Topics include: Vectors, norm, and angle; linear independence and orthonormal sets; applications to document analysis. Clustering and the k-means algorithm. Matrices, left and right inverses, QR factorization. Least-squares and model fitting, regularization and cross-validation. Constrained and nonlinear least-squares.  To follow along with the course schedule and syllabus, visit: https://web.stanford.edu/class/engr108/   To view all online courses and programs offered by Stanford, visit: http://online.stanford.edu​|4shS22XxZYw
385|9|13|Stanford ENGR108: Introduction to Applied Linear Algebra , 2020 , Lecture 13 - VMLS k means|2021-02-25T00:54:25Z|Lectures by Professor Stephen Boyd, Stanford University.  Introduction to applied linear algebra with emphasis on applications.   Topics include: Vectors, norm, and angle; linear independence and orthonormal sets; applications to document analysis. Clustering and the k-means algorithm. Matrices, left and right inverses, QR factorization. Least-squares and model fitting, regularization and cross-validation. Constrained and nonlinear least-squares.  To follow along with the course schedule and syllabus, visit: https://web.stanford.edu/class/engr108/   To view all online courses and programs offered by Stanford, visit: http://online.stanford.edu​|liaZ_SCuE1w
386|9|14|Stanford ENGR108: Introduction to Applied Linear Algebra , 2020 , Lecture 14-VMLS k means app.|2021-02-25T00:54:22Z|Lectures by Professor Stephen Boyd, Stanford University.  Introduction to applied linear algebra with emphasis on applications.   Topics include: Vectors, norm, and angle; linear independence and orthonormal sets; applications to document analysis. Clustering and the k-means algorithm. Matrices, left and right inverses, QR factorization. Least-squares and model fitting, regularization and cross-validation. Constrained and nonlinear least-squares.  To follow along with the course schedule and syllabus, visit: https://web.stanford.edu/class/engr108/   To view all online courses and programs offered by Stanford, visit: http://online.stanford.edu​|a4GjONqojzM
387|9|15|Stanford ENGR108: Intro to Applied Linear Algebra , 2020 , Lecture 15-VMLS linear ind.|2021-02-25T00:56:28Z|Lectures by Professor Stephen Boyd, Stanford University.  Introduction to applied linear algebra with emphasis on applications.   Topics include: Vectors, norm, and angle; linear independence and orthonormal sets; applications to document analysis. Clustering and the k-means algorithm. Matrices, left and right inverses, QR factorization. Least-squares and model fitting, regularization and cross-validation. Constrained and nonlinear least-squares.  To follow along with the course schedule and syllabus, visit: https://web.stanford.edu/class/engr108/   To view all online courses and programs offered by Stanford, visit: http://online.stanford.edu​|RikEADik56Q
388|9|16|Stanford ENGR108: Introduction to Applied Linear Algebra , 2020 , Lecture 16-VMLS Gram Schmidt algo.|2021-02-25T00:58:06Z|Lectures by Professor Stephen Boyd, Stanford University.  Introduction to applied linear algebra with emphasis on applications.   Topics include: Vectors, norm, and angle; linear independence and orthonormal sets; applications to document analysis. Clustering and the k-means algorithm. Matrices, left and right inverses, QR factorization. Least-squares and model fitting, regularization and cross-validation. Constrained and nonlinear least-squares.  To follow along with the course schedule and syllabus, visit: https://web.stanford.edu/class/engr108/   To view all online courses and programs offered by Stanford, visit: http://online.stanford.edu​|IZhpqYeKZEM
389|9|17|Stanford ENGR108: Introduction to Applied Linear Algebra , 2020 , Lecture 17 - VMLS matrix notation|2021-02-25T00:59:34Z|Lectures by Professor Stephen Boyd, Stanford University.  Introduction to applied linear algebra with emphasis on applications.   Topics include: Vectors, norm, and angle; linear independence and orthonormal sets; applications to document analysis. Clustering and the k-means algorithm. Matrices, left and right inverses, QR factorization. Least-squares and model fitting, regularization and cross-validation. Constrained and nonlinear least-squares.  To follow along with the course schedule and syllabus, visit: https://web.stanford.edu/class/engr108/   To view all online courses and programs offered by Stanford, visit: http://online.stanford.edu​|dTeN9ypsVTU
390|9|18|Stanford ENGR108: Introduction to Applied Linear Algebra , 2020 , Lecture 18-VMLS matrix vector mult|2021-02-26T17:37:02Z|Lectures by Professor Stephen Boyd, Stanford University.  Introduction to applied linear algebra with emphasis on applications.   Topics include: Vectors, norm, and angle; linear independence and orthonormal sets; applications to document analysis. Clustering and the k-means algorithm. Matrices, left and right inverses, QR factorization. Least-squares and model fitting, regularization and cross-validation. Constrained and nonlinear least-squares.  To follow along with the course schedule and syllabus, visit: https://web.stanford.edu/class/engr108/   To view all online courses and programs offered by Stanford, visit: http://online.stanford.edu​|1_LEoD33gC0
391|9|19|Stanford ENGR108: Introduction to Applied Linear Algebra , 2020 , Lecture 19-VMLS matrix vector  ex.|2021-02-26T18:24:45Z|Lectures by Professor Stephen Boyd, Stanford University.  Introduction to applied linear algebra with emphasis on applications.   Topics include: Vectors, norm, and angle; linear independence and orthonormal sets; applications to document analysis. Clustering and the k-means algorithm. Matrices, left and right inverses, QR factorization. Least-squares and model fitting, regularization and cross-validation. Constrained and nonlinear least-squares.  To follow along with the course schedule and syllabus, visit: https://web.stanford.edu/class/engr108/   To view all online courses and programs offered by Stanford, visit: http://online.stanford.edu​|JbtNJM1hM_4
392|9|20|Stanford ENGR108: Introduction to Applied Linear Algebra , 2020 , Lecture 20-VMLS selector matrices|2021-02-26T19:51:48Z|Lectures by Professor Stephen Boyd, Stanford University.  Introduction to applied linear algebra with emphasis on applications.   Topics include: Vectors, norm, and angle; linear independence and orthonormal sets; applications to document analysis. Clustering and the k-means algorithm. Matrices, left and right inverses, QR factorization. Least-squares and model fitting, regularization and cross-validation. Constrained and nonlinear least-squares.  To follow along with the course schedule and syllabus, visit: https://web.stanford.edu/class/engr108/   To view all online courses and programs offered by Stanford, visit: http://online.stanford.edu​|jXN50fuRSqE
393|9|21|Stanford ENGR108: Introduction to Applied Linear Algebra , 2020 , Lecture 21 - VMLS incidence matrix|2021-02-26T22:13:21Z|Lectures by Professor Stephen Boyd, Stanford University.  Introduction to applied linear algebra with emphasis on applications.   Topics include: Vectors, norm, and angle; linear independence and orthonormal sets; applications to document analysis. Clustering and the k-means algorithm. Matrices, left and right inverses, QR factorization. Least-squares and model fitting, regularization and cross-validation. Constrained and nonlinear least-squares.  To follow along with the course schedule and syllabus, visit: https://web.stanford.edu/class/engr108/   To view all online courses and programs offered by Stanford, visit: http://online.stanford.edu​|v7CH2ry-vrQ
394|9|22|Stanford ENGR108: Introduction to Applied Linear Algebra , 2020 , Lecture 22 - VMLS convolution|2021-02-26T22:15:15Z|Lectures by Professor Stephen Boyd, Stanford University.  Introduction to applied linear algebra with emphasis on applications.   Topics include: Vectors, norm, and angle; linear independence and orthonormal sets; applications to document analysis. Clustering and the k-means algorithm. Matrices, left and right inverses, QR factorization. Least-squares and model fitting, regularization and cross-validation. Constrained and nonlinear least-squares.  To follow along with the course schedule and syllabus, visit: https://web.stanford.edu/class/engr108/   To view all online courses and programs offered by Stanford, visit: http://online.stanford.edu​|kTg0vKppOl8
395|9|23|Stanford ENGR108: Introduction to Applied Linear Algebra , 2020 , Lecture 23-VMLS vector linear func|2021-02-26T22:18:48Z|Lectures by Professor Stephen Boyd, Stanford University.  Introduction to applied linear algebra with emphasis on applications.   Topics include: Vectors, norm, and angle; linear independence and orthonormal sets; applications to document analysis. Clustering and the k-means algorithm. Matrices, left and right inverses, QR factorization. Least-squares and model fitting, regularization and cross-validation. Constrained and nonlinear least-squares.  To follow along with the course schedule and syllabus, visit: https://web.stanford.edu/class/engr108/   To view all online courses and programs offered by Stanford, visit: http://online.stanford.edu​|tQ3nrT6cZGE
396|9|24|Stanford ENGR108: Introduction to Applied Linear Algebra , 2020 , Lecture 24-VMLS linear func models|2021-02-26T22:20:55Z|Lectures by Professor Stephen Boyd, Stanford University.  Introduction to applied linear algebra with emphasis on applications.   Topics include: Vectors, norm, and angle; linear independence and orthonormal sets; applications to document analysis. Clustering and the k-means algorithm. Matrices, left and right inverses, QR factorization. Least-squares and model fitting, regularization and cross-validation. Constrained and nonlinear least-squares.  To follow along with the course schedule and syllabus, visit: https://web.stanford.edu/class/engr108/   To view all online courses and programs offered by Stanford, visit: http://online.stanford.edu​|8SD2zRNWQcU
397|9|25|Stanford ENGR108: Introduction to Applied Linear Algebra , 2020 , Lecture 25 - VMLS linear equations|2021-02-26T22:23:41Z|Lectures by Professor Stephen Boyd, Stanford University.  Introduction to applied linear algebra with emphasis on applications.   Topics include: Vectors, norm, and angle; linear independence and orthonormal sets; applications to document analysis. Clustering and the k-means algorithm. Matrices, left and right inverses, QR factorization. Least-squares and model fitting, regularization and cross-validation. Constrained and nonlinear least-squares.  To follow along with the course schedule and syllabus, visit: https://web.stanford.edu/class/engr108/   To view all online courses and programs offered by Stanford, visit: http://online.stanford.edu​|6zWKMTVGkgQ
398|9|26|Stanford ENGR108: Introduction to Applied Linear Algebra , 2020 , Lecture 26-VMLS linear dynamic sys|2021-02-26T22:25:53Z|Lectures by Professor Stephen Boyd, Stanford University.  Introduction to applied linear algebra with emphasis on applications.   Topics include: Vectors, norm, and angle; linear independence and orthonormal sets; applications to document analysis. Clustering and the k-means algorithm. Matrices, left and right inverses, QR factorization. Least-squares and model fitting, regularization and cross-validation. Constrained and nonlinear least-squares.  To follow along with the course schedule and syllabus, visit: https://web.stanford.edu/class/engr108/   To view all online courses and programs offered by Stanford, visit: http://online.stanford.edu​|NS0jvPLtgGc
399|9|27|Stanford ENGR108: Introduction to Applied Linear Algebra , 2020 , Lecture 27-VMLS matrix mult|2021-02-26T22:27:38Z|Lectures by Professor Stephen Boyd, Stanford University.  Introduction to applied linear algebra with emphasis on applications.   Topics include: Vectors, norm, and angle; linear independence and orthonormal sets; applications to document analysis. Clustering and the k-means algorithm. Matrices, left and right inverses, QR factorization. Least-squares and model fitting, regularization and cross-validation. Constrained and nonlinear least-squares.  To follow along with the course schedule and syllabus, visit: https://web.stanford.edu/class/engr108/   To view all online courses and programs offered by Stanford, visit: http://online.stanford.edu​|t-G9n2uCa4k
400|9|28|Stanford ENGR108: Introduction to Applied Linear Algebra , 2020 , Lecture 28-VMLS matrix mult ex|2021-02-26T22:30:10Z|Lectures by Professor Stephen Boyd, Stanford University.  Introduction to applied linear algebra with emphasis on applications.   Topics include: Vectors, norm, and angle; linear independence and orthonormal sets; applications to document analysis. Clustering and the k-means algorithm. Matrices, left and right inverses, QR factorization. Least-squares and model fitting, regularization and cross-validation. Constrained and nonlinear least-squares.  To follow along with the course schedule and syllabus, visit: https://web.stanford.edu/class/engr108/   To view all online courses and programs offered by Stanford, visit: http://online.stanford.edu​|NZfWiVuRMnI
401|9|29|Stanford ENGR108: Introduction to Applied Linear Algebra , 2020 , Lecture 29-VMLS mtrx pwrs & QR fac|2021-02-26T23:00:00Z|Lectures by Professor Stephen Boyd, Stanford University.  Introduction to applied linear algebra with emphasis on applications.   Topics include: Vectors, norm, and angle; linear independence and orthonormal sets; applications to document analysis. Clustering and the k-means algorithm. Matrices, left and right inverses, QR factorization. Least-squares and model fitting, regularization and cross-validation. Constrained and nonlinear least-squares.  To follow along with the course schedule and syllabus, visit: https://web.stanford.edu/class/engr108/   To view all online courses and programs offered by Stanford, visit: http://online.stanford.edu​|vDCZpK96k7k
402|9|30|Stanford ENGR108: Introduction to Applied Linear Algebra , 2020 , Lecture 30-VMLS left & right inv.|2021-02-26T21:47:44Z|Lectures by Professor Stephen Boyd, Stanford University.  Introduction to applied linear algebra with emphasis on applications.   Topics include: Vectors, norm, and angle; linear independence and orthonormal sets; applications to document analysis. Clustering and the k-means algorithm. Matrices, left and right inverses, QR factorization. Least-squares and model fitting, regularization and cross-validation. Constrained and nonlinear least-squares.  To follow along with the course schedule and syllabus, visit: https://web.stanford.edu/class/engr108/   To view all online courses and programs offered by Stanford, visit: http://online.stanford.edu​|WFx4oV2JcRM
403|9|31|Stanford ENGR108: Introduction to Applied Linear Algebra , 2020 , Lecture 31-VMLS solving linear eqs|2021-02-26T21:43:35Z|Lectures by Professor Stephen Boyd, Stanford University.  Introduction to applied linear algebra with emphasis on applications.   Topics include: Vectors, norm, and angle; linear independence and orthonormal sets; applications to document analysis. Clustering and the k-means algorithm. Matrices, left and right inverses, QR factorization. Least-squares and model fitting, regularization and cross-validation. Constrained and nonlinear least-squares.  To follow along with the course schedule and syllabus, visit: https://web.stanford.edu/class/engr108/   To view all online courses and programs offered by Stanford, visit: http://online.stanford.edu​|lWEJPWFIgtw
404|9|32|Stanford ENGR108: Introduction to Applied Linear Algebra , 2020 , Lecture 32 - VMLS pseudo inverse|2021-02-26T21:37:51Z|Lectures by Professor Stephen Boyd, Stanford University.  Introduction to applied linear algebra with emphasis on applications.   Topics include: Vectors, norm, and angle; linear independence and orthonormal sets; applications to document analysis. Clustering and the k-means algorithm. Matrices, left and right inverses, QR factorization. Least-squares and model fitting, regularization and cross-validation. Constrained and nonlinear least-squares.  To follow along with the course schedule and syllabus, visit: https://web.stanford.edu/class/engr108/   To view all online courses and programs offered by Stanford, visit: http://online.stanford.edu​|R69sATXr1lM
405|9|33|Stanford ENGR108: Introduction to Applied Linear Algebra , 2020 , Lecture 33 - VMLS least squares|2021-02-26T21:34:59Z|Lectures by Professor Stephen Boyd, Stanford University.  Introduction to applied linear algebra with emphasis on applications.   Topics include: Vectors, norm, and angle; linear independence and orthonormal sets; applications to document analysis. Clustering and the k-means algorithm. Matrices, left and right inverses, QR factorization. Least-squares and model fitting, regularization and cross-validation. Constrained and nonlinear least-squares.  To follow along with the course schedule and syllabus, visit: https://web.stanford.edu/class/engr108/   To view all online courses and programs offered by Stanford, visit: http://online.stanford.edu​|q7tuWtK2wWY
406|9|34|Stanford ENGR108: Introduction to Applied Linear Algebra , 2020 , Lecture 34-VMLS least squares ex.|2021-02-26T21:20:14Z|Lectures by Professor Stephen Boyd, Stanford University.  Introduction to applied linear algebra with emphasis on applications.   Topics include: Vectors, norm, and angle; linear independence and orthonormal sets; applications to document analysis. Clustering and the k-means algorithm. Matrices, left and right inverses, QR factorization. Least-squares and model fitting, regularization and cross-validation. Constrained and nonlinear least-squares.  To follow along with the course schedule and syllabus, visit: https://web.stanford.edu/class/engr108/   To view all online courses and programs offered by Stanford, visit: http://online.stanford.edu​|5XhCN-44ua0
407|9|35|Stanford ENGR108: Introduction to Applied Linear Algebra , 2020 , Lecture 35-VMLS LS data fitting|2021-02-26T21:18:10Z|Lectures by Professor Stephen Boyd, Stanford University.  Introduction to applied linear algebra with emphasis on applications.   Topics include: Vectors, norm, and angle; linear independence and orthonormal sets; applications to document analysis. Clustering and the k-means algorithm. Matrices, left and right inverses, QR factorization. Least-squares and model fitting, regularization and cross-validation. Constrained and nonlinear least-squares.  To follow along with the course schedule and syllabus, visit: https://web.stanford.edu/class/engr108/   To view all online courses and programs offered by Stanford, visit: http://online.stanford.edu​|1UssEuFDxd4
408|9|36|Stanford ENGR108: Introduction to Applied Linear Algebra , 2020 , Lecture 36-VMLS fit univariate fnc|2021-02-26T21:41:17Z|Lectures by Professor Stephen Boyd, Stanford University.  Introduction to applied linear algebra with emphasis on applications.   Topics include: Vectors, norm, and angle; linear independence and orthonormal sets; applications to document analysis. Clustering and the k-means algorithm. Matrices, left and right inverses, QR factorization. Least-squares and model fitting, regularization and cross-validation. Constrained and nonlinear least-squares.  To follow along with the course schedule and syllabus, visit: https://web.stanford.edu/class/engr108/   To view all online courses and programs offered by Stanford, visit: http://online.stanford.edu​|R6sObXmPqXw
409|9|37|Stanford ENGR108: Introduction to Applied Linear Algebra , 2020 , Lecture 37 - VMLS validation|2021-02-26T21:12:42Z|Lectures by Professor Stephen Boyd, Stanford University.  Introduction to applied linear algebra with emphasis on applications.   Topics include: Vectors, norm, and angle; linear independence and orthonormal sets; applications to document analysis. Clustering and the k-means algorithm. Matrices, left and right inverses, QR factorization. Least-squares and model fitting, regularization and cross-validation. Constrained and nonlinear least-squares.  To follow along with the course schedule and syllabus, visit: https://web.stanford.edu/class/engr108/   To view all online courses and programs offered by Stanford, visit: http://online.stanford.edu​|SaERMg6BhWk
410|9|38|Stanford ENGR108: Introduction to Applied Linear Algebra , 2020 , Lecture 38 - VMLS classification|2021-02-26T21:08:41Z|Lectures by Professor Stephen Boyd, Stanford University.  Introduction to applied linear algebra with emphasis on applications.   Topics include: Vectors, norm, and angle; linear independence and orthonormal sets; applications to document analysis. Clustering and the k-means algorithm. Matrices, left and right inverses, QR factorization. Least-squares and model fitting, regularization and cross-validation. Constrained and nonlinear least-squares.  To follow along with the course schedule and syllabus, visit: https://web.stanford.edu/class/engr108/   To view all online courses and programs offered by Stanford, visit: http://online.stanford.edu​|3LGyi_9Yn8s
411|9|39|Stanford ENGR108: Introduction to Applied Linear Algebra , 2020 , Lecture 39-VMLS LS classification|2021-02-26T20:06:20Z|Lectures by Professor Stephen Boyd, Stanford University.  Introduction to applied linear algebra with emphasis on applications.   Topics include: Vectors, norm, and angle; linear independence and orthonormal sets; applications to document analysis. Clustering and the k-means algorithm. Matrices, left and right inverses, QR factorization. Least-squares and model fitting, regularization and cross-validation. Constrained and nonlinear least-squares.  To follow along with the course schedule and syllabus, visit: https://web.stanford.edu/class/engr108/   To view all online courses and programs offered by Stanford, visit: http://online.stanford.edu​|J-xhBfJn1mw
412|9|40|Stanford ENGR108: Introduction to Applied Linear Algebra , 2020 , Lecture 40-VMLS multiclass classif|2021-02-26T19:57:59Z|Lectures by Professor Stephen Boyd, Stanford University.  Introduction to applied linear algebra with emphasis on applications.   Topics include: Vectors, norm, and angle; linear independence and orthonormal sets; applications to document analysis. Clustering and the k-means algorithm. Matrices, left and right inverses, QR factorization. Least-squares and model fitting, regularization and cross-validation. Constrained and nonlinear least-squares.  To follow along with the course schedule and syllabus, visit: https://web.stanford.edu/class/engr108/   To view all online courses and programs offered by Stanford, visit: http://online.stanford.edu​|WaOr_4M8yfE
413|9|41|Stanford ENGR108: Introduction to Applied Linear Algebra , 2020 , Lecture 41-VMLS multi objective LS|2021-02-26T21:50:44Z|Lectures by Professor Stephen Boyd, Stanford University.  Introduction to applied linear algebra with emphasis on applications.   Topics include: Vectors, norm, and angle; linear independence and orthonormal sets; applications to document analysis. Clustering and the k-means algorithm. Matrices, left and right inverses, QR factorization. Least-squares and model fitting, regularization and cross-validation. Constrained and nonlinear least-squares.  To follow along with the course schedule and syllabus, visit: https://web.stanford.edu/class/engr108/   To view all online courses and programs offered by Stanford, visit: http://online.stanford.edu​|p18I80UDjNs
414|9|42|Stanford ENGR108: Introduction to Applied Linear Algebra , 2020 , Lecture 42-ctrl via multi obj LS|2021-03-02T17:16:29Z|Lectures by Professor Stephen Boyd, Stanford University.  Introduction to applied linear algebra with emphasis on applications.   Topics include: Vectors, norm, and angle; linear independence and orthonormal sets; applications to document analysis. Clustering and the k-means algorithm. Matrices, left and right inverses, QR factorization. Least-squares and model fitting, regularization and cross-validation. Constrained and nonlinear least-squares.  To follow along with the course schedule and syllabus, visit: https://web.stanford.edu/class/engr108/   To view all online courses and programs offered by Stanford, visit: http://online.stanford.edu​|qr9uzoRkYK0
415|9|43|Stanford ENGR108: Introduction to Applied Linear Algebra , 2020 , Lecture 43-MLS est & inversion|2021-02-26T21:54:10Z|Lectures by Professor Stephen Boyd, Stanford University.  Introduction to applied linear algebra with emphasis on applications.   Topics include: Vectors, norm, and angle; linear independence and orthonormal sets; applications to document analysis. Clustering and the k-means algorithm. Matrices, left and right inverses, QR factorization. Least-squares and model fitting, regularization and cross-validation. Constrained and nonlinear least-squares.  To follow along with the course schedule and syllabus, visit: https://web.stanford.edu/class/engr108/   To view all online courses and programs offered by Stanford, visit: http://online.stanford.edu​|y44HXBjCZ0Q
416|9|44|Stanford ENGR108: Introduction to Applied Linear Algebra , 2020 , Lecture 44-VMLS reg data fitting|2021-02-26T22:00:19Z|Lectures by Professor Stephen Boyd, Stanford University.  Introduction to applied linear algebra with emphasis on applications.   Topics include: Vectors, norm, and angle; linear independence and orthonormal sets; applications to document analysis. Clustering and the k-means algorithm. Matrices, left and right inverses, QR factorization. Least-squares and model fitting, regularization and cross-validation. Constrained and nonlinear least-squares.  To follow along with the course schedule and syllabus, visit: https://web.stanford.edu/class/engr108/   To view all online courses and programs offered by Stanford, visit: http://online.stanford.edu​|XTdzAMf0gG0
417|9|45|Stanford ENGR108: Introduction to Applied Linear Algebra , 2020 , Lecture 45-VMLS constrained LS|2021-02-26T22:04:29Z|Lectures by Professor Stephen Boyd, Stanford University.  Introduction to applied linear algebra with emphasis on applications.   Topics include: Vectors, norm, and angle; linear independence and orthonormal sets; applications to document analysis. Clustering and the k-means algorithm. Matrices, left and right inverses, QR factorization. Least-squares and model fitting, regularization and cross-validation. Constrained and nonlinear least-squares.  To follow along with the course schedule and syllabus, visit: https://web.stanford.edu/class/engr108/   To view all online courses and programs offered by Stanford, visit: http://online.stanford.edu​|vOFsja_YmJc
418|9|46|Stanford ENGR108: Introduction to Applied Linear Algebra , 2020 , Lecture 46-VMLS solve cstr LS prob|2021-03-05T00:15:08Z|Lectures by Professor Stephen Boyd, Stanford University.  Introduction to applied linear algebra with emphasis on applications.   Topics include: Vectors, norm, and angle; linear independence and orthonormal sets; applications to document analysis. Clustering and the k-means algorithm. Matrices, left and right inverses, QR factorization. Least-squares and model fitting, regularization and cross-validation. Constrained and nonlinear least-squares.  To follow along with the course schedule and syllabus, visit: https://web.stanford.edu/class/engr108/   To view all online courses and programs offered by Stanford, visit: http://online.stanford.edu​|maCbeyno_mI
419|9|47|Stanford ENGR108: Introduction to Applied Linear Algebra , 2020 , Lecture 47-VMLS portfolio optim|2021-02-26T19:53:54Z|Lectures by Professor Stephen Boyd, Stanford University.  Introduction to applied linear algebra with emphasis on applications.   Topics include: Vectors, norm, and angle; linear independence and orthonormal sets; applications to document analysis. Clustering and the k-means algorithm. Matrices, left and right inverses, QR factorization. Least-squares and model fitting, regularization and cross-validation. Constrained and nonlinear least-squares.  To follow along with the course schedule and syllabus, visit: https://web.stanford.edu/class/engr108/   To view all online courses and programs offered by Stanford, visit: http://online.stanford.edu​|fKUea6LCzU8
420|9|48|Stanford ENGR108: Introduction to Applied Linear Algebra , 2020 , Lecture 48-VMLS linear quadrt ctrl|2021-02-26T23:22:20Z|Lectures by Professor Stephen Boyd, Stanford University.  Introduction to applied linear algebra with emphasis on applications.   Topics include: Vectors, norm, and angle; linear independence and orthonormal sets; applications to document analysis. Clustering and the k-means algorithm. Matrices, left and right inverses, QR factorization. Least-squares and model fitting, regularization and cross-validation. Constrained and nonlinear least-squares.  To follow along with the course schedule and syllabus, visit: https://web.stanford.edu/class/engr108/   To view all online courses and programs offered by Stanford, visit: http://online.stanford.edu​|MNqVUDkudYM
421|9|49|Stanford ENGR108: Introduction to Applied Linear Algebra , 2020 , Lecture 49-VMLS lin quadrt st est|2021-02-26T23:25:56Z|Lectures by Professor Stephen Boyd, Stanford University.  Introduction to applied linear algebra with emphasis on applications.   Topics include: Vectors, norm, and angle; linear independence and orthonormal sets; applications to document analysis. Clustering and the k-means algorithm. Matrices, left and right inverses, QR factorization. Least-squares and model fitting, regularization and cross-validation. Constrained and nonlinear least-squares.  To follow along with the course schedule and syllabus, visit: https://web.stanford.edu/class/engr108/   To view all online courses and programs offered by Stanford, visit: http://online.stanford.edu​|zu2nYmSfY6U
422|9|50|Stanford ENGR108: Introduction to Applied Linear Algebra , 2020 , Lecture 50-VMLS nonlinear eq. & LS|2021-02-26T23:29:01Z|Lectures by Professor Stephen Boyd, Stanford University.  Introduction to applied linear algebra with emphasis on applications.   Topics include: Vectors, norm, and angle; linear independence and orthonormal sets; applications to document analysis. Clustering and the k-means algorithm. Matrices, left and right inverses, QR factorization. Least-squares and model fitting, regularization and cross-validation. Constrained and nonlinear least-squares.  To follow along with the course schedule and syllabus, visit: https://web.stanford.edu/class/engr108/   To view all online courses and programs offered by Stanford, visit: http://online.stanford.edu​|nK6uSzKQlEs
423|9|51|Stanford ENGR108: Introduction to Applied Linear Algebra , 2020 , Lecture 51-VMLS Leven. Marq. algo|2021-02-26T23:33:37Z|Lectures by Professor Stephen Boyd, Stanford University.  Introduction to applied linear algebra with emphasis on applications.   Topics include: Vectors, norm, and angle; linear independence and orthonormal sets; applications to document analysis. Clustering and the k-means algorithm. Matrices, left and right inverses, QR factorization. Least-squares and model fitting, regularization and cross-validation. Constrained and nonlinear least-squares.  To follow along with the course schedule and syllabus, visit: https://web.stanford.edu/class/engr108/   To view all online courses and programs offered by Stanford, visit: http://online.stanford.edu​|UQsOyMj9lnI
424|9|52|Stanford ENGR108: Introduction to Applied Linear Algebra , 2020 , Lecture 52-VMLS nonlin mdl fitting|2021-02-26T23:36:00Z|Lectures by Professor Stephen Boyd, Stanford University.  Introduction to applied linear algebra with emphasis on applications.   Topics include: Vectors, norm, and angle; linear independence and orthonormal sets; applications to document analysis. Clustering and the k-means algorithm. Matrices, left and right inverses, QR factorization. Least-squares and model fitting, regularization and cross-validation. Constrained and nonlinear least-squares.  To follow along with the course schedule and syllabus, visit: https://web.stanford.edu/class/engr108/   To view all online courses and programs offered by Stanford, visit: http://online.stanford.edu​|W0mamVcppBA
425|9|53|Stanford ENGR108: Introduction to Applied Linear Algebra , 2020 , Lecture 53-VMLS cstrd nonlinear LS|2021-03-05T00:31:26Z|Lectures by Professor Stephen Boyd, Stanford University.  Introduction to applied linear algebra with emphasis on applications.   Topics include: Vectors, norm, and angle; linear independence and orthonormal sets; applications to document analysis. Clustering and the k-means algorithm. Matrices, left and right inverses, QR factorization. Least-squares and model fitting, regularization and cross-validation. Constrained and nonlinear least-squares.  To follow along with the course schedule and syllabus, visit: https://web.stanford.edu/class/engr108/   To view all online courses and programs offered by Stanford, visit: http://online.stanford.edu​|SM_ZieyKicU
426|9|54|Stanford ENGR108: Introduction to Applied Linear Algebra , 2020 , Lecture 54-VMLS aug Lagragian mthd|2021-02-26T23:03:15Z|Lectures by Professor Stephen Boyd, Stanford University.  Introduction to applied linear algebra with emphasis on applications.   Topics include: Vectors, norm, and angle; linear independence and orthonormal sets; applications to document analysis. Clustering and the k-means algorithm. Matrices, left and right inverses, QR factorization. Least-squares and model fitting, regularization and cross-validation. Constrained and nonlinear least-squares.  To follow along with the course schedule and syllabus, visit: https://web.stanford.edu/class/engr108/   To view all online courses and programs offered by Stanford, visit: http://online.stanford.edu​|00njRSL8WNQ
427|10|1|Stanford CS234: Reinforcement Learning , Winter 2019 , Lecture 1 - Introduction - Emma Brunskill|2019-03-30T05:04:43Z|This class will provide a solid introduction to the field of RL. Students will learn about the core challenges and approaches in the field, including generalization and exploration.  Through a combination of lectures, and written and coding assignments, students will become well-versed in key ideas and techniques for RL. Assignments will include the basics of reinforcement learning as well as deep reinforcement learning-- an extremely promising new area that combines deep learning techniques with reinforcement learning. In addition, students will advance their understanding and the field of RL through an open-ended project.   For more information about Stanford’s Artificial Intelligence professional and graduate programs, visit: https://stanford.io/ai|FgzM3zpZ55o
428|10|2|Stanford CS234: Reinforcement Learning , Winter 2019 , Lecture 2 - Given a Model of the World|2019-03-30T05:22:24Z|This class will provide a solid introduction to the field of RL. Students will learn about the core challenges and approaches in the field, including generalization and exploration.  Through a combination of lectures, and written and coding assignments, students will become well-versed in key ideas and techniques for RL. Assignments will include the basics of reinforcement learning as well as deep reinforcement learning-- an extremely promising new area that combines deep learning techniques with reinforcement learning. In addition, students will advance their understanding and the field of RL through an open-ended project.   For more information about Stanford’s Artificial Intelligence professional and graduate programs, visit: https://stanford.io/ai|E3f2Camj0Is
429|10|3|Stanford CS234: Reinforcement Learning , Winter 2019 , Lecture 3 -  Model-Free Policy Evaluation|2019-03-30T05:23:12Z|This class will provide a solid introduction to the field of RL. Students will learn about the core challenges and approaches in the field, including generalization and exploration.  Through a combination of lectures, and written and coding assignments, students will become well-versed in key ideas and techniques for RL. Assignments will include the basics of reinforcement learning as well as deep reinforcement learning-- an extremely promising new area that combines deep learning techniques with reinforcement learning. In addition, students will advance their understanding and the field of RL through an open-ended project.   For more information about Stanford’s Artificial Intelligence professional and graduate programs, visit: https://stanford.io/ai|dRIhrn8cc9w
430|10|4|Stanford CS234: Reinforcement Learning , Winter 2019 , Lecture 4 - Model Free Control|2019-03-30T05:14:11Z|This class will provide a solid introduction to the field of RL. Students will learn about the core challenges and approaches in the field, including generalization and exploration.  Through a combination of lectures, and written and coding assignments, students will become well-versed in key ideas and techniques for RL. Assignments will include the basics of reinforcement learning as well as deep reinforcement learning-- an extremely promising new area that combines deep learning techniques with reinforcement learning. In addition, students will advance their understanding and the field of RL through an open-ended project.   For more information about Stanford’s Artificial Intelligence professional and graduate programs, visit: https://stanford.io/ai|j080VBVGkfQ
431|10|5|Stanford CS234: Reinforcement Learning , Winter 2019 , Lecture 5 - Value Function Approximation|2019-03-30T05:16:44Z|This class will provide a solid introduction to the field of RL. Students will learn about the core challenges and approaches in the field, including generalization and exploration.  Through a combination of lectures, and written and coding assignments, students will become well-versed in key ideas and techniques for RL. Assignments will include the basics of reinforcement learning as well as deep reinforcement learning-- an extremely promising new area that combines deep learning techniques with reinforcement learning. In addition, students will advance their understanding and the field of RL through an open-ended project.   For more information about Stanford’s Artificial Intelligence professional and graduate programs, visit: https://stanford.io/ai|buptHUzDKcE
432|10|6|Stanford CS234: Reinforcement Learning , Winter 2019 , Lecture 6 - CNNs and Deep Q Learning|2019-04-15T21:12:03Z|This class will provide a solid introduction to the field of RL. Students will learn about the core challenges and approaches in the field, including generalization and exploration.  Through a combination of lectures, and written and coding assignments, students will become well-versed in key ideas and techniques for RL. Assignments will include the basics of reinforcement learning as well as deep reinforcement learning-- an extremely promising new area that combines deep learning techniques with reinforcement learning. In addition, students will advance their understanding and the field of RL through an open-ended project.   For more information about Stanford’s Artificial Intelligence professional and graduate programs, visit: https://stanford.io/ai|gOV8-bC1_KU
433|10|7|Stanford CS234: Reinforcement Learning , Winter 2019 , Lecture 7 - Imitation Learning|2019-04-15T21:16:50Z|This class will provide a solid introduction to the field of RL. Students will learn about the core challenges and approaches in the field, including generalization and exploration.  Through a combination of lectures, and written and coding assignments, students will become well-versed in key ideas and techniques for RL. Assignments will include the basics of reinforcement learning as well as deep reinforcement learning-- an extremely promising new area that combines deep learning techniques with reinforcement learning. In addition, students will advance their understanding and the field of RL through an open-ended project.   For more information about Stanford’s Artificial Intelligence professional and graduate programs, visit: https://stanford.io/ai|V7CY68zH6ps
434|10|8|Stanford CS234: Reinforcement Learning , Winter 2019 , Lecture 8 - Policy Gradient I|2019-04-15T21:15:36Z|This class will provide a solid introduction to the field of RL. Students will learn about the core challenges and approaches in the field, including generalization and exploration.  Through a combination of lectures, and written and coding assignments, students will become well-versed in key ideas and techniques for RL. Assignments will include the basics of reinforcement learning as well as deep reinforcement learning-- an extremely promising new area that combines deep learning techniques with reinforcement learning. In addition, students will advance their understanding and the field of RL through an open-ended project.   For more information about Stanford’s Artificial Intelligence professional and graduate programs, visit: https://stanford.io/ai|8LEuyYXGQjU
435|10|9|Stanford CS234: Reinforcement Learning , Winter 2019 , Lecture 9 - Policy Gradient II|2019-05-07T22:42:19Z|This class will provide a solid introduction to the field of RL. Students will learn about the core challenges and approaches in the field, including generalization and exploration.  Through a combination of lectures, and written and coding assignments, students will become well-versed in key ideas and techniques for RL. Assignments will include the basics of reinforcement learning as well as deep reinforcement learning-- an extremely promising new area that combines deep learning techniques with reinforcement learning. In addition, students will advance their understanding and the field of RL through an open-ended project.   For more information about Stanford’s Artificial Intelligence professional and graduate programs, visit: https://stanford.io/ai|E-_ecpD5PkE
436|10|10|Stanford CS234: Reinforcement Learning , Winter 2019 , Lecture 10 - Policy Gradient III & Review|2019-05-07T22:44:38Z|This class will provide a solid introduction to the field of RL. Students will learn about the core challenges and approaches in the field, including generalization and exploration.  Through a combination of lectures, and written and coding assignments, students will become well-versed in key ideas and techniques for RL. Assignments will include the basics of reinforcement learning as well as deep reinforcement learning-- an extremely promising new area that combines deep learning techniques with reinforcement learning. In addition, students will advance their understanding and the field of RL through an open-ended project.   For more information about Stanford’s Artificial Intelligence professional and graduate programs, visit: https://stanford.io/ai|o_i5F1zGPLs
437|10|11|Stanford CS234: Reinforcement Learning , Winter 2019 , Lecture 11 - Fast Reinforcement Learning|2019-05-07T22:47:45Z|This class will provide a solid introduction to the field of RL. Students will learn about the core challenges and approaches in the field, including generalization and exploration.  Through a combination of lectures, and written and coding assignments, students will become well-versed in key ideas and techniques for RL. Assignments will include the basics of reinforcement learning as well as deep reinforcement learning-- an extremely promising new area that combines deep learning techniques with reinforcement learning. In addition, students will advance their understanding and the field of RL through an open-ended project.   For more information about Stanford’s Artificial Intelligence professional and graduate programs, visit: https://stanford.io/ai|RN8qpSs8ozY
438|10|12|Stanford CS234: Reinforcement Learning , Winter 2019 , Lecture 12 - Fast Reinforcement Learning II|2019-05-07T22:50:01Z|This class will provide a solid introduction to the field of RL. Students will learn about the core challenges and approaches in the field, including generalization and exploration.  Through a combination of lectures, and written and coding assignments, students will become well-versed in key ideas and techniques for RL. Assignments will include the basics of reinforcement learning as well as deep reinforcement learning-- an extremely promising new area that combines deep learning techniques with reinforcement learning. In addition, students will advance their understanding and the field of RL through an open-ended project.   For more information about Stanford’s Artificial Intelligence professional and graduate programs, visit: https://stanford.io/ai|jJ7JbQBTChM
439|10|13|Stanford CS234: Reinforcement Learning , Winter 2019 , Lecture 13 - Fast Reinforcement Learning III|2019-05-29T21:57:41Z|This class will provide a solid introduction to the field of RL. Students will learn about the core challenges and approaches in the field, including generalization and exploration.  Through a combination of lectures, and written and coding assignments, students will become well-versed in key ideas and techniques for RL. Assignments will include the basics of reinforcement learning as well as deep reinforcement learning-- an extremely promising new area that combines deep learning techniques with reinforcement learning. In addition, students will advance their understanding and the field of RL through an open-ended project.   For more information about Stanford’s Artificial Intelligence professional and graduate programs, visit: https://stanford.io/ai|Hg_uyWezMM0
440|10|14|Stanford CS234: Reinforcement Learning , Winter 2019 , Lecture 15 - Batch Reinforcement Learning|2019-05-29T22:00:53Z|This class will provide a solid introduction to the field of RL. Students will learn about the core challenges and approaches in the field, including generalization and exploration.  Through a combination of lectures, and written and coding assignments, students will become well-versed in key ideas and techniques for RL. Assignments will include the basics of reinforcement learning as well as deep reinforcement learning-- an extremely promising new area that combines deep learning techniques with reinforcement learning. In addition, students will advance their understanding and the field of RL through an open-ended project.   For more information about Stanford’s Artificial Intelligence professional and graduate programs, visit: https://stanford.io/ai|zPU1SRHuAW8
441|10|15|Stanford CS234: Reinforcement Learning , Winter 2019 , Lecture 16 - Monte Carlo Tree Search|2019-05-29T22:04:01Z|This class will provide a solid introduction to the field of RL. Students will learn about the core challenges and approaches in the field, including generalization and exploration.  Through a combination of lectures, and written and coding assignments, students will become well-versed in key ideas and techniques for RL. Assignments will include the basics of reinforcement learning as well as deep reinforcement learning-- an extremely promising new area that combines deep learning techniques with reinforcement learning. In addition, students will advance their understanding and the field of RL through an open-ended project.   For more information about Stanford’s Artificial Intelligence professional and graduate programs, visit: https://stanford.io/ai|vDF1BYWhqL8
